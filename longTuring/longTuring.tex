\documentclass[11pt]{memoir}

\pagestyle{plain}
%\pagestyle{simple}
\setlrmargins{*}{*}{1}
\checkandfixthelayout

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{3}
\counterwithout{section}{chapter}
\counterwithin{equation}{section}
%\numberwithin{equation}{section} % in amsmath
%\counterwithout{figure}{chapter}
\counterwithin{figure}{section}

\makeatletter
% To correct a memoir bug:
\renewcommand{\@memmain@floats}{%
  \counterwithin{figure}{section}
  \counterwithin{table}{section}}
\makeatother

\firmlists

% If you do not want the bibliography on a separate page:
\renewcommand{\bibsection}{% 
\section*{\bibname} 
\prebibhook}

\usepackage[backref,hyperindex,colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage[numbered]{bookmark} % Allows to place a bookmark, see the title. Shows section numbers.
% \usepackage[all]{hypcap} % After hyperref, to anchor floats correctly.
% \usepackage{float}
 % After hyperref:
\usepackage[algo2e,algosection,tworuled,noend,noline]{algorithm2e}
\usepackage[baskerville]{gacs}
%\usepackage[sf-headings]{gacs}
\usepackage{gacs-algo} % After hyperref.
% After gacs.sty

%\usepackage[pagecolor={LightCyan1}]{pagecolor}
%\usepackage[pagecolor={DarkSeaGreen1}]{pagecolor}
%\usepackage[pagecolor={Honeydew1}]{pagecolor}
%\usepackage[pagecolor={Azure1}]{pagecolor}
%\usepackage[pagecolor={Cornsilk1}]{pagecolor}
%\usepackage[pagecolor={Ivory1}]{pagecolor}

\hyphenation{com-plex-ity des-tin-at-ion co-lon-ies}

\newcommand{\shownotes}{1}
\ifnum\shownotes=1
\newcommand{\authnote}[3]
{\text{{ \textcolor{#3}{\( \langle\hspace{-0.2em}\langle \)\textsf{\footnotesize #1: #2}\( \rangle\hspace{-0.2em}\rangle \)}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\Pnote}[1]{{\authnote{P}{#1}{cyan}}}
\newcommand{\Inote}[1]{{\authnote{I}{#1}{blue}}}

\theoremstyle{definition} % not italicized
\newtheorem{Premark}{\color{cyan}Peter remark}
\newenvironment{premark}{\begin{Premark}\color{cyan}}{\varqed\end{Premark}}

\newtheorem{Iremark}{\color{blue}Ilir remark}[Premark]
\newenvironment{iremark}{\begin{Iremark}\color{blue}}{\varqed\end{Iremark}}

\renewcommand{\Pnote}[1]{\begin{premark}#1\end{premark}}
\renewcommand{\Inote}[1]{\begin{iremark}#1\end{iremark}}

\renewcommand{\le}{\leq}
\renewcommand{\ge}{\geq}

\renewcommand{\vek}[1]{\mathbf{#1}}

\newcommand{\fld}[1]{\ensuremath{\textit{#1\/}}}
%\newcommand{\rul}[1]{\ensuremath{\texttt{\slshape #1\/}}}
\newcommand{\rul}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\tNormal}{e_{\mathrm{normal}}}
\newcommand{\tZig}{e_{\mathrm{zig}}}
\newcommand{\tHeal}{e_{\mathrm{heal}}}
\newcommand{\tRebuild}{e_{\mathrm{rebuild}}}

% Using def for the possibility of switching between LaTeX and XeTeX:
\def\B{B}  
\def\U{U}

\newcommand{\va}{\vek{a}}
\newcommand{\Bad}{\mathrm{Bad}}
\newcommand{\Vacant}{\mathrm{Vac}}
\newcommand{\blank}{\text{\textvisiblespace}}
\newcommand{\Configs}{\mathrm{Configs}}
\newcommand{\D}{D}
\newcommand{\E}{E}
\newcommand{\f}{f}
\newcommand{\F}{F}
\def\G{G}
\newcommand{\h}{h}
\renewcommand{\H}{H}
\newcommand{\hc}{\hat h}
\newcommand{\vhc}{\vek{\hat h}}
\newcommand{\Int}{\mathrm{Int}}
\newcommand{\Noise}{\mathit{Noise}}
\newcommand{\Output}{\mathit{output}}
\newcommand{\passno}{\pi}
\newcommand{\PenetrationLen}{\mathrm{PenetLen}}
\newcommand{\Plus}{\oplus}
\newcommand{\Minus}{\ominus}
\newcommand{\pos}{\mathrm{pos}}
\newcommand{\curcell}{\textrm{cur-cell}}
\newcommand{\Q}{Q}
\newcommand{\R}{R}
\newcommand{\Tu}{T}
\newcommand{\Tus}{T^{*}}
\newcommand{\V}{V}
\newcommand{\Z}{Z}
\newcommand{\z}{z}

\newcommand{\Addr}{\fld{Addr}}
\newcommand{\cAddr}{\fld{cAddr}}
\newcommand{\Turned}{\fld{Turned}}
\newcommand{\Core}{\fld{Core}}
\newcommand{\cCore}{\fld{cCore}}
\newcommand{\Dir}{\fld{Dir}}
\newcommand{\cDir}{\fld{cDir}}
\newcommand{\Drift}{\fld{Drift}}
\newcommand{\Doomed}{\fld{Doomed}}
\newcommand{\cDrift}{\fld{cDrift}}
%\renewcommand{\G}{\fld{NonAdj}}
\newcommand{\CDwell}{\cns{dwell}}
\newcommand{\Adj}{\fld{Adj}}
\newcommand{\cHold}{\fld{Hold}}
\newcommand{\Hold}{\fld{Hold}}
\newcommand{\Index}{\fld{Index}}
\newcommand{\cInfo}{\fld{cInfo}}
\newcommand{\Info}{\fld{Info}}
\newcommand{\Kind}{\fld{Kind}}
\newcommand{\cKind}{\fld{cKind}}
\newcommand{\cLevel}{\fld{cLevel}}
\newcommand{\Mode}{\fld{Mode}}
\newcommand{\Prog}{\fld{Prog}}
\newcommand{\Heal}{\fld{Heal}}
\newcommand{\rHeal}{\rul{Heal}}
\newcommand{\Plan}{\fld{Plan}}
\newcommand{\Rebuild}{\fld{Rebuild}}
\newcommand{\Stage}{\fld{Stage}}
\newcommand{\State}{\fld{State}}
\newcommand{\cState}{\fld{cState}}
\newcommand{\Sweep}{\fld{Sweep}}
\newcommand{\cSweep}{\fld{cSw}}
\newcommand{\cWork}{\fld{cWork}}
\newcommand{\Work}{\fld{Work}}
\newcommand{\ZigDepth}{\fld{ZigDepth}}
\newcommand{\FDepth}{\fld{FeatherDepth}}
\newcommand{\ZigDir}{\fld{ZigDir}}

\newcommand{\Bridge}{\mathrm{Bridge}}
\newcommand{\Committing}{\mathrm{Committing}}
\newcommand{\Coordinated}{\mathrm{Coordinated}}
\newcommand{\decode}{\mathrm{decode}}
\newcommand{\dir}{\mathrm{dir}}
\newcommand{\encode}{\mathrm{encode}}
\newcommand{\front}{\mathrm{front}}
\newcommand{\Histories}{\mathrm{Histories}}
\newcommand{\Trajectories}{\mathrm{Trajectories}}
\newcommand{\Last}{\mathrm{Last}}
\newcommand{\Marking}{\mathrm{Marking}}
\newcommand{\Member}{\mathrm{Member}}
\newcommand{\Normal}{\mathrm{Normal}}
\newcommand{\patch}{\mathrm{patch}}
\newcommand{\Planning}{\mathrm{Planing}}
\newcommand{\Rebuilding}{\mathrm{Rebuilding}}
\newcommand{\score}{\mathrm{score}}
\newcommand{\Target}{\mathrm{Target}}

\newcommand{\PadLen}{\mathit{PadLen}}
\newcommand{\Interpr}{\mathit{Interpr}}

\newcommand{\Healing}{\mathrm{Healing}}
\newcommand{\start}{\mathrm{start}}
\newcommand{\state}{\mathrm{state}}
\newcommand{\Stem}{\mathrm{Stem}}
\newcommand{\tape}{\mathrm{tape}}
\newcommand{\TransferSw}{\mathrm{TransferSw}}
\newcommand{\Un}{\mathrm{Univ}}

\newcommand{\increment}[1]{#1\mathord{+}\mathord{+}}
\newcommand{\decrement}[1]{#1\mathord{-}\mathord{-}}


\newcommand{\ruAddrJmp}{\rul{AddrJmp}}
\newcommand{\Alarm}{\rul{Alarm}}
% \newcommand{\Commit}{\rul{Commit}}
\newcommand{\Comp}{\rul{Compute}}
\newcommand{\BigAlarm}{\rul{BigAlarm}}
%\newcommand{\Vacate}{\rul{Vacate}}
% \newcommand{\Mark}{\rul{Mark}}
\newcommand{\Move}{\rul{Move}}
% \newcommand{\Plan}{\rul{Plan}}
\newcommand{\ruSwing}{\rul{Swing}}
\newcommand{\Transfer}{\rul{Transfer}}
\newcommand{\UsefulComp}{\rul{UsefulComp}}
\newcommand{\WriteProgramBit}{\rul{WriteProgramBit}}
\newcommand{\Zigzag}{\rul{Zigzag}}

\newcommand{\mrk}{\mathrm{mrk}}
\newcommand{\K}{K}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Zg}{\mathcal{Z}_g}

\newcommand{\cns}[1]{c_{\textrm{\upshape #1}}}
\newcommand{\CAtt}{\cns{attack}}
\newcommand{\CPass}{\cns{pass}}
\newcommand{\CRebuild}{\cns{rebuild}}
\newcommand{\CCleanEsc}{\cns{clean-esc}}
\newcommand{\CEsc}{\cns{esc}}
\newcommand{\CCleanS}{\cns{clean-s}}
\newcommand{\CCleanT}{\cns{clean-t}}
%\newcommand{\CDepth}[1]{\cns{depth-#1}}
\newcommand{\cIncr}{\cns{incr}}
\newcommand{\CSpill}{\cns{spill}}

\newcommand{\Left}{\text{left}}
\newcommand{\Right}{\text{right}}

\renewcommand{\r}{r} % what was it?

\begin{document}

\title{A reliable Turing machine}
% Why do I need this?  Some people get the title bookmarked even without this.
\bookmark[page=1,level=0]{A reliable Turing machine}

\author{Ilir \c{C}apuni \and Peter G\'acs}
% \\ Boston University
% \\ gacs@bu.edu

\maketitle

\begin{abstract}
The title says it.
\end{abstract}

\tableofcontents*

\section{Introduction}

\subsection{To be written}

\subsection{Turing machines}\label{sec:TM}

Our contribution could use any one of the standard definitions of a Turing machine, but it will
be convenient to modify it.
Let us recall that a Turing machine is defined by a finite set \( \Gamma \) of \df{internal states},
a finite alphabet \( \Sigma \) of \df{tape symbols}, a transition function
\begin{align*}
             \delta\colon\Sigma\times \Gamma\to \Sigma\times\Gamma\times\{-1,1\},
\end{align*}
and possibly some distinguished states and tape symbols.
Any array \( A\in\Sigma^{\bbZ} \) is called the \df{tape configuration}, or \df{tape content}.
At any time, the head is at some integer position \( \h \), and is observing the tape
symbol \( A(\h) \).
The meaning of \( \delta(a,q)=(a',q',d) \) is that if \( A(\h)=a \) and the state is \( q \) then
the \( A(\h) \) will be rewritten as \( a' \) and \( \h \) will change to \( \h+d \).

We will use a model that is slightly different, but has clearly the same expressing power.
There is no set of internal states, but the head observes and modifies \emph{two}
neighboring tape cells at a time.
Thus, a Turing machine is defined as a pair
\begin{align*}
   (\Sigma,\tau).
 \end{align*}
The tape alphabet \( \Sigma \) contains at least the distinguished
symbols \( \blank,0,1 \) where \( \blank \) is called the \df{blank symbol}.
We have a \df{transition function}
\begin{align*}
             \tau\colon\Sigma^{2}\to \Sigma^{2}\times\{-1,1\}.
\end{align*}
The tape is blank at all but finitely many positions.
A \df{configuration} is a pair 
        \begin{align*}
             (A,\h),
        \end{align*}
\( \h\in\bbZ \) is the current \df{head position}, or \df{observed cell}, or \df{current cell},
and \( A\in\Sigma^{\bbZ} \) is the \df{tape content}, or \df{tape configuration}:
at position \( p \), the tape contains the symbol \( A(p) \).
If \( \xi=(A,\h) \) is a configuration then we will write
        \begin{align}\label{eq:config-1}
             \quad \xi.\tape=A,\quad \xi.\pos=\h.
        \end{align}
Though the tape alphabet may contain
non-binary symbols, we will restrict input and output to binary.

The transition function \( \tau \) tells us how to compute the next
configuration from the present one.
When the head observes the pair of tape cells
with content \( \va=(a_{0},a_{1}) \) at positions \( \h \), \( \h+1 \) then denoting
         \begin{align*}
           (\va',j)=\tau(\va),
         \end{align*}
will change tape content at positions \( \h \), \( \h+1 \) to \( a'_{0} \), \( a'_{1} \),
and move the head to tape position to \( \h+j \).

\begin{definition}[Fault]\label{def:fault}
We say that a \df{fault} occurs at time \( t \) if the output \( (\va',j) \)
of the    transition function at this time is replaced with some other value
(which is then used to compute the next configuration).
\end{definition}


\subsection{Codes, and the result}

For fault-tolerant computation, some redundant coding of the information is needed.

\begin{definition}[Codes]\label{def:codes}
    Let \( \Sigma_{1},\Sigma_{2} \) be two finite alphabets.
    A \df{block code} is given by a positive integer \( \Q \)---called
    the \df{block size}---and a pair of functions
    \begin{align*}
            \psi_{*} :\Sigma_{2}\to\Sigma_{1}^{\Q},
            \quad
            \psi^{*}:\Sigma_{1}^{\Q}\to\Sigma_{2}
    \end{align*}
    with the property \( \psi^{*}(\psi_{*}(x))=x \).
It is extended to strings by encoding each letter individually:
\( \psi_{*}(x_{1},\dots,x_{n})=\psi_{*}(x_{1})\dotsm\psi_{*}(x_{n}) \).
\end{definition}

For ease of spelling out a result, let us consider only computations whose outcome
is a single symbol, at tape position 1.
We will consider the machine \df{halted} if it writes the blank symbol at tape position 0.

\begin{theorem}\label{thm:main-main}
There is a Turing machine \( M_{1} \) with a 
function \( a\mapsto a.\Output \) defined on its alphabet, 
such that
for any Turing machine \( G \) with alphabet \( \Sigma \)
there are \( 0\le\eps <1 \) and \( \alpha_{1},\alpha_{2}>0 \) 
with the following property.

For each input length \( n=\abs{x} \) a block code
\( (\varphi_{*}, \varphi^{*}) \) of block size \( \Q=O((\log n)^{\alpha_{1}}) \) can be constructed 
such that the following holds.

Let \( M_{1} \) start its work from the initial tape configuration \( \xi=(\varphi_{*}(x),0) \).
Assume further that
during its operation, faults occur independently at random with probabilities \( \le \eps \).

Suppose that on input \( x \) machine \( G \) at time \( t \) writes writes the blank symbol
at position 0, and value \( y \) at position 1.
Then denoting by \( \eta(u) \) the configuration machine \( M_{1} \) at time \( u \),
at any time \( t' \) after
 \begin{align*}
   t\cdot (\log t)^{\alpha_{2}},
 \end{align*}
we have \( \eta(t').\tape(1).\Output= y \)
with probability at least \( 1 - O(\eps) \).
\end{theorem}

We emphasize that the actual
code \( \varphi \) of the construction will depend on \( n \) only in a simple way:
it will be the ``concatenation'' of one and the same fixed-size
code with itself, \( O(\log\log n) \) times.

\subsection{A shortcut solution}

A fault-tolerant one-dimensional cellular automaton is constructed
in~\cite{GacsSorg01}.
If our Turing machine could just simulate such an automaton, it would become
fault-tolerant.
This can indeed almost be done provided that the size of the computation is known in advance.
The cellular automaton can be made finite, and we could define
a ``kind of'' Turing machine with a \emph{circular tape} simulating it.
But this solution requires input size-dependent hardware.

It seems difficult to define a fault-tolerant sweeping 
behavior on a regular Turing machine needed to 
simulate cellular automaton, without recreating
an entire hierarchical construction---as we are doing here.


\section{Overview of the construction}

A Turing machine that simulates ``reliably'' any other
Turing machine even when it is subjected to isolated bursts of faults of constant size,
is given in~\cite{burstyTuring13}.
By \df{reliably} we mean that the 
simulated computation can be decoded from the history
of the simulating machine despite occasional damages.


\subsection{Isolated bursts of faults}\label{sec:bursts}

Let us give a brief overview of a machine \( M_{1} \) that
can withstand isolated bursts of faults, as most of its construction will be reused
in the probabilistic setting.

We break up the task of error correction into several 
problems to be solved.
The solution of one problem gives rise to another problem one, 
but the process converges.
\begin{description}
\item[Redundant information] The tape information of the simulated Turing machine
will be stored in a redundant form, more precisely in the form of a block code.
\item[Redundant processing] The block code will be decoded, the retrieved information 
will be processed, and the result recorded.
To carry out all this in a way that limits the propagation of faults, the tape will be split
into tracks that can be handled separately, and the major processing steps will be 
carried out three times within one work period.
\item[Local repair] All the above process must be able to recover from a local burst of faults.
For this, it is organized into a rigid, locally checkable structure
with the help of local addresses, and some other tools like sweeps and 
short switchbacks (zigzags).
The major tool of local correction, the local healing procedure, turns out to be the most
complex part of the construction.
\item[Disturbed local repair] A careful organization of the healing procedure
makes sure that even if a new burst interrupts it (or jumps into its middle),
soon one or two new invocations of it will finish the job (whenever needed).
\end{description}

Here is some more detail.
Each tape cell of the simulated machine \( M_{2} \) will be represented by a block of
size \( \Q \) of the simulating machine \( M_{1} \) called a \df{colony}.
Each step of \( M_{2} \) will be simulated by a computation of \( M_{1} \) called
a \df{work period}.
During this time, the head of \( M_{1} \) makes a number of sweeps over the
current colony and its right neighbor colony, decodes the represented cell symbols,
then computes and encodes the new symbols, and finally moves the head 
to the new position of the head of \( M_{2} \).

In order to protect information from the propagation of errors,
the tape of \( M_{1} \) is subdivided into \df{tracks}: each track corresponds to a 
\df{field} of a cell symbol of \( M_{1} \) viewed as a data record.
Each stage of computation will be repeated three times.
The results will be stored in separate tracks, and a final cell-by-cell majority vote
will recover the result of the work period from them.

All this organization is controlled by a few key fields, for example a field
called \( \Addr \) showing the position of each cell in the colony, and a field
\( \Sweep \) showing the last sweep of the computation (along with its direction)
that has been performed already.
The technically most challenging part of the construction is the protection of this
control information from bursts.

For example, a burst can reverse the head in the middle of a sweep.
Our goal is that such structural disruptions be discovered locally, so
we cannot allow the head to go far from the place where it was turned back.
Therefore the head's movement will not be straight even during a single
sweep: it will make frequent zigzags.
This will trigger the healing procedure if for example a turn-back is detected.

It is a significant challenge that the healing procedure
itself can be interrupted (or started) by a burst.

\begin{remark}
This description uses some words in an informal way.
Some of these will get precise definition later.
For example, the word \df{colony} will have at least two formal definitions.
From the point of view of the program (transition function), it is just a 
sequence of addresses from \( 0 \) to \( \Q-1 \).
From the point of view of the analysis of the behavior of the machine, it is a sequence of
actual adjacent tape cells with the address field having theses values.
\end{remark}


\subsection{Hierarchical construction}\label{sec:hier}

In order to build a machine resists faults 
occurring independently of each other with some small probability,
we take the approach suggested in~\cite{Kurd78},
and implemented in~\cite{Gacs1dim86} and~\cite{GacsSorg01}
for the case of one-dimensional cellular automata, with some ideas
from the tiling application of~\cite{DurandRomashShenTiling12}.
We will build a \df{hierarchy of simulations}:
machine \( M_{1} \) simulates machine \( M_{2} \) which 
simulates machine \( M_3 \), and so on.
For simplicity we assume all these machines have the same program,
and all simulations have the same block size.

One cell of machine \( M_3 \) is simulated by one colony of machine \( M_{2} \).
Correspondingly, one cell of \( M_{2} \) is simulated by
one colony of machine \( M_{1} \).
So one cell of \( M_3 \) is simulated by \( \Q^{2} \) cells of \( M_{1} \).
Further, one step of machine \( M_3 \) is simulated by one
work period of \( M_{2} \) of, say, \( O(\Q^{2}) \) steps.
One step of \( M_{2} \) is simulated by one work period of \( M_{1} \),
so one step of \( M_3 \) is simulated by \( O(\Q^{4}) \) steps of \( M_{1} \).

Per construction, machine \( M_{1} \) can withstand
bursts of faults whose size is \( \le \beta \) for some constant parameter \( \beta \), that
are separated by some \( O(\Q^{2}) \) fault-free steps.
Machines \( M_{2} \), \( M_3 \), \( \dots \) have the same program, so it
would be natural to expect that machine
\( M_{1} \) can withstand also some \emph{additional}, larger bursts
of size \( \le \beta \Q \) if those are separated by at least \( O(\Q^{4}) \) steps.

But a new obstacle arises.
On the first level, damage caused by a big burst spans several colonies.
The repair mechanism of machine \( M_{1} \) outlined in Section~\ref{sec:bursts} above 
is too local to recover from such extensive damage.
This cannot be allowed, since then the whole hierarchy would stop working.
So we add a new mechanism to \( M_{1} \) that, more modestly,
will just try to restore a large enough portion of the
tape, so it can go on with the simulation of \( M_{2} \), even if all 
original information was lost.
For this, \( M_{1} \) may need to rewrite an area as large as a few colonies.
This will enable the low-level healing procedure of 
machine \( M_{2} \) to restore eventually a higher-level order.

All machines above \( M_{1} \) in the hierarchy are
``virtual'': the only hardware in the construction is machine \( M_{1} \).
Moreover, they will not be ordinary Turing machines, but \df{generalized} ones,
with some new features that are not needed on the lowest level but seem necessary
in a simulated Turing machine: for example they
allow a positive distance between neighboring tape cells.

A tricky issue is ``forced self-simulation'': while we are constructing machine \( M_{1} \)
we want to give it the feature that it will simulate a machine \( M_{2} \) that
works just like \( M_{1} \).
The ``forced'' feature means that this simulation should
work without any written program (that could be corrupted).

This will be achieved by
a construction similar to the proof of the Kleene's fixed-point 
theorem (also called recursion theorem).
We first fix a (simple) programming language to express the transition
function of a Turing machine.
We write an interpreter for it in this same language (just as compilers for the 
C language are sometimes written in C).
The program of the transition function of \( M_{2} \)
(essentially the same as that of \( M_{1} \))
in this language, is a string that will be
``hard-wired'' into the transition function of \( M_{1} \), 
so that \( M_{1} \), at the start of each work period, can write
it on a working track of the current colony.
Then the work period will interpret it, 
applying it to the data found there, resulting
in the simulation of \( M_{2} \).

In this way, an infinite sequence of simulations arises, in order
to withstand larger and larger but sparser and sparser bursts of faults.

Since the \( M_{1} \) uses the universal interpreter, which in turns
simulates the same program, it is natural to ask
how  machine \( M_{1} \) simulates a given Turing machine \( G \) that does the 
actual useful computation?
For this task, we set aside a separate track 
on each machine \( M_{i} \), on which some arbitrary other Turing machine can be
simulated.
The higher the level of the machine \( M_{k} \) that performs this
``side-simulation'', the higher the reliability.
Thus, only the simulations \( M_{k}\to M_{k+1} \) are forced, without program
(that is a hard-wired program):
the side simulations can rely on written programs, since the firm
structure in the hierarchy \( M_{1},M_{2},\dots \) will support them reliably.



\subsection{From combinatorial to probabilistic noise}

The construction we gave in the previous subsection
was related to increasing bursts that are not frequent.
In essence, that noise model is combinatorial.
To deal with probabilistic noise combinatorially,
we stratify the set of faulty times \( \Noise \) as follows.
For a series of parameters \( \beta_{k}, V_{k} \),
we first remove ``isolated bursts'' of type \( \pair{\beta_{1}}{V_{1}} \) of elements of this set.
(The notion of ``isolated bursts'' of type \( \pair{\beta}{V} \)
will be defined appropriately.)
Then, we remove isolated bursts of type \( \pair{\beta_{2}}{V_{2}} \) from the remaining set,
and so on.
It will be shown that with the appropriate choice of parameters, with probability 1,
eventually nothing is left over from the set \( \Noise \).

A composition of two reliable simulations is even more reliable.
We will see that a sufficiently large hierarchy of such
simulations resists probabilistic noise.


\subsection{Difficulties}\label{sec:novelties}

Let us spell out some of the main problems that the paper deals with, 
and some general ways they will be solved or avoided.
Some more specific problems will be pointed out later, along with their solution.



\begin{description}

\item[Non-aligned colonies] A large burst of \( M_{1} \) can modify the order of
entire colonies or create new ones with gaps between them.

To overcome this problem conceptually, we 
introduce the notion of a \df{generalized Turing machine}
allowing for non-adjacent cells.
Each such machine has a parameter \( \B \) called the \df{cell body size}.
The cell body size of a Turing machine in Section~\ref{sec:TM} would still remain
\( 1 \).

    \item[No structure] What to do when the head is in a middle of an empty area
       where no structure exists?
To ensure reliable passage across such areas,
we will try to keep everything filled with cells, even if these are
not part of the main computation.

\item[Clean areas]
    Noise can create areas over
which the predictability of the simulated machine is limited.
In these areas the (on this level) invisible structure
of the underlying simulation may be destroyed.
These areas should not simply be considered blank, since
blankness implies predictable behavior.
We call these areas of  ``disordered'', and call the complement \df{clean}.
Note that transition properties will allow
making conclusions from cleanness, not from disorder.
Examples below will show that when the head enters disorder, it can be ``sucked in''.


\item[Extending cleanness]
 The definition of the generalized
Turing machine stipulates a certain ``magical'' extension of clean intervals,
and also the appearance of a clean ``hole'' around the head 
whenever it passes a certain amount of time in a small interval,
and the cleaning of an interval if it is passed noiselessly a certain number of times.
(Of course, these properties needs to be implemented in simulation, which is one of the
main burdens of the actual construction.)
Once an area is cleaned, it will be re-populated with new cells.
Their content is not important, what matters is the restoration of predictability.

\item[Rebuilding] If local repair fails, a special rule will be invoked that reorganizes a
larger part of the tape (of the size of a few colonies instead of only a few cells).
This is the mechanism implementing the ``magical'' restoration on the next level.

\end{description}

The following examples show the difficulties that necessitate some of the complexities of the
construction and proof.

\begin{example}[Need for zigging]\label{xmp:zig}
  When the head works in a colony of machine \( M_{1} \) performing a simulation of a cell of machine \( M_{2} \),
  it works in sweeps across the colony.
  But a burst of faults could reverse the head in the middle of a sweep, leaving it uncompleted.
  This way a local burst could create non-local inconsistency.
\end{example}

To handle the problem of Example~\ref{xmp:zig}, the head will proceed in zigzags: every
  step advancing the head in the simulation
  is followed by \( \Z \) steps of going backward and forward again, just checking consistency
  (and starting a healing process if necessary).
  The parameter \( \Z \) will be chosen appropriately.

\begin{example}[Need for feathering]\label{xmp:feather}
  Some big noise can create a number of intervals \( I_{1},I_{2},\dots,I_{n} \)
  consisting of colonies of machine \( M_{1} \), each interval with its own simulated head,
  where the neighboring intervals are in no relation to each other.
  When the head is about to return from the end of \( I_{k} \)
  (never even to zig beyond it, see the discussion after Example~\ref{xmp:zig}),
  a burst can carry it over to \( I_{k+1} \) where
  the situation may be symmetric: it will continue the simulation that \( I_{k+1} \) is performing.
  (The rightmost colony of \( I_{k} \) and the leftmost colony of \( I_{k+1} \) need not be complete:
  what matters is only that the simulation in \( I_{k} \) would not bring the head beyond its right end,
  and the simulation in \( I_{k+1} \) would not bring the head beyond its left end.)

  The head can be similarly captured to \( I_{k+2} \), then much later back from \( I_{k+1} \) to \( I_{k} \),
  and so on.
  This way the restoration of structure in \( M_{2} \) may be delayed too long.
\end{example}

The device by which we will mitigate the effect of this kind of capturing is another property of the
the movement of the head which will call \df{feathering}:
if the head turns back from a tape cell then next time it must go beyond.
This requires a number of adjustments to the program (see later).

\begin{example}[Two slides over disorder]
Consider two levels of simulation as outlined in Section~\ref{sec:hier}: 
machine \( M_{1} \) simulates \( M_{2} \) which simulates \( M_{3} \).
The tape of \( M_{1} \) is subdivided into colonies of size \( \Q_{1} \).
The tape content of the 
current colony of level 1 represents not only the content of the currently
observed tape cell of machine \( M_{2} \), but also its state.
A burst on level 1 has size \( O(1) \), while a burst on level 2 has size \( O(\Q_{1}) \).

Suppose that \( M_{1} \) is performing a simulation in colony \( C_{0} \).
An earlier big burst may have created a large interval \( D \) of disorder
on the right of \( C_{0} \), even reaching into \( C_{0} \).
For the moment, let \( C_{0} \) be called a \df{victim} colony.
Assume that the left edge of \( D \) represents the last stage of a transfer operation to the right neighbor 
colony \( C_{0}+Q_{1} \).
When the head, while performing its work in \( C_{0} \), moves close to its right end, a small burst may 
carry it over into \( D \).
There it will be ``captured'', and continue the (unintended) right transfer operation.
This can carry the head, over several successful colony simulations in \( D \), to some
victim colony \( C_{1} \) on the right from which it will be captured to the right similarly.
This can continue over new and new victim colonies \( C_{i} \) (with enough space between them to allow for
new bursts to occur), all the way inside the disorder \( D \).
So the \( M_{2} \) cells in \( D \) will fail to simulate \( M_{3} \).

After a while the head may return to the left in \( D \)
(performing the simulations in its colonies).
When it gets at the right end of a victim colony \( C_{i} \), a burst might move it back there.
There is a case when \( C_{i} \) now can just continue its simulation and then send the head
further left: when before the head was captured on its right,
it was in the last stage of simulating a left turn of the head of machine \( M_{2} \).

In summary, 
a big burst can create a disordered area \( D \) which can capture the head and on which the head can slide
forward and back without recreating any level of organization beyond the second one.
\end{example}

\begin{example}[Unbounded number of slides over disorder]
  Let us describe a certain ``organization'' of a disordered area in which an unbounded number of passes
  may be required to restore order.
For some \( n<0 \), let the cells of \( M_{1} \) at positions
\( x_{-Q_{1}},\dots,x_{n} \), where \( x_{i+1}=x_{i}+B_{1} \),
represent part of a healthy colony \( C(x_{-Q_{1}}) \) starting at \( x_{-Q_{1}} \), where \( x_{n} \)
is the rightmost cell of \( C(x_{-Q_{1}}) \)
to which the head would come in the last sweep before
the simulation will move to the \emph{left} neighbor colony \( C(x_{-2Q_{1}}) \).
Let them be followed by cells \( x_{n+1},\dots, x_{Q_{1}-1},\dots\)
which represent the last sweep of a transfer operation to the \emph{right} neighbor colony \( C(x_{0}) \).
If the head is in cell \( x_{n} \), a burst can transfer it to \( x_{n+1} \).
The cell state of \( M_{2} \) simulated by \( C(x_{-Q_{1}}) \) need to be in \emph{no relation} to 
the cell state of \( M_{2} \) simulated by \( C(x_{0}) \).
This was a capture of the head by a burst of \( M_{1} \) across the point 0, to the right.

We can repeat the capture scenario, say around points \( i Q_{1}Q_{2} \),
and this way cells of \( M_{3} \) simulated by \( M_{2} \) (simulated by \( M_{1} \))
can be defined arbitrarily, with no consistency needed between any two neighbors.
(We did not write \( i Q_{1} \) just in case bursts are not allowed in neighboring colonies.)
In particular, we can define them to implement a \emph{leftward} capture scenario
via level 3 bursts at points \( i Q_{1}Q_{2}Q_{3}Q_{4} \), allowing to simulate arbitrary cells of \( M_{5} \)
with no consistency requirement between neighbors.
So \( M_{5} \) could again implement a rightward capture scenario, and so on.
In summary, a malicious arrangement of disorder and noise allows \( k \) passes
after which the level of organization is still limited to level \( 2 k + 1 \).
\end{example}

Our construction will ensure that, on the other hand,
\( O(k) \) passes (free of \( k \)-level will restore organization to level \( k \).
This will property of the construction will be incorporated into our definition of a generalized
Turing machine.


\section{Notation}\label{sec:notation}

Most notational conventions given here are common; some other ones will
also be useful.

\begin{description}

\item [Natural numbers and integers] 
By \( \bbZ \) we denote the set of integers.
\begin{align*}
   \bbZ_{>0}&=\setOf{x}{x\in \bbZ,\;  x>0}, \\
   \bbZ_{\ge 0}&=\bbN=\setOf{x}{x\in \bbZ,\;  x\ge 0}.
\end{align*}

\item [Intervals]
We use the standard notation for intervals:
\begin{align*}
   \clint{a}{b}&=\setOf{x}{a\le x \le b},\quad \lint{a}{b}=\setOf{x}{a\le x < b}, \\
   \rint{a}{b}&=\setOf{x}{a< x \le b}, \quad  \opint{a}{b}=\setOf{x}{a< x < b}.
\end{align*}
We will also write \( \lint{a}{b} \) in place of \( \lint{a}{b}\cap \bbZ \), 
whenever this leads to no confusion.
Instead of \( \lint{x+a}{x+b} \), sometimes we will write 
\begin{align*}x + \lint{a}{b}.\end{align*}

\item [Ordered pairs]
Ordered pairs are also denoted by \( \pair{a}{b} \),
but it will be clear from the context whether we are
referring to an ordered pair or open interval.

\item [Comparing the order of a number and an interval]
For a given number \( x \) and interval \( I \), we
write
\begin{align*} x \ge I \end{align*}
if for every \( y\in I \),  \( x \ge y \).

\item [Distance]
The distance between two real numbers \( x \) and \( y \) is defined
in a usual way:
\begin{align*}
    d(x,y)= \abs{x-y}.
\end{align*}

The \df{distance of a point \( x \) from interval \( I \)}  is
\begin{align*}
    d(x,I)= \min_{y\in I}d(x,y).
\end{align*}

\item [Ball, neighborhood, ring, stripe]
A \df{ball of radius \( r>0 \), centered at \( x \)} is
\begin{align*}
    B(x,r)= \setOf{y}{d(x,y)\le r}.
\end{align*}
An \df{\( r \)-neighborhood of interval } \( I \) is
\begin{align*}
    \setOf{x}{d(x,I)\le r}.
\end{align*}
An \df{\( r \)-ring} around interval \( I \) is
\begin{align*}
    \setOf{x}{d(x,I)\le r \txt{ and } x \notin I}.
\end{align*}
An \df{\( r \)-stripe to the right of interval \( I \)} is
\begin{align*}
    \setOf{x}{d(x,I)\le r \txt{ and } x \notin I \txt{ and } x>I}.
\end{align*}

\item[Logarithms] Unless specified differently,
the base of logarithms throughout this work is 2.

\end{description}


\section{Specifying a Turing machine}\label{sec:specifying}

Let us introduce the tools allowing to describe the reliable Turing machine.

\subsection{Universal Turing machine}\label{sec:UTM}

We will describe our construction in terms of
universal Turing machines,
operating on binary strings as inputs and outputs.
We define universal Turing machines in a way that allows
for rather general ``programs''.

 \begin{definition}[Standard pairing]
For a (possibly empty) binary string\\ \( x=x(1)\dotsm x(n) \) let us introduce the map
 \[
   \ang{x} = 0^{\abs{x}}1 x,
 \]
Now we encode pairs, triples, and so on, of binary strings as follows:
 \begin{align*}
        \ang{s,t} &=\ang{s}t,
\\ \ang{s,t,u} &= \ang{\ang{s,t},u},
 \end{align*}
and so on.

From now on, we will assume that our alphabets \( \Sigma \), \( \Gamma \)
are of the form \( \Sigma=\{0,1\}^{s} \), \( \Gamma=\{0,1\}^{g} \), that is 
our tape symbols and machine states are viewed as binary strings of a certain length.
Also, if we write \( \ang{i,u} \) where \( i \) is some number, it is understood
that the number \( i \) is represented in a standard way by a binary string.
\end{definition}

\begin{definition}[Computation result, universal machine]
 Assume that a Turing machine \( M \) starting on binary \( x \),
 at some time \( t \)
 arrives at the first time at some final state.
 Then we look at the longest (possibly empty)
 binary string to be found starting at position
 0 on the tape, and call it the \df{computation result} \( M(x) \).
We will write
 \begin{align*}
   M(x,y)=M(\ang{x,y}),\quad M(x,y,z)=M(\ang{x,y,z}),
 \end{align*}
and so on.

A Turing machine \( U \) is called \df{universal} 
among Turing machines with
binary inputs and outputs, if for every Turing machine \( M \),
there is a binary string \( p_{M} \) such that for all \( x \) we have
\( U(p_{M},x)=M(x) \).
(This equality also means that the computation denoted on the left-hand side 
reaches a final state if and only if the computation on the right-hand side does.)
\end{definition}

Let us introduce a special kind of universal Turing machines, to be
used in expressing the transition functions of other Turing machines.
These are just the Turing machines for which the so-called \( s_{mn} \) theorem
of recursion theory holds with \( s(x,y)=\ang{x,y} \).

\begin{definition}[Flexible universal Turing machine]\label{def:univ-TM}
A universal Turing machine will be called \df{flexible} if 
whenever \( p \) has the form \( p=\ang{p',p''} \) then
\begin{align*}
 U(p,x)= U(p',\ang{p'',x}).
 \end{align*}
Even if \( x \) has the form \(x =\ang{x',x''} \), this definition chooses
\( U(p',\ang{p'',x}) \) over \( U(\ang{p,x'},x'') \), that is starts with 
parsing the first argument
(this process converges, since \( x \) is  shorter than \( \ang{x,y} \)).
\end{definition}

It is easy to see that there are flexible universal Turing machines.
On input \( \ang{p,x} \),
a flexible machine first checks whether its ``program'' \( p \) 
has the form \( p=\ang{p',p''} \).
If yes, then it applies \( p' \) to the pair \( \ang{p'',x} \).
(Otherwise it just applies \( p \) to \( x \).)

\begin{definition}[Transition program]
  Consider an arbitrary Turing machine \( M \) with alphabet
\( \Sigma \), and transition function \( \tau \).
A binary string \( \pi \) will be called a \df{transition program} of \( M \) if
whenever \( \tau(\va)=(\va',j) \) we have
 \begin{align*}
 U(\pi,\va)=\ang{\va,j}.
 \end{align*}
We will also require that the computation induced by the program makes
\( O(\abs{\pi}+\abs{a}) \) left-right turns, over a length tape \( O(\abs{\pi}+\abs{a}) \).
\end{definition}

The transition program just provides a way to compute
the (local) transition function of \( M \) by the universal machine,
it does not organize the rest of the simulation.

\begin{remark}
 In the construction of universal Turing machines provided by the textbooks
(though not in the original one given by Turing), the program is generally a string
encoding a table for the transition
function \( \tau \) of the simulated machine \( M \).
Other types of program are imaginable: some simple transition functions can
have much simpler programs.
However, our fixed machine is good enough (similarly to the optimal machine
for Kolmogorov complexity).
If some machine \( U' \) simulates \( M \) via a
very simple program \( q \), then
 \begin{align*}
     M(x)=U'(q,x) = U(p_{U'},\ang{q,x}) = U(\ang{p_{U'},q},x),
 \end{align*}
so \( U \) simulates this computation via the program \( \ang{p_{U'},q} \).
\end{remark}

\subsection{Rule language}\label{sec:language}

In what follows we will describe the generalized Turing machines \( M_{k} \) for all \( k \).
They are all similar, differing only in the parameter \( k \); the most important activity
of \( M_{k} \) is to simulate \( M_{k+1} \).
The description will be uniform, except for the parameter \( k \).
We will denote therefore \( M_{k} \) simply by \( M \), and \( M_{k+1} \)  by \( M^{*} \).
Similarly we will denote the block size \( \Q_{k} \) of the block code of the 
simulation simply by \( \Q \).

Instead of writing a huge table describing the transition function \( \tau_{k}=\tau \),
we present the transition function as a set of \df{rules}.
It will be then possible to write one \emph{interpreter} program that carries
out these rules; that program can be written for some fixed flexible 
universal machine \( \Un \).

Each rule consists of some (nested) conditional statements,
similar to the ones seen in an ordinary program:
 ``\textbf{if} \textit{condition} \textbf{then} \textit{instruction}
\textbf{else} \textit{instruction}'', 
where the condition
is testing values of some fields of the state and the observed cell, and
the instruction can either be elementary, or itself a conditional statement. 
The elementary instructions are an \df{assignment} of a value to a field
of the state or cell symbol, or a command to move the head.
Rules can call other rules, but these calls will never form a cycle.
Calling other rules is just a shorthand for nested conditions.

Even though rules are written like procedures of a program,
they describe a single transition.
When several consecutive statements are given, then they
%(almost always)
change different fields of the state or
cell symbol, so they can be executed simultaneously.
% Otherwise and in general, even if a field is updated in
% some previous statement, in all following statements that use
% this field, its old value is considered.

Assignment of value \( x \) to a field \( y \) of the state or cell symbol will
be denoted by \( y \gets x \).
We will also use some conventions introduced by the C language:
namely,
\( x\gets x+1 \) and \( x\gets x-1 \) are abbreviated to \( \increment{x} \) and
\( \decrement{x} \) respectively.

Rules can also have parameters, like \( \ruSwing(a,b,u,v) \).
Since each rule is called only a constant number of times in the whole program,
the parametrized rule can be simply seen as a shorthand.

Mostly we will describe
the rules using plain English, but it should always be clear that they
are translatable into such rules.


% \section{Fields}\label{sec:fields}

\begin{sloppypar}
For the machine \( M \) we are constructing, each tape symbol will 
be a tuple \( q=(q_{1},q_{2},\dots,q_{k}) \),
where the individual elements of the tuple will be called \df{fields}, and will
have symbolic names.
For example, we will have fields \( \Addr \) and \( \Drift \),
and may write \( q_{1} \) as \( q.\Addr \) or just \( \Addr \), 
\( q_{2} \) as \( q.\Drift \) or \( \Drift \), and so on.
\end{sloppypar}

In what follows we describe some of the most important fields we will use;
others will be introduced later.

A properly formatted configuration of \( M \) splits the tape into blocks of \( \Q \)
consecutive cells called \df{colonies}.
One colony of the tape of the simulating
machine represents one cell of the simulated machine.
The two colonies that correspond to the pair of cells that the
simulated machine is scanning is called the \df{base colony-pair}
(a precise definition will be based on the actual history of the work of \( M \)), and its
members will be called the \df{left} and \df{right base colony}.
Sometimes the left base colony will just be called the \df{base colony}.
Most of the computation proceeds over the base colony-pair.
The direction of the simulated head movement, once figured out by the computation,
is called the \df{drift}.
Two neighbor colonies may not be adjacent, in which case the cells will form
a \df{bridge} between them.
There will be a field whose value is called the \df{mode}:
 \begin{align*}
   \Mode\in\{ \Normal,\Healing, \Rebuilding \}.
 \end{align*}
 If for example its value in the current cell is \( \Normal \) we will say that the
 computation is in \df{normal} mode.
 In this case, the machine is engaged in the regular business of simulation.
The \df{healing} mode tries to correct some local fault due to a couple of neighboring
bursts, while the \df{rebuilding} mode attempts to restore the colony structure
on the scale of a couple of colonies.

The array of values of the same field of the cells will be called a \df{track}.
Thus, we will talk about the \( \Hold \) track of the tape, corresponding to the
\( \Hold \) field of cells.

% The basic fields of the state and of cells are listed in Section~\ref{sec:fields-list}
% with some hints of
% their function (this does not replace our later definition of the transition function).
Each field of a cell has also a possible value
\( \emptyset \) whose approximate meaning is ``undefined''.

Some fields and parameters are important enough to introduce them right away.
The 
\begin{align*}
   \Info
 \end{align*}
track of a colony of \( M \)
contains the string that encodes the content of the simulated cell of \( M^{*} \).
\begin{align*}
 \Prog
 \end{align*}
track stores the program of \( M^{*} \), in an appropriate form 
to be interpreted by the simulation.
The field 
 \begin{align*}
  \Addr
 \end{align*}
of the cell shows the position of the cell in its colony:
it takes values in \( \lint{0}{Q} \).
The direction in \( \{-1,1\} \) in which the simulated head moves will be denoted by
 \begin{align*}
   \Drift.
 \end{align*}
The number of the last sweep of the work period will depend on the drift \( d \), 
and will be denoted by 
\begin{align}\label{eq:Last}
   \Last(d).
 \end{align}
The
 \begin{align*}
 \Sweep
 \end{align*}
field counts the sweeps that the head makes during the work period.
In calculating parameters, we will make use of  
\begin{align}\label{eq:V}
   \V=\max(\Last(-1),\Last(1)).
 \end{align}
Cells will be designated as belonging to a number of possible \df{kinds}, signaled by the
field 
\begin{align*}
     \Kind
 \end{align*}
with values
\begin{align*}
          \Member_{0},\Member_{1},\Bridge,\Vacant, \Stem.
\end{align*}
Here is a description of the role of these cell kinds.
Cells of the base colony-pair are of type \( \Member_{0} \) and \( \Member_{1} \) respectively.
If the base colony pair is not adjacent then there will be a \df{bridge} of \( <\Q \)
adjacent cells of type \( \Bridge \) between them.

\begin{definition}\label{def:extends}
We will say that the bridge \df{extends} a colony if its cells are adjacent to it.  
\end{definition}
Under normal conditions,  
if the \( \Drift \) track on the bridge has value \( 1 \) then it is extending the left base
colony, otherwise is extending the right base colony.
Cells of the bridge will continue the addressing (modulo \( \Q \)) of the colony it extends.

The stem kind is sometimes convenient when some cells need to be created temporarily
that do not participate in any known colony structure.
We will also try to keep all areas between colonies filled with (not necessarily adjacent)
stem cells.
For example the computation may find that a colony does not properly encode
a tape cell by the required error-correcting code.
Then we want to ``kill'' the whole colony.
This will happen by turning the kind of each of its cells to \( \Stem \).
% The reason is that even the ``vacuum'' taking the place of the colony needs some
% structure allowing the head to traverse it later in a reliable way.

% It is useful to group some of the fields especially important for the simulation structure into
% a ``super'' field,
%     \begin{align}\label{eq:Core}
%        \Core=(\Addr, \Sweep, \Drift, \Kind), 
%         \cCore=(\cAddr, \cSweep, \cDrift, \cKind). %, \cVisSw).
%     \end{align}
%(where the field \( \cVisSw \) will also be explained later).

%     \item \( \Af \) field of a cell will be set to true if
%     this cell belongs to a colony that ``represents'' a cell
%     that wants to become ``vacant''.
%     (We will explain  this notion later.)

  % The fields used in healing mode are all collected as subfields of the field
  %   \( \Heal \) of the state.
  %   They will be introduced in the definition of the healing rule.

During healing, some special fields of the state and cell are used, they will be subfields of 
the field
 \begin{align}\label{eq:Heal}
   \Heal.
 \end{align} 
In particular, there will be a \( \Heal.\Sweep \) field.

Healing only changes the cells that need to be changed.
But during rebuilding, the tape will also be used.
we will work with subfields of the field \( \Rebuild \), 
and a cell will be called \df{marked for rebuilding} if \( \Rebuild.\Sweep\ne 0 \).


\section{Exploiting structure in the noise}\label{sec:noise}

\subsection{Sparsity}\label{sec:sparsity}
Let us introduce a technique connecting the combinatorial and probabilistic
noise models.

\begin{definition}[Centered rectangles, isolation]
Let \( \vek{r}=\pair{r_{1}}{r_{2}} \), \( r_{1}, r_{2}\ge 0 \),
be a two-dimensional nonnegative vector.
An \df{rectangle} of radius \( \vek{r} \) \df{centered} at \( \vek{x} \) is
\begin{align}\label{eq:ball1}
  B(\vek{x},\vek{r}) = \setOf{\vek{y}}{\abs{y_{i} - x_{i}} \le r_{i}, i=1,2}.
\end{align}  
Let \( E\subseteq \bbZ^{2} \) be a two-dimensional set.
A point \( \vek{x} \) of \( E \) is \df{\( \pair{\vek{r}}{\vek{r}^{*}} \)-isolated} if
\begin{align*}
  E \cap B(\vek{x},\vek{r}^{*})\subseteq B(\vek{x}, \vek{r}).
 \end{align*}
Set \( E \) is \df{\( (\vek{r}, \vek{r}^{*}) \)-sparse} 
if \( D(E, \vek{r}, \vek{r}^{*})=\emptyset \), that is 
it consists of \( (\vek{r}, \vek{r^{*}}) \)-isolated points.
Let
\begin{align}
  D(E,\vek{r}, \vek{r}^{*}) =
     \setOf{\vek{x}\in E}{\vek{x} \txt{ is not } (\vek{r}, \vek{r}^{*})\txt{-isolated
  from } E}.
\end{align}
\end{definition}

\begin{definition}[Sparsity]\label{def:sparsity}
Let
\begin{align}\label{eq:beta}
 \beta\ge 9
 \end{align}
 be a parameter, and let 
\begin{align*}
  0<\B_{1}<\B_{2}<\dotsm,\quad
  \Tu_{1}<\Tu_{2}<\dotsm,\quad
  1\le\gamma_{1}\le\gamma_{2}\le\dotsm
\end{align*}
be sequences of positive integers to be fixed later.
For a two-dimensional set \( E \), let \( E^{(1)} = E \).
For \( k>1 \) we define recursively:
\begin{align}\label{eq:noise^k}
    E^{(k+1)} = D(E^{(k)}, \beta\pair{\B_{k}}{\Tu_{k}}, \gamma_{k}\pair{\B_{k+1}}{\Tu_{k+1}}).
\end{align}
Set \( E^{(k)} \) is called the \df{\( k \)-th residue} of \( E \).
It is \( k \)-\df{sparse} if \( E^{(k+1)}=\emptyset \).
It is simply \df{sparse} if \( \bigcap_{k}E^{(k)}=\emptyset \).

When \( E=E^{(k)} \) and \( k \) is known
then we will denote \( E^{(k+1)} \) simply by \( E^{*} \).
\end{definition}

The following lemma connects the above defined sparsity notions to the requirement
of small fault probability.
It is formulated somewhat redundantly, for easier application.

\begin{lemma}[Sparsity]\label{lem:sparsiness}
Let \( \Q_{k} = \B_{k+1}/\B_{k} \), \( \U_{k} = \Tu_{k+1}/\Tu_{k} \), and
\begin{align}\label{eq:growth-assumption}
  % \lim_{k\rightarrow\infty}\frac{\log(\U_{k} \Q_{k})}{1.5^k}=0.
  \lim_{k\rightarrow\infty}\frac{\log(\gamma_{k}\U_{k} \Q_{k})}{1.5^k}=0.
\end{align}
For sufficiently small \( \eps \), for every \( k\ge 1 \) the following holds.
Let \( E\subseteq \bbZ\times \bbZ_{\ge 0} \)
be a random set with the property that each pair \( \pair{p}{t} \) belongs to \( E \)
independently from the other ones with probability \( \le \eps \).

Then for each point \( \vek{x} \)  and each \( k \),
 \begin{align*}
   \Pbof{B(\vek{x},(\B_{k}, \Tu_{k}))\cap E^{(k)}\neq\emptyset} <2\eps \cdot 2^{-1.5^{k}}.
 \end{align*}
As a consequence, the set \( E \) is sparse with probability 1.
\end{lemma}

\begin{proof}
Let \( k=1 \).
Rectangle \( B(\vek{x},(\B_{1}, \Tu_{1})) \) is a single point, hence
the probability of our event is \( <\eps \).
Let us prove the inequality by induction, for \( k+1 \).

Note that our 
event depends at most on the rectangle \( B(\vek{x},3(\B_{k}, \Tu_{k})) \).
Let
\begin{align*}
   p_{k}=2\eps\cdot 2^{-1.5^{k}}.
\end{align*}
Suppose \( \vek{y} \in E^{(k)}\cap B(\vek{x},\gamma_{k}(\B_{k+1}, \Tu_{k+1})  ) \).
Then, according to the definition of \( E^{(k)} \),  there is a point
\begin{align}\label{eq:sparse-as}
 \vek{z} \in
 B(\vek{y},\gamma_{k}(\B_{k+1},\Tu_{k+1}))\cap E^{(k)}\setminus B(\vek{y},\beta(\B_{k}, \Tu_{k})).
 \end{align}
Consider a standard partition of the (two-dimensional) space-time into
rectangles \( K_{p}=\vek{c}_{p}+\lint{-\B_{k}}{\B_{k}}\times \lint{-\Tu_{k}}{\Tu_{k}} \)
with centers \( \vek{c}_{1},\vek{c}_{2},\dots \).
The rectangles \( K_{i},K_{j} \) containing \( \vek{y} \) and \( \vek{z} \)
respectively intersect \( B(\vek{x}, 2\gamma_{k}(\B_{k+1}, \Tu_{k+1})) \).
The triple-size rectangles 
\( K'_{i}=c_{i} + \lint{-3\B_{k}}{3\B_{k}}\times \lint{-3\Tu_{k}}{3\Tu_{k}} \) and
\( K'_{j} \) are disjoint, since \eqref{eq:beta} and~\eqref{eq:sparse-as} imply
 \( \abs{y_{1} - z_{1}}>\beta\B_{k} \) and \( \abs{y_{2} - z_{2}}>\beta\Tu_{k} \).

The set \( E^{(k)} \) must intersect two rectangles \( K_{i} \),
\( K_{j} \) of size \( 2(\B_{k}, \Tu_{k}) \) separated by at least \( 4(\B_{k}, \Tu_{k}) \),
of the big rectangle \( B(\vek{x},2\gamma_{k}(\B_{k+1}, \Tu_{k+1})) \).

By the inductive hypothesis, the event \( \cF_{i} \) that
\( K_{i} \) intersects \( E_{k} \) has probability bound \( p_{k} \).
It is independent of the event \( \cF_{j} \), since these events depend
only on the triple size disjoint rectangles \( K'_{i} \) and \( K'_{j} \).

The probability that both of these events hold is at most \( p_{k}^{2} \).
The number of possible rectangles
\( K_{p} \) intersecting \( B(\vek{x},2\gamma_{k}(\B_{k+1}, \Tu_{k+1})) \) is
at most
\( C_{k}:=((2\gamma_{k}^{2}\U_{k} \Q_{k})+2)^{2} \), so the number of possible pairs of rectangles
is at most \( C_{k}^{2}/2 \), bounding the probability of our event by
 \begin{align*}
   C_{k}^{2}p_{k}^{2}/2
    &=
      2 C_{k}^{2}\eps^{2} 2^{-1.5^{k+1}}\cdot 2^{-0.5\cdot 1.5^{k}}
   \\ &=2\eps 2^{-1.5^{k+1}} \cdot \eps
        C_{k}^{2}2^{-0.5\cdot 1.5{k}}.
 \end{align*}
Since \( \lim_{k}\frac{\log{(\gamma_{k}\U_{k} \Q_{k})}}{1.5^k}=0 \),
%Since \( \lim_{k}\frac{\log{(\U_{k} \Q_{k})}}{1.5^k}=0 \),
the last factor is \( \le 1 \) for sufficiently small  \( \eps \).
\end{proof}

In our construction we will choose
\begin{align*}
  \gamma_{k}=\Omega(k^{2}),
  \quad \Q_{k} = \Omega(\gamma_{k}^{2}),
  \quad \U_{k} = \Omega(\Q_{k}^{2}),
\end{align*}
while still satisfying~\eqref{eq:growth-assumption}.

\subsection{Error-correcting code}\label{sec:coding}

Let us add error-correcting features to block codes introduced in
Definition~\ref{def:codes}.

\begin{sloppypar}
\begin{definition}[Error-correcting code]\label{def:err-code}
A block code is \( (\beta,t) \)-\df{burst-error-correcting},
if for all \( x\in\Sigma_{2} \), \( y\in\Sigma_{1}^{\Q} \) we
have \( \psi^{*}(y)=x \) whenever \( y \) differs from
\( \psi_{*}(x) \) in at most \( t \) intervals of size \( \le\beta \).

For such a code, we will call a word \( y\in\Sigma_{1}^{\Q} \) is \( r \)-\df{compliant}
if it differs from a codeword of the code by at most \( r \) intervals of size \( \le\beta \).
\end{definition}
  \end{sloppypar}

\begin{example}[Repetition code]\label{xmp:tripling}
  Suppose that \( \Q\ge 3\beta \) is divisible by 3,
  \( \Sigma_{2}=\Sigma_{1}^{\Q/3} \), \( \psi_{*}(x)=xxx \).
  Let \( \psi^{*}(y) \) be obtained as follows.
  If \( y=y(1)\dots y(\Q) \), then \( x=\psi^{*}(y) \) is defined as follows:
    \( x(i)=\maj(y(i),y(i+\Q/3),y+2\Q/3) \).
    For all \( \beta\le \Q/3 \), this is a
    \( (\beta,1) \)-burst-error-correcting code.

    If we repeat 5 times instead of 3, we get a \( (\beta,2) \)-burst-error-correcting
    code.
    Let us note that there are much more efficient such codes than just repetition.
 \end{example}

Consider a Turing machine 
\( (\Gamma, \Sigma,\tau, q_{\start},F) \) (actually a generalized one to be defined later)
simulating some Turing machine \( (\Gamma^{*}, \Sigma^{*},\tau^{*}, q^{*}_{\start},F^{*}) \).
We will assume that \( \Gamma^{*}\cup\set{\emptyset} \),
and the alphabet \( \Sigma^{*} \) are subsets of the set of  binary strings
\( \{0,1\}^{l} \) for some \( l<\Q \) (we can always ignore some states or tape
symbols, if we want).

\begin{definition}[Interior]\label{def:interior}
Let 
\begin{align*}
  \PadLen 
\end{align*}
be a parameter to be defined later (in~\eqref{eq:PadLenDef}).
Consider an interval \( I \) of neighbor cells.
A cell belongs to the \df{interior} of \( I \) if there are at least \( \PadLen \) neighbors between it 
and the complement of \( I \).
In particular, we will talk about the interior of a colony.
\end{definition}

We will store the coded information in the interior of the colony, since it is more exposed 
to errors near the boundaries.
So let \( (\upsilon_{*}, \upsilon^{*}) \) be a \( (\beta,2) \)-burst-error-correcting block code
\begin{align*}
  \upsilon_{*}: \{0,1\}^{l} \cup \set{\emptyset}
   \to\{0,1\}^{(\Q-2\cdot\PadLen)\B}.
\end{align*}
We could use, for example, the repetition code of Example~\ref{xmp:tripling}.
Other codes are also appropriate, but we require that they have some fixed
programs \( p_{\encode} \), \( p_{\decode} \)
on the universal machine \( \Un \), in the following sense:
 \begin{align*}
   \upsilon_{*}(x)=\Un(p_{\encode},x),\quad
   \upsilon^{*}(y)=\Un(p_{\decode}, y).
 \end{align*}

Also, these programs must work in quadratic time and linear space on a one-tape
Turing machine (as the repetition code certainly does).

Let us now define the block code \( (\psi_{*}, \psi^{*}) \) used in the
definition of the configuration code \( (\varphi_{*}, \varphi^{*}) \) as 
outlined in Section~\ref{sec:hier-codes}.
We define
\begin{equation}\label{eq:psi}
   \psi_{*}(a)  = 0^{\PadLen}\upsilon_{*}(a)0^{\PadLen}.
\end{equation}
It will be easy to compute the configuration code from \( \psi_{*} \),
once we know what fields there are which ones need initialization.

The decoded value \( \psi^{*}(x) \) is obtained by first removing \( \PadLen \)
symbols from both ends of \( x \) to get \( x' \), and then computing \(
\upsilon^{*}(x') \).


\section{The model}\label{sec:model}

Recall the definition of sparsity in Section~\ref{sec:sparsity}: there will be 
a sequence \( 0<\B_{1}<\B_{2}<\dotsm \) of ``scales'' in space and a sequence
\( \Tu_{1}<\Tu_{2}<\dotsm \) of scales in time, and a constant \( \beta \).
We will define a sequence of simulations \( M_{1}\to M_{2}\to\dotsm \) where
each \( M_{k} \) is a machine simulating one on a higher level.
For simplicity, we will use the notation \( M=M_{k} \), \( M^{*}=M_{k+1} \),
and similarly for the other parameters, for example \( \B,\Tu, \Q, \U \).
As already indicated in Section~\ref{sec:hier}, these machines will generalize
ordinary Turing machines, with a number of new features.

\begin{notation}
  For an interval \( I=\lint{a}{b} \) and some \( c\ge 0 \) let
  \begin{align*}
   \Int(I,c) = \lint{a+c}{b-c}.
  \end{align*}
  Similarly for intervals \( \rint{a}{b} \), \( \clint{a}{b} \), \( \opint{a}{b} \).
\end{notation}

\subsection{Generalized Turing machine}\label{sec:gen-TM}

Standard Turing machines do not have
operations like ``creation'' or ``killing'' of cells, nor
do they allow for cells to be non-adjacent.
We introduce here a \df{generalized Turing machine}.
It depends on an integer \( \B \ge 1 \) that denotes the cell body size,
and an upper bound \( \Tu \) on the transition time.
These parameters are convenient since they provide the illusion that the different Turing
machines in the hierarchy of simulations all operate on the same linear space and the same time
interval.
Even if the notions of cells, alphabet and state are different for each machine of the hierarchy, 
at least the notion of a \emph{location on the tape} is the same.
There will also be a \df{ pass number} parameter \( \passno \),
whose meaning will be explained in the definition of trajectories.


\begin{definition}[Generalized Turing machine]\label{def:gen-TM}
    A \df{generalized Turing machine} \( M \) is defined by a tuple
        \begin{align}\label{eq:gen-TM}
             (\Sigma, \tau, \Adj,\B, \Tu, \passno),
       \end{align}
    where \( \Sigma \) is a finite set called the \df{alphabet},
        \begin{align*}
             \tau: \Sigma^{2}\to \Sigma^{2}\times\{-1,1\}
        \end{align*}
    is the \df{transition function},
\( \Adj \) is a function on the alphabet (can be called a ``field'' if a symbol is viewed 
as a piece of data, a record with several fields).
\( \Adj\colon\Sigma\to\{ 0, 1 \} \) will show 
whether the last move was from an adjacent cell.
% The function \( \cDir\colon\Sigma\to\{-1,1\} \) of the cell content
% needs to always point towards the head, so 
% the transition function \( \tau \) is required to have the property that
% if \( (a',q',j)=\tau(a,q) \) then \( a'.\cDir=j \).
The integer \( \B\ge 1 \) is called the \df{cell body size},
and the real number \( \Tu \) is a bound on the transition time.
The integer \( \passno \) will be the number of passes needed to clean an area (see below).
Among the elements of the tape alphabet \( \Sigma \), 
we distinguish the elements \( 0,1,\Bad,\Vacant \).
The role of the symbols \( \Bad \) and \( \Vacant \) will be clarified below.
\end{definition}

The definition of a configuration below introduces a new concept, the \df{current cell-pair} position
\( \vhc = (\hc_{0},\hc_{1}) \).
(We will continue calling \( \hc_{0} \) the \df{current cell}.)
Here is some explanation for the need of this new variable.
A generalized Turing machine \( M_{2} \) may be simulated by some lower-level (generalized) Turing
machine \( M_{1} \).
Not all details of the tape content of \( M_{1} \) are visible on the level of \( M_{2} \): what is visible
is the content of the cells of \( M_{2} \) decoded from those of \( M_{1} \).
The currently observed cell-pair in the simulated machine \( M_{2} \) is \( \vhc \).
On the other hand the true head position of \( M_{1} \) is \( \h \)
which may perform a lot of oscillatory movement
not directly relevant to the work of \( M_{2} \).

\begin{definition}[Configuration]\label{def:config}
     Consider a generalized Turing machine~\eqref{eq:gen-TM}.
    A \df{configuration} is a tuple
        \begin{align*}
             (A,\h,\vhc),
        \end{align*}
where \( A:\bbZ\to\Sigma \), \( \h,\hc_{j}\in\bbZ \).
The array \( A \) is the tape configuration.
As in~\eqref{eq:config-1} before, \( \h \) is the head position, but it may differ from the current cell
\( \hc_{0} \), so now if \(  \xi= (A,\h,\vhc) \) then we write
        \begin{align}\label{eq:config-2}
             \xi.\tape=A, \quad \xi.\pos=\h,\quad \xi.\curcell_{j}=\hc_{j}. 
        \end{align}
A point \( p \) is \df{clean} if  \( A(p)\ne\Bad \).
A set of points is \df{clean} if it consists of clean points.

We say that there is a \df{cell} at a position \( p\in\bbZ \) if the interval
\( p+\lint{0}{\B} \) is clean and \( A(p)\ne \Vacant \).
In this case, we call the interval \( p+\lint{0}{\B} \) the \df{body} of this cell.
Cells must be at distance \( \ge\B \) from each other, that is their
bodies must not intersect.
If they are at a distance \( <2\B \) from each other then they are called \df{neighbors}.
They are called \df{adjacent} if the distance is exactly \( \B \).
A configuration will always have the following property:
\begin{varenum}{C}
\item\label{i:config.sharp-ends}
 Any endpoint of a maximal clean interval is the end of a cell body in that direction.
\end{varenum}
% For all cells \( p \), the value \( A(p).\cDir \) is required to point towards 
% the head position \( \h \), that is 
%  \begin{align*}
%    A(p).\cDir=\sign(\h-p).
%  \end{align*}
Whenever the interval \( \h+\lint{4\B}{4\B} \) is clean there must be a
pair of neighbor cells at some positions \( \hc_{0},\hc_{1} \) within
this interval called the \df{current cell-pair}.

The array \( A \) is \( \Vacant \) everywhere but in finitely many positions.
Let
    \begin{align*}
         \Configs_{M}
    \end{align*}
    denote the set of all possible configurations of a Turing machine \( M \).
\end{definition}

All the above definitions can clearly be localized to define a configuration
\df{over a space interval} \( I \), where it is always understood that \( \h\in I \), that is 
\( I \) contains the head.

\begin{definition}[Local configuration, replacement]
\label{def:local-config}
  A \df{local configuration on} a (finite or infinite)
  interval \( I \) is given by values assigned to the cells
  of \( I \), along with the following information: whether
  the head is to the left of, to the right of or inside
  \( I \), and if it is inside, on which cell.

  If \( I' \) is a subinterval of \( I \), then a local configuration
  \( \xi \) on \( I \) clearly gives rise to a local configuration
  \( \xi(I') \) on \( I' \) as well, called its
  \df{subconfiguration}: If the head of \( \xi \) was in \( I \)
  and it was for example to the left of \( I' \), then now
  \( \xi(I') \) just says that it is to the left, without specifying it position.

  Let \( \xi \) be a configuration and \( \zeta(I) \) a local
  configuration that contains the head if and only if
  \( \xi(I) \) contains the head.
  Then the configuration \( \xi\vert\zeta(I) \) is obtained by
  replacing \( \xi \) with \( \zeta \) over the interval \( I \),
  further if \( \xi \) contains the head then also replacing
  \( \xi.\pos \) with \( \zeta.\pos \).
\end{definition}

It is natural to name a sequence of configurations that is conceivable as a computation
(faulty or not) of a Turing machine as ``history''.
The histories that obey the transition function then could be called ``trajectories''.
In what follows we will 
stretch this notion to encompass also some limited violations of the
transition function.
In connection with any underlying Turing machine with a given starting configuration, we will
denote by
\begin{align}\label{eq:noise-first}
   \Noise\subseteq \bbZ\times \bbZ_{\ge 0}
\end{align}
the set of space-time points \( \pair{p}{t} \), such that
a fault occurs at time \( t \) when the head is at position \( p \).

\begin{definition}[History]\label{def:history}
  \begin{sloppyenv}
    For a generalized Turing machine~\eqref{eq:gen-TM}, 
consider a sequence \( \eta = (\eta(0), \eta(1), \dots) \) of configurations with
\( \eta(t) = \) \( (A(\cdot, t), \h(t), \vhc(t)) \), along with a noise set \( \Noise \).
The \df{switching times} of this sequence are the times \( t \) when 
the position or content of the current cell-pair changes.
The interval between two consecutive switching times is the \df{dwell period}.
The pair
      \end{sloppyenv}
    \begin{align*}
       (\eta,\Noise)
    \end{align*}
    will be called a \df{history} of machine \( M \) if the following conditions hold.
        \begin{itemize}
            \item \( \abs{\h(t) - \h(t')} \le \abs{t' - t} \).

            \item In two consecutive configurations, content \( A(p,t) \) of the positions \( p \)
              not in \( \h(t) + \lint{-2\B}{2\B} \), remains the same: for example
                  \( A(n,t+1) = A(n,t) \) for all \( n \notin \h(t) + \lint{-2\B}{2\B} \).
            \item At each noise-free switching time the head is on the new current cell:
\( \hc_{0}(t)=\h(t) \).
In particular, when at a switching time a current cell becomes
\( \Vacant \), the head must already be on another (current) cell.

            \item The length of any noise-free 
dwell period in which the head is staying on clean positions is at most \( \Tu \).

        \end{itemize}
    Let
        \begin{align*}
            \Histories_{M}
        \end{align*}
    denote the set of all possible histories of \( M \).

We say that a cell \df{dies} in a history if it becomes \( \Vacant \).

All the above definition can be \df{localized} to define a history
\df{over a space-time rectangle} \( I\times J \), 
where it is always understood that \( \h\in I \) for all times \( t\in J \),
that is \( I \) contains the head throughout the time interval considered.
\end{definition}

In Section~\ref{sec:traj} below, we will define a certain subset of possible histories, called \df{trajectories}.
The set of trajectories of \( M \) will be denoted by
\begin{align*}
   \Trajectories_{M}.
 \end{align*}

\subsection{Simulation}\label{sec:sim}

Until this moment, we used the term ``simulation'' informally, to denote
a correspondence between configurations of
two machines which remains preserved during the computation.
In the formal definition, this correspondence will essentially be a code
\( \varphi=(\varphi_{*},\varphi^{*}) \).
The \emph{decoding} part of the code is the more important.
We want to say that machine \( M_{1} \) simulates machine \( M_{2} \) via
simulation \( \varphi \) if whenever \( (\eta, \Noise) \) is a trajectory of \( M_{1} \) 
then \( (\eta^{*},\Noise^{*}) \),
defined by \( \eta^{*}(\cdot,t)=\varphi^{*}(\eta(\cdot,t)) \), is a
trajectory of \( M_{1} \).
Here, \( \Noise^{*} \) is computed by an appropriate mapping.

We will make, however, two refinements.
First, we may weaken the condition by requiring this only for
those \( \eta \) for which the initial configuration
 \( \eta(\cdot,0) \) has been obtained by encoding, that is it has the form 
\( \eta(\cdot,0)=\varphi_{*}(\xi) \).
The encoding function gets a role this way in the definition, after all.

But there is a more complex refinement.
When a colony is in transition between encoding one simulated value to encoding another one,
there may be times when the value represented by it before the transition
is already not decodable from it, and the value after the transition is not yet decodable from it.
So we will define simulation decoding as a mapping \( \Phi^{*} \)
between \emph{histories}, not just configurations.
This allows a certain amount of ``looking back'':
the map \( \Phi^{*} \) can depend on the configurations at the beginning of the ``work period''.

It is the mapping \( \Phi^{*} \) that will also define \( \Noise^{*} \).
A history was defined above in Definition~\ref{def:history} 
as a pair \( (\eta,\Noise) \), so we will have
\( \Phi^{*}(\eta,\Noise)=(\eta^{*},\Noise^{*}) \).
The meaning of \( \Noise^{*} \) will be, just as in Definition~\ref{def:sparsity} of sparsity:
it will be obtained from \( \Noise \) by deleting those small isolated parts that the 
error-correcting simulation can deal with.

\begin{definition}[Simulation] \label{def:simulation-central}
Let \( M_{1},M_{2} \) be two generalized Turing machines, and let
\begin{align*}
    \varphi_{*}:\Configs_{M_{2}} \to \Configs_{M_{1}}
\end{align*}
be a mapping from configurations of \( M_{2} \)
to those of \( M_{1} \), such that it maps
starting configurations into starting configurations.
We will call such a map a \df{configuration encoding}.
Let
\begin{align*}
   \Phi^{*}:\Histories_{M_{1}} \to \Histories_{M_{2}}
\end{align*}
be a mapping.
The pair \( (\varphi_{*}, \Phi^{*})  \)
is called a \df{simulation} (of \(  M_{2}  \) by \(  M_{1}  \)) if for every
trajectory \(  (\eta, \Noise)  \) with initial
configuration \(  \eta(\cdot,0)=\varphi_{*}(\xi)  \),
the history \(  (\eta^{*},\Noise^{*})=\Phi^{*}(\eta,\Noise)  \) is
a trajectory of machine \(  M_{2}  \).

We say that \( M_{1} \) \df{simulates} \( M_{2} \) if there is a simulation
\( (\varphi_{*},\Phi^{*}) \) of \( M_{2} \) by \( M_{1} \).
\end{definition}

\subsection{Hierarchical codes}\label{sec:hier-codes}

Recall the notion of a code in Definition~\ref{def:codes}.

\begin{definition}[Code on configurations]\label{def:configuration-code}
\begin{sloppypar}
 Consider two generalized Turing machines \( M_{1},M_{2} \) with the corresponding
alphabets and transition functions, and an integer \( \Q\ge 1 \).
We require
\begin{align}\label{eq:B_{2}-B_{1}-\Q}
  \B_{2} = \Q \B_{1}.
\end{align}
Assume that a block code
\(
   \psi_{*}:\Sigma_{2}\to\Sigma_{1}^{\Q}
\)
is given, with an appropriate decoding function, \( \psi^{*} \).
Symbol \( a\in\Sigma_{2} \), is interpreted the content of some tape square.
% For all \( a,q \), if \( \psi_{*}(a,q) = (b_{1},\dots,b_{\Q}) \) then we require
% \( b_{i}.\cDir=a.\cDir \) for each \( i \).
\end{sloppypar}

This block code gives rise to a \df{code on configurations}, that is a pair of functions
    \begin{align*}
        \varphi_{*} :\Configs_{M_{2}} \to \Configs_{M_{1}},
        \quad
        \varphi^{*}:\Configs_{M_{1}} \to \Configs_{M_{2}}
    \end{align*}
    that encodes some (initial) configurations \( \xi \) of \( M_{2} \) into configurations of \( M_{1} \).
    Let \( \xi \) be a configuration of \( M_{2} \) with \( \xi.\curcell_{j}=\xi.\pos+j\B_{2} \).
    We set \( \varphi_{*}(\xi).\pos = \xi.\pos \), \( \varphi_{*}.\curcell_{j}= \varphi_{*}(\xi).\pos+j\B_{1} \),
  and
\begin{align*}
 \varphi_{*}(\xi).\tape(i\B_{2}, \dots, (i+1)\B_{2} - \B_{1}) = \psi_{*}(\xi.\tape(i)).
 \end{align*}
A configuration \( \xi \) is called a \df{code configuration} if it has the form \( \xi=\varphi_{*}(\zeta) \).
 \end{definition}


\begin{definition}[Hierarchical code]\label{def:hierarchical-code}
For \( k\ge 1 \), let \( \Sigma_{k} \) be an alphabet, of a generalized Turing machine \( M_{k} \).
Let \( \Q_{k}>0 \) be an integer colony size, let \( \varphi_{k} \)
be a code on configurations defined by a block code
  \begin{align*}
       \psi_{k}: \Sigma_{k+1}\to \Sigma_{k}^{\Q_{k}}
  \end{align*}
as in Definition~\ref{def:configuration-code}.
The sequence \( (\Sigma_{k},\varphi_{k}) \), (\( k\ge 1) \),  is
called a \df{hierarchical code}.
For the given hierarchical code, the configuration \( \xi^{1} \) of \( M_{1} \)
is called a \df{hierarchical code configuration} if a sequence
of configurations \( \xi^{2},\xi^{3},\dots \) of \( M_{2},M_{3},\dots \) exists with
\begin{align*}
 \xi^{k}=\varphi_{*k}(\xi^{k+1})
 \end{align*} 
for all \( k \).
(Of course, then whole sequence is determined by \( \xi^{1} \).)

Let \( M_{1} \), \( M_{2} \), \( \dots\) be a sequence of generalized Turing machines,
let \( \varphi_{1} \), \( \varphi_{2} \) , \(\dots \) be a hierarchical code for this sequence,
let \( \xi^{1} \) be a hierarchical code configuration for it, where \( \xi^{k} \) is an
initial configuration of \( M_{k} \) for each \( k \).
Let further be a sequence of mappings \( \Phi^{*}_{1} \), \( \Phi^{*}_{2} \), \( \dots \) be
given such that for each \( k \), the pair \( (\varphi_{k*},\Phi_{k}^{*}) \),
is a simulation of \( M_{k+1} \) by \( M_{k} \).
Such an object is called a \df{tower}.
\end{definition}

The main task of the work will be the definition of a tower, since the simulation
property is highly nontrivial.

\subsection{Trajectories}\label{sec:traj}

This subsection completes the definition of a generalized Turing machine.
The set of trajectories \( \Trajectories_{M} \) 
is defined in terms of constraints imposed on the burst-free parts of a history.
We discuss these properties first informally.

\begin{description}
\item[Transition Function] This property says (in more precise terms)
that in the absence of noise and in a clean area, the
transition function is obeyed.

\item[The Spill Bound] limits the extent to which a disordered interval can spread while
the head is in it.

\item[Escape] limits the time that the head can spend in a small area.

\item[Attack Cleaning] helps erode the disorder as the head repeatedly enters and leaves it.

\item[Pass Cleaning] limits the number of back-and-forth slides that can happen over a
large disordered area (under certain conditions).
Otherwise every time the head reaches, say, its left end,
a new burst could expand the disorder.
As these bursts are not visible in the simulated trajectory \( \eta^{*} \),
the Spill Bound property of \( \eta^{*} \) could be violated.

\end{description}

\begin{definition}
Suppose that at times \( t' \) before a switching time \( t \) but after 
any previous switch, the current cell-pair has state \( \va \),
further after the switch the same cell pair has state \( \va' \)
(including when one of these cells dies).

We say that the switch is \df{dictated by the transition function} if
\begin{align*}
 (\va',\sign(\hc_{0}(t')-\hc_{0}(t))) =  \tau(\va),
 \end{align*}
further \( a'_{0}.\Adj=1 \) if and only if \( \hc_{0}(t')-\hc_{0}(t)\in\{-B,0,B\} \).
\end{definition}

We will need the following constants, to be fixed later.
\begin{align}\label{eq:cns.traj}
  \CAtt<\beta/2,
  \CSpill,
  \theta,
  \CPass\le\theta/6,
  \CEsc.
 \end{align}
 % \begin{definition}[Clean hole]\label{def:clean-hole}
% The following definitions will use the constants
% % If in a configuration the head is in a clean interval of size \( \ge 1.5 \CAtt\B \),
% % at a distance \( \ge\B \) from the complement, then we will say that the head is \df{inside a clean hole}.
% \end{definition}

\begin{premark}
  Some details of the following definition need to be adjusted yet.
\end{premark}


\begin{definition}[Trajectory]\label{def:traj}
\begin{sloppypar}
   A history  \( (\eta, \Noise) \) of a generalized Turing 
machine~\eqref{eq:gen-TM} with \(\eta(t) =\)
\( (A(t), \h(t), \vhc(t)) \)
is called a \df{trajectory} of \( M \) if the following conditions hold, in any 
noise-free time interval \( J \).
  \end{sloppypar}
\begin{description}

\item[Transition Function]\label{i:def.traj.transition}
Suppose that there is a switch, and the current cell-pair is 
is inside the clean area, by a distance of at least \( 2.5\B \).
Then the new state of the current cell-pair, and
the direction towards the new current head position
are dictated by the transition function.
If a new current cell did not exist before then it is adjacent to one of the
previous ones before the switch.
The only change on the tape occurs on the interval enclosing the new and old current cells.
Further, the length of the dwell period is bounded by \( \Tu \).

\item[Spill Bound]\label{i:spill-bound}
  An interval of disorder can spread by at most \( \CSpill \B \)
  on either side while it contains the head.

\item[Escape] \label{i:def.traj.escape}
  The head will leave any interval of size \( \le \lambda\B \) with \( 1\le\lambda\le 3\beta \)
  within time \( \CEsc\lambda^{2}\Tu \).

\begin{sloppypar}
\item[Attack cleaning] \label{i:def.traj.attack-cleaning}
Suppose that the current cell-pair is at the end of a maximal clean interval of size
\( \ge (\CAtt+1)\B \).
Suppose further that the transition function directs the head right.
Then by the time the head comes back to \( x-\CAtt \B \), the right end of the clean interval containing it
advances to the right by at least \( \B \).
 \end{sloppypar}

A similar property is required when ``left'' and ``right'' are interchanged.

\item[Pass Cleaning] This property will be defined below in Definition~\ref{def:pass-cleaning}.

\end{description}

\end{definition}

To motivate the Pass Cleaning property, 
the following example shows that in our noise model,
the number of passes over some disorder needed to eliminate it
cannot be a universal constant: it has to depend on the machine.

\begin{example}[Many slides over disorder]\label{xpl:unbounded}
  Let us describe a certain ``organization'' of a disordered area along with a scenario
  in which an unbounded number of passes may be required to restore order.
For some \( n<0 \), let the cells of \( M_{1} \) at positions
\( x_{-Q_{1}},\dots,x_{n} \), where \( x_{i+1}=x_{i}+B_{1} \),
represent part of a healthy colony \( C(x_{-Q_{1}}) \) starting at \( x_{-Q_{1}} \), where \( x_{n} \)
is the rightmost cell of \( C(x_{-Q_{1}}) \)
to which the head would come in the last sweep before
the simulation will move to the \emph{left} neighbor colony \( C(x_{-2Q_{1}}) \).
Let them be followed by cells \( x_{n+1},\dots, x_{Q_{1}-1},\dots\)
which represent the last sweep of a transfer operation to the \emph{right} neighbor colony \( C(x_{0}) \).
When the head is in cell \( x_{n} \), a burst transfers it to \( x_{n+1} \).
The cell state of \( M_{2} \) simulated by \( C(x_{-Q_{1}}) \) needs to be in \emph{no relation} to 
the cell state of \( M_{2} \) simulated by \( C(x_{0}) \).
This was a capture of the head by a burst of \( M_{1} \) across the point 0, to the right.

We can repeat the capture scenario, say around points \( i Q_{1}Q_{2} \),
and this way cells of \( M_{3} \) simulated by \( M_{2} \) (simulated by \( M_{1} \))
can be defined arbitrarily, with no consistency needed between any two neighbors.
(We did not write \( i Q_{1} \) just in case bursts are not allowed in neighboring colonies.)
In particular, we can define them to implement a \emph{leftward} capture scenario
via level 3 bursts at points \( i Q_{1}Q_{2}Q_{3}Q_{4} \), allowing to simulate arbitrary cells of \( M_{5} \)
with no consistency requirement between neighbors.
So \( M_{5} \) could again implement a rightward capture scenario, and so on.
In summary, a malicious arrangement of disorder and noise allows \( k \) passes
after which the level of organization is still limited to level \( 2 k + 1 \).
\end{example}

Our construction will set \( \passno=O(k) \) for a machine on level \( k \) of the simulation,
that is \( O(k) \) passes will restore organization to level \( k \).
(Essentially, the third slide should raise the level.)
But some restriction is needed: if between the noiseless passes there is a large number of
\df{intrusions} that are not passes, then each of these intrusions can bring a little new disorder, possibly
undoing the effect of passes.
Just limiting the \emph{number}
of intrusions is not sufficient, since there can be too many local intrusions.
In our noise model, a space-time area is noiseless for the simulated machine \( M^{*} \) if,
essentially, every \( 2\gamma\cdot (\Q\B)\times (\U\Tu) \) rectangle contains at most one burst
(of size \( \beta\cdot \B\times \Tu \)).
So instead of limiting the number of intrusions, we will limit the number of a certain kind of rectangles
covering them.

\begin{definition}
  For a space-time path \( P \) of the head and an interval \( I \),
let \( P_{u:v} \) be its part between \( u \) and \( v \).
We write \( P_{:v} \) for the part from start to \( v \) and \( P_{u:} \)
for the part from \( u \) to end.

A part \( P_{u:v} \)
is a \df{pass} of \( I \) if \( P \) enters \( I \) at time \( u \) and leaves it on the other side
at time \( v \) (without leaving \( I \) before).
We say that \( P \) \df{intrudes} \( I \) if it enters and leaves \( I \) on the same side.
\end{definition}

% \begin{definition}
%   % For some \( W,H>0 \), a \( (W,H) \)-\df{rectangle} is a space-time rectangle of width \( \le W \) and
%   % height \( \le H \).
%   % We say that the rectangle has \df{max-height} if its height is \( H \),
%   % and \df{max-width} if its width is \( W \).
%   % It is \df{maximal} if its width or height is maximal.
% % For a path \( P \) let \( R(P) \) be the smallest rectangle containing \( P \).
% % The rectangles \( R(P_{:t}) \) are increasing with \( t \) both in width and height.
% % Let \( R_{W,H}(P) \) be defined as the largest \( R(P_{:t}) \) that is still a \( (W,H) \)-rectangle.
% \end{definition}

\begin{definition}[\( W \)-step length]
  Consider a path \( P \) over some time interval \( \lint{t}{u} \).
  Break it up into segments of the form \( \lint{a}{b} \) in such a way
  that each segment has size \( \le W \) and only the last segment can have size \( <W \).
  Such a segmentation is called a \( W \)-\df{segmentation} of \( P \), and the
  number of segments is the \( W \)-\df{count} of \( P \).
  
  If a path \( P \) may pass or intrude some interval \( I \) several times then
  we call the \( W \)-count of intrusions of \( P \) into \( I \) the sum of the \( W \)-counts
  of all intrusions.
\end{definition}

Note that if \( P \) is noiseless and \( W=\lambda\B \) with \( 1\le \lambda\le 3\beta \) then
by the Escape property,
the time spent in each segment of a \( W \)-segmentation of \( W \) is at most \( \CEsc\lambda^{2}\Tu \).

Recall the constants defined in~\eqref{eq:cns.traj}.

\begin{definition}[Pass cleaning property]\label{def:pass-cleaning}
  Suppose that a path \( P \) passes interval \( I \) at least \( \pi \) times, and
  the \( \theta\B \)-count of the intrusions of \( P \) into \( I \) is at most \( \passno^{2} \).
 Then during \( P \) the interior \( \Int(I, \CPass\B) \) of the interval \( I \) becomes clean.
 \end{definition}

The trajectory properties can clearly be localized to some space-time
rectangle just as the definition of history was.


\section{Simulation structure}

In what follows we will describe the program of the reliable Turing machine
(more precisely, a simulation of each \( M_{k+1} \) by \( M_{k} \) as defined above).
Most of the time, we will just refer to \( M_{k} \) as \( M \) and to \( M_{k+1} \) as
\( M^{*} \).
Cells will be grouped into colonies, where  \( \Q=\B^{*}/\B \) is the colony size.
The behavior of the head on
each colony simulates the head of \( M^{*} \) on the corresponding cell
of \( M^{*} \).
The process takes a number of steps, constituting a \df{work period}.

\begin{definition}\label{def:Tu}
The parameter \( \U=\Tus/\Tu \) is defined to be
twice the maximum number of steps that any simulation work period can take.
\end{definition}

Machine \( M \) will perform the simulation even if the noise
in which it operates is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse.
By the above definitions, 
sparsity means that noise comes in \df{bursts} that are confined to
rectangles of size \( \beta\pair{\B}{\Tu} \) (affecting at most \( \beta \) consecutive tape cells), 
and are separated from each other in time in such a way that there is at most one burst
in any \( \gamma \) neighboring work periods.
A design goal for the program the following:

\begin{goal}[Local correction]\label{goal:locality}
Correct a burst within space and time comparable to its size, and much smaller than the 
size of a simulation work period.
\end{goal}

There are some difficulties faced by our desire for a structured presentation:
In order to analyze the error-correcting performance even of one a part of the  program,
we may need to see the whole, since the noise can change the environment of the head into  
some state corresponding to an arbitrary other part.

We mentioned \df{modes} in Section~\ref{sec:language}.
Ordinary simulation proceeds in the normal mode. 
To see whether the basic structure
supporting this process is broken somewhere, each step will check 
whether the current cell-pair is \df{coordinated}
(see Definition~\ref{def:coordinated}).
If not then the state of the current pair will be changed into the \df{healing} mode.
We will also say that \df{alarm} will be called.
On the other hand, the state enters into \df{rebuilding} mode on some indications that
healing fails.
% The crudest outline of the main rule of machine \( M \) is given in
% Rule~\ref{alg:main1}; the \( \Comp \) and \( \Transfer \) rules will be
% outlined below.
(Rebuilding may be triggered inside the \( \rHeal \) rule.)

% \begin{algorule}\caption{\textbf{Main rule}\label{alg:main1}}

%   \If{the mode is normal}{
%     \lIf{\algNot \( \Coordinated \)}{\( \rHeal \)}
%     \lElseIf{\( 1 \le \Sweep < \TransferSw(1) \)}{ \( \Comp \) }
%     \lElseIf{\( \TransferSw(1) \le \Sweep < \Last \)}{ \( \Transfer \)}
%    \lElseIf{\( \Last \le \Sweep  \)}{move the head to the new base.}
%   }
% \end{algorule}


\subsection{Head movement}\label{sec:sweep}

The global structure of a work period is this:
\begin{description}

\item[Computation phase] 
The new state of the simulated cell pair, and the simulated direction (called the drift) is computed.
Then the ``meaningfulness'' of the result is checked.
During this phase the head sweeps, roughly, back-and-forth between
the ends of the base colony-pair.

\item[Transfer phase]
  The head moves into the neighbor colony in the simulated head direction called drift (building or rebuilding
  a bridge if needed).
  Redundancy is used to protect this movement from a burst.  
\end{description}

The timing is controlled by a field \( \Sweep \).
We can read off the direction of the sweep \( s \) using the formula
     \begin{align}\label{eq:sweep-dir}
       \dir(s)=(-1)^{s + 1}.
     \end{align}
Some issues complicate the head movement.

\subsubsection{Zigging}\label{sec:zigging}

\begin{definition}[Front]\label{def:front}
  During normal computation, as the head sweeps in a certain direction, it increases \( \Sweep \) field by 1.
Let us call the last site in which this increase occured, the \df{front}.
\end{definition}
Globally in a configuration, due to disorder, there may be more than one front, but locally
we can talk about ``the'' front without fear of confusion.

A burst could turn the head back in the middle of its sweep, as pointed out in Example~\ref{xmp:zig}.
To detect such an event promptly (in accordance with Goal~\ref{goal:locality}),
a \df{zigzag} movement will be superimposed on the sweeping, the head will swing forward and backward
from the front.
On its backward zig, the head can check the consistency of the last few cells.
On its forward zig it can check whether it is not about to enter into forbidden territory.
Zigging will use the following parameters:
\begin{align}\label{eq:ZDef}
  \f &= 5, \quad \Z = c \passno^{4}       
\end{align}
for an appropriate constant \( c \). %!
After every \( \f \) steps of progress, the head will perform a forward-backward-forward
zigzag of size approximately \( \Z\B \), checking the area of size \( \Z\B \) around the current
cell-pair (the turn positions will be modified in Section~\ref{sec:feathering} below).
The fields
\begin{align*}
\ZigDir\in\{-1,0,1\}, \ZigDepth\in\lint{-2\Z}{2\Z}
\end{align*}
of the current cell control the process.
At the front, we have \( \ZigDepth=0 \) and \( \ZigDir=0 \).
At the start of zigging, we set \( \ZigDir\gets -1 \), and starts \df{descending} into a (forward) zig,
while increasing \( \ZigDepth \).
When the front reaches the extreme edge of zigging at an appropriate turning point (see below),
we set \( \ZigDir\gets 1 \),
the head turns towards the front and starts \df{ascending}, while decreasing \( |\ZigDepth| \).
Once the front is reached, a similar backward zigging swing is also
performed (with \( \ZigDepth \) decreasing below 0).

% Normally the head should reach the front exactly when \( \ZigDepth \) reaches 0.
% However, if it reaches the front earlier (for whatever cause), then we just set \( \ZigDepth\gets 0 \).
% If it does not reach the front by this time (for whatever cause), then the head will just keep marching forward, 
% keeping \( \ZigDepth=0 \) until the front is reached.
% \\Pnote{Do we still have an example now that zigging begins only well inside?}

\subsubsection{Feathering}\label{sec:feathering}

Example~\ref{xmp:feather} above pointed out the need for the following feature of the simulation program.
% \begin{example}\label{xmp:feather-need}
% Let \( C(x) \) denote the colony with starting point \( x \).
% Consider the following scenario.

% \begin{enumerate}
% \item\label{i:leave-dirt}
% After the last turn at the end of a right sweep on the right end of
% colony \( C(x) \), at the bottom of the zig, say in
% position \( y = x+(\Q+\Z)\B \) if the zig goes outside the colony, a burst creates some disorder.
% Then the simulation dictates the head to leave \( C(x) \) on the left.

% \item\label{i:later-burst} 
% Much later, on the first sweep of a return to \( C(x) \), this disorder traps the head, and
% lets it start a right sweep to the right of \( y \).  
% When the head returns on a zig to \( y \), 
% a new burst corrects everything on the left of \( y \), including \( y \), but leaves
% the start of the new (false) right sweep on the right of \( y \)---even though
% the simulation does not dictate any move to the colony \( C(x+\Q\B) \).

% \item\label{i:repeat} Much later, the pair of events \ref{i:leave-dirt}-\ref{i:later-burst}
% happens again and again, allowing the started false sweep on the right of \( y \) to continue.
% After \( \Z \) repetitions of this, the zig does not return already.

% \end{enumerate}
% This way, a number of cleverly placed distant bursts can trigger an effect on the
% next level.
% (A probability model more sophisticated than sparsity might deal with this problem, but we did not find it.)
% \end{example}

% The sequence of Example~\ref{xmp:feather-need} would not occur if
% our machines had the following property:

\begin{definition}[Feathering]\label{def:feathering}
A Turing machine execution is said to have the \( c \)-\df{feathering} property if the following holds.
If the head turned back at a position \( x \) at some time, then next time it comes within distance \( c \)
of \( x \), it can turn back only at least \( c \) steps beyond \( x \).
\end{definition}

(The name ``feathering'' refers to the picture of the path of the head in a space-time diagram.)
The following example suggests that any computation can be reorganized to accomodate feathering,
without too much extra cost.

\begin{example}[1-feathering]\label{xmp:feathering}
Suppose that, arriving from the left at position 1, the head decides to turn left again.
In repeated instances, it can then turn back at the following sequence of positions:
\begin{align*}
 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, 5, 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, \dots
 \end{align*}
\end{example}

If in the original execution the head turned back \( t \) consecutive
times to the left from position \( p \), then now it will 
turn back from somewhere in a zone of size \( O(\log t) \) to the right of \( p \) in 
each of these times.
(Computing the exact turning point is not necessary.)

We will handle feathering at two kinds of turns: \df{small} turns and \df{big} turns.
Let 
\begin{align}\label{eq:FDef}
  \F = \passno^{4}.
 \end{align}
 This allows us to define the earlier used parameter
\begin{align}\label{eq:PadLenDef}
  \PadLen &= \F\log\Q.   
 \end{align}
The process will be handled by a field 
\begin{align*}
   \Turned\in\{ 0,1,2 \}.
\end{align*}
The value \( 1 \) shows the memory of a recent turn.
The value \( 2 \) shows the memory of a recent big turn within distance \( \F \).
When a cell is passed in which \( \Turned>0 \) was found then this value is set to \( 0 \).
\begin{description}
\item[Small turns]
are done during zigging and healing.
Move until you found a cell with \( \Turned\ne 1 \), then turn.
Set \( \Turned\gets 1 \) in this cell.

\item[Big turns] are done during the ends of sweeps in simulation and rebuilding.
Move until you found \( \F \) consecutive cells with \( \Turned<2 \)
the last one of which has \( \Turned=0 \), then make a turn.
On the cell after the turn, set \( \Turned\gets 1 \),
and also on the first interval of  \( \F \) cells after the turn, set \( \Turned\gets 2 \).
\end{description}
The control of the above procedures can be accomplished by a counter, say
called \( \FDepth \).

In both cases, there is no limit on the length of the attempt; however, 
a small turn attempt can take longer than \( O(1) \) steps and
a big turn attempt can take longer than \( O(\F\log\Q) \) steps only in the presence of
large disorder (those cases will be analyzed in the context of large disorder).
We give here only an informal argument justifying this statement; a
formal proof must wait until 
a complete definition of the simulation and of the notion of large disorder.

For the moment, let us call cells  \df{red} with by \( \Turned=1 \)
and \df{orange} with \( \Turned=2 \).
Red cells can delay small turns;
they can only arise in the forward turn of a zig in the previous sweep.
Zigs are by the definition of zigging spaced by \( \f \) cells apart.
Big turns are delayed by orange cells; they are also the ones to create orange cells.
But they will be organized as in Example~\ref{xmp:feathering},
therefore a big turn attempt will be delayed by at most \( \F \) times the logarithm
of the total number of big turns at one end of a colony or rebuild area.

The simulated Turing machine will also have the feathering property,
therefore the simulation will not turn back 
from the same colony without passing it in the meantime.

% \begin{description}
%   \item[Zigging in normal mode]
% Above, we made sure that the points for turning from left to right during simulation
% introduced at the bottom of a zig when sweeping right have \( \Addr\equiv \tZig\pmod{\f} \).
% This way, during left sweep, the head will always find a place within any interval of
% \( \f \) cells with \( \CanTurn=\true \); indeed, the zigging turns during the right sweep
% happened only in one cell in any interval of size \( \f \).

%   \item[Sweeping in normal mode]
% Recall from~\eqref{eq:V} that the simulation work period consists of at most \( \V \) sweeps,
% and according to Definition~\ref{def:interior}, an area of
% \begin{align}\label{eq:PadLenDef}
%  \PadLen = \F\cei{\log\V}+\Z
%  \end{align} 
% cells on both ends of the colony are left free of simulation information---they can be used for
% feathering.
% Suppose that the head is to be turned back at ``the right end''.
% Then at a distance \( \PadLen \) from the right end, 
% it starts looking for a place to turn;
% it will actually turn back at the first position coming after this
% with \( \CanTurn=\true \), and \( \Addr\equiv \tNormal \pmod{\F} \).

% Note that possible sweep turning points are separated from each other by at least
% \( \F \), a rather large number.
% The rationale for this will become clear only when analyzing the boundary of a large
% clean and a large disordered area.

% \item[End of turn in healing]
% The healing procedure needs a turning point at the end of its range.
% In case the cell in question has an address (is not a stem cell) then
% it will need to have \( \Addr\equiv \pm\tHeal\pmod{\f} \).

% \item[Rebuilding]
% Rebuilding mode will have its own internal addresses: as with healing (see later),
% to the left of the rebuilding front, the addresses are counted from the left end, and to the right
% of the head from the right end.
% When sweeping right then the bottom of the left zig will again be at a 
% point with left address \( \equiv\tZig\pmod{\f} \).
% The turns of rebuilding sweeos are handled similarly to the turns of normal sweeps,
% using the rebuilding addresses modulo \( \F \).
% But also, 
% In rebuilding mode, the places to turn must have \( \Addr\equiv\tRebuild\pmod{\F} \). 


% \end{description}

We introduce a convenient notation.
\begin{notation}\label{not:plus}
Suppose that within a procedure a turn attempt is made after a move of \( n \) cells in some direction.
Then the head may not be able to turn right away, the turn attempt moves it a few more steps in the same
direction.
In this case we will say that the head moves \( n^{+} \) cells and turns.
Thus \( n^{+} \) is greater than \( n \), but by not much: only an amount \( O(1) \)
for small turns, and \( O(\f_{2}\log \Q) \) for big turns.
\end{notation}

\subsection{Computation phase}\label{sec:computation-phase}

The first phase of the simulation, called the \( \Comp \) rule,
computes new values for current cell-pair of the
simulated machine \( M^{*} \) represented by the current (base) colony-pair,
and the direction of the move of the head of  \( M^{*} \).
The simulated cell state of \( M^{*} \) will be represented on the track \( \Info \) of the
representing colony.
The move direction of \( M^{*} \) 
will be represented in the \( \Drift \) field of \emph{each} cell of the base colony-pair
(so the whole track must be filled with \( -1 \)'s or \( 1 \)'s).
During the execution of this rule, the head sweeps the base colony-pair.

Recall Definition~\ref{def:err-code}.
The rule \( \Comp \) will rely on a certain fixed \( (\beta,3) \) burst-error-correcting
code, moreover
it expects that each of the words found on the \( \Info \) track
is 2-compliant.  % Probably \( (\beta,2) \) would be sufficient.
The rule \( \rul{ComplianceCheck} \) checks whether a word is \( 2 \)-compliant.

The rule \( \Comp \) essentially repeats 3 times % why was here 5?
the following \df{stages}: decoding, applying the transition, encoding.
Then it calls \( \rul{ComplianceCheck} \); if the latter fails
it will mark the colony for rebuilding.
It uses some additional tracks.
A useful one is called
\begin{align*}
   \Adj'.
\end{align*}
At the time of the computation, this field of each cell of the left base colony 
should contain a single symbol in \( \{ 0, 1 \} \), according to whether the
previous left base colony was adjacent or not.
A track called \( \Work \) can be used for auxiliary computation.
In more detail:
\begin{enumerate}
\item For every \( j=1,\dots,3 \),       % Why was here 5 times instead of 3?
  % if \( \Addr \in \set{0, \dots, \Q-1} \)
  do 
       \begin{enumerate}

          \item Calling by \( \va \) the pair of strings found on the \( \Info \) track of
            the interior of the base colonies,
            decode it into the pair of strings \( \tilde\va=\upsilon^{*}(\va) \)
            (this should be the current state of the simulated cell-pair), and
            store it on some auxiliary track in the base colony-pair.
            Do this by simulating the universal machine on the \( \Prog \) track:
            \( \tilde\va = \Un(p_{\decode}, \va) \).
            However, then replace \( \tilde a_{0}.\Adj \) with the value found on the
            \( \Adj' \) track.
            % Perform all this computation in a way using no information from outside
            % the address interval \( J \), nor on any 
            % part of the state brought back into this interval other than the address,
            % sweep and zigging fields.

          \item \label{i:comp.trans}
           Compute the value \( (\va',d)=\tau^{*}(\tilde\va) \).
           Since the program of the transition function \( \tau^{*} \) is not written explicitly anywhere, 
           this ``self-simulation'' step needs some elaboration, see Section~\ref{sec:self-simulation}.

            \item\label{i:comp.write}
              Write the encoded new cell states \( \upsilon_{*}(\va') \) onto the
              \( \Hold[j].\Info \) track of the interior of the base colony-pair.
              Write \( d \) into the \( \Hold[j].\Drift \) field of \emph{each cell} of
              the left base colony.

              Special action needs to be taken in case 
              a new state \( a'_{0} \) or \( a'_{1} \) is a vacant one, that is for example
              \( a'_{0}.\Kind^{*}=\Vacant^{*} \).
              In this case, write \( 1 \) onto the \( \Hold[j].\Doomed \) track (else 0).

        \end{enumerate}

       \item
      % \item Repeat the following twice (hoping that at least
      %   one repetition will be burst-free):  % Why twice?
            Sweeping through the base colony pair,
            at each cell compute the majority of \( \Hold[j].\Info \), \( j=1,\dots,3 \),
            and write it into the field \( \Info \).
            Proceed similarly, and simultaneously, with \( \Drift \).

       \item       For \( j=1,\dots,3 \), call \( \rul{ComplianceCheck} \) on the \( \Info \) track, and
write the resulting bit into the \( \fld{Compliant}_{j} \) track.

Then pass through the colony and turn each cell in which the majority 
of \( \fld{Compliant}_{j} \), \( j=1,\dots,3 \) is false, into a stem cell
(thus destroying the colony if the result was false everywhere).

  \end{enumerate}

It can be arranged---and we assume so---that the total number of sweeps of this
phase, and thus the starting sweep number of the next phase, depends only on \( \Q \).

\subsection{Forced self-simulation}\label{sec:self-simulation}

Step~\ref{i:comp.trans} of Section~\ref{sec:computation-phase} needs elaboration.

\subsubsection{New primitives}

We will make use of a special track
\begin{align*}
   \Work
 \end{align*}
of the cells and the special field
\begin{align*}
   \Index
 \end{align*}
that can store a certain address of a colony.

Recall from Section~\ref{sec:language} that the program
of our machine is a list of nested
``\textbf{if} \emph{condition} \textbf{then} \emph{instruction}
\textbf{else} \emph{instruction}''
statements.
As such, it can be represented as a binary string 
 \begin{align*}
   R.
 \end{align*}
If one writes out all details of the construction of the present paper, this string \( R \)
becomes completely explicit, an absolute constant.
But in the reasoning below, we treat it as a parameter.

Let us provide a couple of \df{extra primitives} to the rules.
First, they have access to the parameter \( k \) of machine \( M=M_{k} \), 
to define the transition function
 \begin{align*}
            \tau_{R,k}(\va).
 \end{align*}
The other, more important, new primitive is a special instruction
 \begin{align*}
   \WriteProgramBit
 \end{align*}
in the rules.
When called, this instruction makes the assignment \( \Work\gets R(\Index) \).
This is the key to self-simulation: \emph{the program has
access to its own bits}.
If \( \Index=i \) then it writes \( R(i) \) onto the current position of the \( \Work \) track.


\subsubsection{Simulating the rules}

By convention, in our fixed flexible universal machine \( \Un \),
program \( p \) and input \( x \) produce an output \( \Un(p,x) \).
Since the structure of all rules is very simple, they can be read and
interpreted by \( \Un \) in reasonable time:

\begin{theorem}
There is a constant string called \( \Interpr \) with the property that for
all positive integers \( k \), string \( R \) that is a
sequence of rules, and a pair of bit strings \( \va=(a_{0},a_{1}) \) with \( a_{j}\in\Sigma_{k} \),
 \begin{align*}
  \Un(\Interpr,R,0^{k},\va)=\tau_{R,k}(\va).
 \end{align*}
The computation on \( \Un \) takes time \( O(\abs{R}\cdot \abs{\va}) \).
\end{theorem}

The proof parses and implements the rules in the string \( R \); each of these rules
checks and writes a constant number of fields.

Implementing the \( \WriteProgramBit \) instruction is straightforward:
Machine \( \Un \) determines the number \( i \)
represented by the simulated \( \Index \) field, 
looks up \( R(i) \) in \( R \), and writes it into the simulated \( \Work \) field.

Note that there is no circularity in these definitions:
  \begin{itemize}
  \item 
The instruction \( \WriteProgramBit \) is written \emph{literally}
in \( R \) in the appropriate place, as ``\(\WriteProgramBit \)''.
The string \( R \) is \emph{not part} of the rules (that is of itself).  
  \item On the other hand, the computation in
\( \Un(\Interpr,R,0^{k},\va) \) 
has \emph{explicit} access to the string \( R \) as one of the inputs.
  \end{itemize}

Let us show the computation step invoking the ``self-simulation'' in detail.
In the earlier outline, step~\ref{i:comp.trans} of Section~\ref{sec:computation-phase},
said to compute \( \tau^{*}(\tilde\va) \)
(for the present discussion, we will just consider computing 
\( \tau^{*}(\va)=\tau_{k+1}(\va) \)), where \( \tau=\tau_{k} \),
and it is assumed that \( \va \) is available on an appropriate auxiliary track.
We give more detail now of how to implement this step:

\begin{enumerate}
\item Onto the \( \Work \) track, write the string \( R \).
To do this, for \( \Index \) running from 1 to \( \abs{R} \), 
execute the instruction \( \WriteProgramBit \) and move right.
Now, on the \( \Work  \) track, replace it with \( \ang{\Interpr,0^{k+1},R,\va} \).
Here, string \( \Interpr \) is a constant, so it is just hardwired.
String \( R \) already has been made available.
String \( 0^{k+1} \) can be written since the parameter \( k \) is available.
String \( \va \) is available on the track where it is stored.
\begin{sloppypar}
 \item Simulate the universal automaton \( \Un \) on track \( \Work \):
   it computes \( \tau_{R,k+1}(\va)=\Un(\Interpr,R,0^{k+1}, \va) \)
as needed.  
\end{sloppypar}
\end{enumerate}

This achieves the forced self-simulation.
Note what we achieved:

\begin{itemize}
  \begin{sloppypar}
\item On level 1, the transition function \( \tau_{R,1}(\va) \) is defined completely
when the rule string \( R \) is given.
It has the forced simulation property by definition, and
string \( R \) is \emph{``hard-wired''} into it in the following way.
If \( (\va',d)=\tau_{R,1}(\va) \), then
\begin{align*}
  a'_{0}.\Work=R(a_{0}.\Index)
\end{align*}
whenever \( a_{0}.\Index \) represents a number between 1 and \( \abs{R} \),
and the values \( a_{0}.\Sweep \), \( a_{0}.\Addr \) satisfy the conditions
under which the instruction \( \WriteProgramBit \) is 
called in the rules (written in \( R \)).
      \end{sloppypar}

      \begin{sloppypar}
\item The forced simulation property of the \emph{simulated}
transition function \( \tau_{R,k+1}(\cdot) \) is 
achieved by the above defined computation 
step---which \emph{relies on} the forced simulation property of \( \tau_{R,k}(\cdot) \).
              \end{sloppypar}
\end{itemize}

\begin{remark}
This construction resembles the proof of Kleene's fixed-point theorem.
\end{remark}

\subsection{Transfer phase}\label{sec:TransferPhase}

In the transfer phase the \( \Transfer \) rule takes over: control will be transferred to the
neighbor colony in the direction of the simulated head movement: this is
called the direction of the transfer, or the \df{drift}.
As we will see the transfer phase takes only a constant number of sweeps:
three pairs of sweeps should be sufficient (see the details below).
During this phase, the range of the head
includes the base colony pair and a neighbor colony
in the direction of the drift called \df{target colony}, including possible bridges between them.
The bridges present some extra complication; let us address it.

\begin{sloppypar}
The sweep number in which we start transferring in direction \( \delta \) is called
\( \TransferSw(\delta) \), the \df{transfer sweep}.
We have \( \TransferSw(-1) =\TransferSw(1)+1 \).  
\end{sloppypar}

\begin{definition}[Adjacency of cells]\label{def:adjacent}
  Cells \( a \) and \( b \) are \df{adjacent} if \( \abs{a-b}=\B \).
  Otherwise, if \( \B < \abs{a- b} < 2\B \), then
  \( a \) and \( b \) are two \df{non-adjacent neighbor cells}.
For the sake of the present discussion, a \df{colony} is a sequence of \( \Q \) adjacent
cells whose \( \Addr \) value runs from \( 0 \) to \( \Q-1 \).
It may be \df{extended} by a bridge of up to \( \Q-1 \) adjacent cells.

If the bodies of two cells are not adjacent, but are at a distance \( <\B \) then the space
between them is called a \df{small gap}.
We also call a small gap such a space between the bodies of two colonies.
On the other hand, if the distance of the bodies of two colonies is \( >\B \) 
but \( <\Q\B \) then the space between them is called a \df{large gap}.
\end{definition}

Before the transfer phase, the base colony pair consists of two colonies,
having cells of kind \( \Member_{0} \) and \( \Member_{1} \).
There may be a bridge between them, of \( <\Q \) cells of kind \( \Bridge \)
adjacent to each other, and extending one of the base
colonies (see Definition~\ref{def:extends} and the remark following it).

The transfer phase moves the base colony pair left, if \( \Drift=-1 \), and right, if \( \Drift=1 \)
in the left base colony.

Consider \( \Drift=-1 \) first.
The kind of cells of the left base colony will turn from \( \Member_{0} \) to \( \Member_{1} \).
Then a bridge is built to the left (extending the \( -1 \) value on the \( \Drift \) track).
This bridge can override an opposite old bridge (``old'' meaning 
that its \( \Sweep \) is maximal) or move 
into an empty area, kill other bridge cells or stem cells if they are not adjacent, while it extends.
There are two ways that this bridge extension can finish.
\begin{itemize}
\item It stops at the boundary of another colony on the left
  (which consists of cells of type \( \Member_{0} \)).
  (This can happen in the very first step, in which case no bridge is built.)
  Then this colony becomes the new left base colony, and the head moves to its left end
  (extending again the \( -1 \) value on the \( \Drift \)  track).
  
\item The bridge reaches the size of \( \Q \) cells.
  In this case, the bridge will be converted into the new left base colony, the kind of its cells
  becoming \( \Member_{0} \).
  \end{itemize}
Recall that the \( \Adj \) field determines
whether the current cell is adjacent to the cell  where the head came from.
After the transfer stage, we write the \( \Adj' \) track of the
new left base colony: it becomes \( 0 \) if either there is a nonempty bridge,
or there is a gap (found with the help of the \( \Adj \) field) between the old and new left base colony.
This is done again three times, storing candidate values into \( \Hold[j].\Adj' \),
repeating and doing a majority vote.

\begin{sloppypar}
In the last sweep, 
in the old base colony pair, if the majority of \( \cHold[j].\Doomed \), \( j=1,\dots,3 \), 
is 1 then turn the scanned cell into a stem cell: 
in other words, carry out the destruction.
\end{sloppypar}

The case \( \Drift=1 \) is analogous, with the following logical changes.
First the bridge between the base colonies is possibly rebuilt, to extend the left base colony.
Then the kind of cells of the right base colony will turn from \( \Member_{1} \)
to \( \Member_{0} \), and it becomes the new left base colony.
Then a bridge is build to its right (extending the \( 1 \) value on the \( \Drift \) track).
It stops at the boundary of another colony on the right
(which consists of cells of type \( \Member_{1} \)), or becomes itself a colony with elements having
kind \( \Member_{1} \).

\section{Health}            \label{sec:health}

The main part of the simulation uses an error-correcting
code to protect information stored in \( \Info \) field.
However, faults can ruin the simulation structure and disrupt the simulation itself.
The error-correcting capabilities of the code 
will preserve the content of the \( \Info \) track as long as the coding-decoding
process implemented in the simulation is carried out.
The correctness of the process itself relies only on
the structural integrity of a configuration; this in its turn is maintained with the help of a small number
of fields.
Below we outline the necessary relations among them 
allowing the identification and correction of local damage.

A configuration with local structural integrity will be called \df{healthy}.
\begin{sloppypar}
\begin{remark}
In all discussions of the health of a configuration \( \xi=(A,\h,\vhc) \), we can ignore
the current cell position \( \hc_{0}=\xi.\curcell \), since at all switching times 
it agrees with \( \h=\xi.\pos \).
\end{remark}
\end{sloppypar}

All configurations are made up of homogenous segments.

\begin{definition}[Homogenous segment]\label{def:segments}
  A segment of non-stem adjacent neighbor cells is \df{homogenous} if
  if its addresses are increasing from left to right, and its members
  have the same value of \( \Kind \), \( \Sweep \), \( \Drift \), \( \Adj' \).
The \df{left end} of a segment is the left edge of its first cell, and its \df{right end} is 
the right edge of its last cell.
\end{definition}

A key property of our definition of health is that it is completely determined by the
types of segments allowed and the types of boundaries allowed between them.
This allows the local detection of failures of health.
The big picture is that cells of a healthy configuration are grouped into gapless colonies, with
certain transitional segments inside and between them.
\begin{itemize}
\item There is a base colony-pair, with possibly a bridge between them going in the direction
  of its own drift.
  If the sweep inside this colony-pair shows that the \( \Comp \) rule is being performed then
  the sweep boundaries allowed inside the colony-pair are the front, and the edges
  according to past turns at the end of the interior of the base colony-pair.
  Outside each edge is a segment with a lower sweep number.

\item  During the first sweep of transfer, the base colony (of the pair)
  in the direction of the drift is possibly extended by a bridge towards the target colony.
  This bridge may be in the process of consuming an earlier bridge extended in the opposite direction.
  In the later part of the first sweep of transfer, the bridge already reaches
  the target colony.

  If the bridge extends to a full colony then this is converted to the corresponding kinds of member
  cells in the second sweep of the transer.

  In the later sweeps of the transfer, segments ahead and behind the front show the changes done
  by the transfer phase, including the change of \( \Adj' \) in the last sweep.
  
\item Non-base colonies are called \df{outer colonies}.
If an outer colony is not adjacent to its neighbor colony closer to the base,
then it is extended by a bridge that covers this gap.

\end{itemize}
\Pnote{Pictures!}

Let us define outer cells and boundary types in more detail.

\begin{definition}[Outer cells, desert]\label{def:outer-cells}
    Recall the definition of the sweep value
    \(  \Last(\delta)  \) \\ from~\eqref{eq:Last}.
    For \( \delta \in \{ -1,1 \} \), if a cell is stem or has
\( \Drift = \delta \),  \( \Sweep = \Last(\delta) \)
    then it will be called a \df{right outer cell} if
    \( \delta = -1 \), a \df{left outer cell} if \( \delta = 1 \).
\end{definition}

According to this definition, a stem cell is both a left and a right outer cell.
Recall Definition~\ref{def:front} of the front.


\begin{definition}[Boundaries]%\label{def:segments}
% \begin{description}
% \item[Outer segment]
% Outer cells that could belong to the same extended colony.

% \item[Workspace segment]  
%   Part of an extended base colony, or part of a former outer
%   colony that is in the process of being transferred to.
%   The latter is called a \df{target segment}, its colony the \df{target colony}.
%   The cells of the target segment are not outer cells since the their sweep shows the start of transfer.
%   It is not extended by a bridge.
% \end{description}
A boundary of a homogenous segment is called \df{rigid} if its address is the end 
address of a colony in the same direction.
A \df{boundary pair} 
is a right boundary followed by a left boundary at distance \( <\B \).
It is a \df{hole} if the distance is positive.
It is \df{rigid} if at least one of its elements is.
\end{definition}

The health of a configuration \( \xi \) of a generalized Turing machine 
\( M \) is defined over a certain interval \( A \).
It depends on \( \xi_{\tape}(A) \), further on whether \( \xi_{\pos} \) is in \( A \) and
if it is, where.
But we will mention the interval \( A \) explicitly only where it is necessary.
In particular, some of the structures described above may fall partly or fully
outside \( A \).

Defining health formally can be done mechanically on the basis of the above informal description,
but the detaile description would be tedious.

% The definition below adds more detail to the above informal description of a healthy configurations.

% \begin{definition}[Healthy configuration]\label{def:healthy}
% We require that the mode be normal, and the following conditions hold, with \( \delta=\Drift \).

% \begin{description}
% \item[Segments]
%   All non-stem cells belong to full extended colonies.
%   In more detail:
%   \begin{itemize}
%   \item An \df{extended left/right base colony} consisting of cells of kind
%     \( \Member_{0}/\Member_{1} \), possibly extended by bridges.
%     In the first transfer sweep, cells of type \( \Member_{0} \) may be in the process of turning
%     into \( \Member_{1} \) or vice versa, according to the drift.
%   \item \df{Extended outer colonies}.
%   \item Desert filling out the gaps between the above parts.
%   \end{itemize}
%   The following kinds of segment can occur.
%   \begin{itemize}    
%   \item If \( \Sweep = \TransferSw(\delta) \) then the base colony pair is being moved left or right
%     according to \( \delta \).
%     This involves a possibly a bridge to be extended, and also the change of \( \Member_{j} \) to
%     \( \Member_{1-j} \) as the transfer requires.
%   \item If \( \Sweep = \TransferSw(\delta) +1 \) and the transfer did not take place yet,
%     then the whole bridge is being converted into a new base colony
%     as the head is traveling in direction \( -\delta \).
%   \item In the last sweep, the new member cells obtain the \( \Adj' \) values are changed as needed.
%   \end{itemize}
%   These are the only possible segments to be seen in a healthy area.
        
%     \item [The front] 
%       The farthest position \( \front(\xi) \) to which the head has 
%       advanced before starting a new zig is called the \df{front}:
%       it can be computed from the fields \( \xi.\pos \) and \( \ZigDepth \) of the current cell, 
%       but can also be reconstructed from the tape, namely from the \( \Sweep \) track.
%       It is always inside the extended base colony or the target.

%      % \item[Workspace]

%      %   \begin{sloppypar}
%      %      The \df{workspace} is an interval of non-outer cells, such that:         
%      %    \end{sloppypar}
        
%      %    \begin{itemize}
        
%      %    \item For \( \Sweep < \TransferSw(\delta) \), it is equal to the base colony.

%      %    \item In case of \( \Sweep = \TransferSw(\delta) \), it is the smallest interval including
%      %          the base colony and the cell neighboring to \( \front(\xi) \) on the side of the base colony.

%      %    \item If \( \TransferSw(\delta) < \Sweep < \Last(\delta) \),
%      %          then it is equal to the union of the extended base colony and the target.

%      %    \item When \( \Sweep = \Last(\delta) \), it is the smallest interval including the target
%      %          (future  base) colony and \( \front(\xi) \).
%      %    \end{itemize}

%    \end{description}
 % \end{definition}

A tape configuration is called \df{healthy} on an interval \( A \)
when there is a head position (possibly outside \( A \) ) and a 
state that turns it into a healthy configuration.

 \begin{definition}
The tuple of fields
\begin{align*}
   \Core =(\Addr, \Sweep, \Drift, \Kind)
 \end{align*}
is called the \df{core}.   
 \end{definition}

Health only depends on the state, including the 
zigging field \( \Z \), of the current cell, further the  \( \Core \) track
and the lack of heal or rebuild marks on the tape.
Note that in a healthy configuration every cell's \( \Core \) field
determines the direction in which the front is found, from the point of
view of the cell.

A violation of the health requirements can sometimes be noted immediately:

\begin{definition}[Coordination] \label{def:coordinated}
   The current cell pair is \df{coordinated} 
   if it is possible for them to be together in a healthy configuration.
\end{definition}

% Recall that in Rule~\ref{alg:main1},
In normal mode, if lack of coordination is discovered then the healing procedure is called.
The following lemmas show 
how local consistency checking will help achieve longer-range consistency.

\begin{lemma}\label{lem:coordination1}
  In a healthy configuration, each \( \Core \) 
  value along with \( \Z \) determines uniquely the 
  \( \Core \) value of the other cell of the current cell pair, with the following exceptions.
  \begin{itemize}
  \item During the first transferring sweep, while
    creating a bridge, the front can be a stem cell or the first cell of an outer colony.
  \item Every jump backward from the target colony can end up on the last cell of a 
    bridge (whose address is not recorded in the cell from which the head is jumping)
    or the last cell of the base colony.
  \end{itemize}
  \end{lemma}
  \begin{proof}
  To compute the values in question, calculate \( \Z \) steps backwards from the front,
  referring to the properties listed above.
\Pnote{Elaborate!}    
  \end{proof}

\begin{lemma}[Health extension]\label{lem:health-extension}
  Let \( \xi \) be a \emph{tape} configuration that is healthy on intervals \( A_{1}, A_{2} \) 
where \( A_{1}\cap A_{2} \) contains a whole cell body of \( \xi \).
Then \( \xi \) is also healthy on \( A_{1}\cup A_{2} \).
\end{lemma}
\begin{proof}
  The statement follows easily from the definitions.
\Pnote{Elaborate!}    
\end{proof}

In a healthy configuration, the possibilities of finding non-adjacent neighbor
cells are limited.

\begin{lemma}\label{lem:two-domains}
  An interval of size \( <\Q \) over which the configuration \( \xi \) is healthy
contains at most two maximal sequences of adjacent non-stem neighbor cells.
\end{lemma}
\begin{proof}
Indeed, by definition a healthy configuration consists of full extended colonies, with 
possibly stem cells between them.
An interval of size \( <\Q \) contains sequences of adjacent cells 
from at most two such extended colonies.
\end{proof}

% Let us classify the boundary pairs possible in a healthy configuration.
% Rigid pairs:

% \begin{enumerate}[label=\upshape{(r\arabic*)}, ref=r\arabic*]
% \item\label{i:rigid.outer-workspace}
%   Between an outer extended colony or a desert, and a colony closer to the base.
% \item\label{i:rigid.bridge-target} Between the extended base colony 
% and the target or an outer colony.
% \end{enumerate}

% Non-rigid pairs are at the front:
% \begin{enumerate}[label=(nr\arabic*), ref=nr\arabic*]

%   \item\label{i:nr.bridge-bridge} End of a new bridge in direction \( \delta \)
% not within distance \( <\B \) of a rigid boundary of a colony of drift \( -\delta \) or its own target.
% On the other side is either desert or the end of an old bridge.

%   \item\label{i:nr.aligned} Between aligned segments:
%     \begin{enumerate}[label=(\arabic*), ref=nr\arabic{enumi}.\arabic*]
%      \item\label{i:nr.aligned.differ-1} Between sweep values differing by 1,
%      \item\label{i:nr.aligned.bridge-target} between a new bridge and the target it is being converted into,
%      \item\label{i:nr.aligned.target-member} 
% between a target in direction \( \delta \) and the remaining segment of member cells, of drift \( -\delta \),
%      \item\label{i:nr.aligned.member-target}  between a target and the member cells replacing it in the last sweep,
%      \item\label{i:nr.aligned.internal-end} 
% between an internal segment and an end segment (see Definition~\ref{def:segments}).
%     \end{enumerate}

% \end{enumerate}

% The following is worth noting.

% \begin{lemma}
% In a healthy configuration, any interval of size \( <\Q\B \) contains at most 3 boundary pairs,
% only one of which can be  a hole.
% \end{lemma}
% \begin{proof}
% Any interval of size \( <\Q\B \) contains at most one rigid left boundary and
% one rigid right boundary.
% Any nonrigid boundary coincides with the front.
% A hole can exist only at the end of a bridge.
% If there are two bridges, then the hole can only be between their ends.
% \end{proof}

\begin{lemma}\label{lem:infer-between}
In a healthy configuration, 
a cell's content shows whether it is an outer cell, colony cell, bridge cell or target colony cell.
It also shows whether the cell is to the left or to the right of the front.

The \( \Core \) track of a homogenous segment can be satisfactorily reconstructed
from any of its cells.
The reconstruction is unique, with one exception: in case the segment is not internal, the
\( \Sweep \) values are not uniquely defined, only the parity and the direction of its 
change between the ends.
\end{lemma}
\begin{proof}
The information mentioned in the first sentence is explicit in some fields.
About the front: if the cell is outer then \( \Drift \) shows its direction from the front.
In case it is not outer, then the parity of \( \Sweep \) shows it: odd values are on the left, even 
ones on the right. 

The uniqueness of reconstruction is a direct consequence of the definitions.
\end{proof}

\section{Healing and rebuilding}\label{sec:healing}

\subsection{Annotation, admissibility, stitching}\label{sec:stitching}

In this section we show how to correct configurations of machine \( M \)
that are ``almost'' healthy.

\begin{definition}[Annotation]\label{def:annotation}
  An \df{annotated configuration} is a tuple
  \begin{align*}
    (\xi,\chi, \cI),
  \end{align*}
  with the following meaning.

  \( \xi \) is a configuration.

  \( \cI \) is a set of disjoint intervals called \df{islands}.
Their complement is clean.

  \( \chi \) is a healthy configuration differing from \( \xi \) only in the islands.

The \df{base colony} of \( (\xi,\chi, \cI) \) is that of \( \chi \).
The head is \df{free} when it is not in any island,
and the observed cell is in normal mode, coordinated with its pair.
  \end{definition}

\begin{definition}[Admissibility]\label{def:admissible}
An annotation is \df{admissible} on an interval \( K \) if the following holds on any subinterval 
\( J \) of \( K \) of size \( \Q\B \):
\begin{varenum}[series=admissible]{a}
\item The disorder of \( J \) is covered by 3 intervals, each of size \( \le \beta'\B \).
\item There are at most 3 islands in \( J \), each of size at most \( \cns{island}\beta\B \).
\end{varenum}
A configuration is \df{admissible} on \( K \) if it can be annotated in a way admissible on \( K \).
\end{definition}

We will show that a configuration admissible over an interval of a certain size can be locally corrected;
moreover, in case the configuration is clean then this correction
can be carried out by the machine \( M \) itself.
For the time being, we will just talk about an admissible configuration, without specifying the interval
\( K \).

We will deal with the cleaning process later, but it will imply that,
in the absence of new noise, islands will not grow much, and the ones near the head
will be eliminated.

\begin{definition}[Substantial segments]\label{def:substantial}
Let \( \xi(A) \) be a tape configuration over an interval \( A \).
A homogenous segment of size at least \( 7\cns{island}\beta\B \)
will be called \df{substantial}.
The area between two neighboring maximal
substantial segments or between an end of \( A \) and the closest substantial segment in \( A \)
will be called \df{ambiguous}.
It is \df{terminal} if it contains an end of \( A \).
  Let
 \begin{align*}
     \Delta &= 39\cns{island}\beta, % ,\quad \Delta'=\Delta+3\cns{island}\beta. %?
\\  \E  &\ge 6\Delta. %?
 \end{align*}
\end{definition}

\begin{lemma}\label{lem:ambiguous}
Consider an admissible configuration.
In a substantial segment, each half contains at least one cell outside the islands.

If an interval of size \( \le  \Q\B \) of a tape configuration \( \xi \) differs from a  healthy tape 
configuration \( \chi \) in at most three islands, then 
the size of each ambiguous area is at most \( \Delta\B \).
\end{lemma}
\begin{proof}
The first statement is immediate from the definition of substantial segments 
There are at most 3 boundary pairs in \( \chi \) at a total size of \( 3\B \), and
3 islands of size \( \le\cns{island}\beta\B \).
There are at most 5 non-substantial segments of sizes \( < 7\cns{island}\beta\B \)
between these: this adds up to
\begin{align*}
 < (3+3\cdot\cns{island}\beta+ 5\cdot 7\cdot\cns{island}\beta)\B<39\cdot\cns{island}\beta\B=\Delta\B.
 \end{align*}
\end{proof}

The following lemma forms the basis of the cleaning algorithm.

\begin{lemma}[Stitching]\label{lem:stitching}
In an admissible configuration, inside a clean interval,
let \( U,W \) be two substantial segments separated by an ambiguous area \( V \).
It is possible to change the tape on \( U,V,W \) using only information in \( U,W \) in such a 
way that the tape configuration over \( U\cup V\cup W \) becomes healthy.
Moreover, it is possible for a cellular automaton to do so gradually, keeping admissibility, and
changing the tape in \( U \) or \( W \) or enlarging \( U \) or \( W \)
gradually at the expense of \( V \).
\end{lemma}
We do not worry about the feathering property now, since the machine in question can choose
where to make its turns.
\begin{Proof}
We distinguish several cases, based on the kind of segments involved.
At any step, if we find that \( U\cup V\cup W \) is healthy then we stop.
\begin{step+}{step:stitching.mergeable}
Assume that \( U,W \) both belong to the same extended colony: either an extended base colony, or 
an outer extended colony, or the target.
\end{step+}
\begin{prooof}
If both belong to the interior of the area indicated, then the merging is simple.
In this case, by Lemma~\ref{lem:infer-between}, the content of the \( \Core \) track 
of \( V \) in the healthy configuration is completely determined by that of \( U,W \),
So the intervals \( U \) and \( W \) can be gradually extended towards each other in any order, 
overtaking \( V \).

Suppose that they do not belong to the interior and are, for example towards the right from it.
In this case, the \( \Sweep \) value may be decreasing in both \( U \) and \( W \) to the right.
But since \( U,W \) themselves may overlap with islands, it may happen that the \( \Sweep \) value
on the right end of \( U \) is smaller than the \( \Sweep \) value on the left end of \( W \).
In this case, before extending \( U,W \) towards each other, the \( \Sweep \) values in both
may need to be changed.

By Lemma~\ref{lem:ambiguous}, the left and right halfs of \( U \) contains a cell \( u_{1},u_{3} \) 
each, not belonging to any island, and similarly with \( w_{1},w_{3} \) in \( W \).
Let \( u_{2} \) be leftmost cell of the right half of \( U \) and \( w_{2} \) the rightmost cell of the left 
half of \( W \).
Then
\begin{align*}
 \Sweep(u_{1})\ge \Sweep(u_{2})\ge \Sweep(u_{3})\ge \Sweep(w_{1})\ge \Sweep(w_{2})\ge \Sweep(w_{3}).   
 \end{align*}
So the transformation will change \( \Sweep \) to \( \Sweep(u_{2}) \) everywhere in the right half of \( U \),
and to \( \Sweep(w_{2}) \) everywhere in the left half of \( W \).
After this, it will extend \( U \) towards \( W \), overtaking \( V \) and keeping \( \Sweep \) constant.

In both of the above cases, if \( U  \) and \( W \) belonged to different sides of the front, then the
new front will be the new boundary between \( U \) and \( W \).
\end{prooof} % step:stitching.mergeable

\begin{step+}{step:stitching.not-front}
Suppose now that \( U,W \) do not belong to the same extended colony, but they are on 
the same side of the front: without loss of generality, to the left of it.
\end{step+}
\begin{prooof}
In this case, only \( U \) can be outer: suppose that it is.
Now \( W \) is either in a target or in the base colony.
The base colony cannot have an extension to the left, because given that the front
is to the right, the same sweep that created the extension would have created already a target, 
separating \( U \) from \( W \) too much.
So in both cases, \( W \) is close to its colony's the left end, which is in \( V \).
It must be extended to this left end.
Following this, \( U \) must be extended until its reaches \( W \).
The \( \Sweep \) values can be extended without change, there is no need to coordinate them
between \( U \) and \( W \).

Suppose that \( U \) is a target, and then \( W \) belongs to the extended base colony.
Since both \( U \) and \( W \) are in the interior, the \( \Sweep \) values must be equal, so they
can be extended without change.
Then \( U \) must be extended until its endcell, and then \( W \) must be extended to meet \( U \).
Since both \( U \) and \( W \) are in the interior, the \( \Sweep \) values must be equal, so they
can be extended without change.
\end{prooof} % step:stitching.not-front

\begin{step+}{step:stitching.front}
Suppose that \( U \) and \( W \) belong to different extended colonies, with the front between them.
\end{step+}
\begin{prooof}
The \( \Sweep \) values can be extended without change, there is no need to coordinate them
between \( U \) and \( W \).

Suppose that \( U \) contains no bridge cells: in this case extend it to the right until it hits
a colony endcell.
If you overlap \( W \), decrease \( W \) accordingly.
Due to admissibility, only bridge cells of \( W \) can be overwritten in this way
(colonies don't overlap in a healthy configuration).
Similarly, if \( W \) contains no bridge cells then extend it to the left, until it hits a colony endcell.
Again, only bridge cells of \( U \) can be overwritten in this way.

Only one of \( U \) and \( W \) can be in an outer extended colony, suppose that \( U \) is.
If \( W \) is a target then we already extended it to its limit.
Now extend the bridge of \( U \) to meet it.
If \( W \) is in an extended base colony then extend its bridge until either meets \( U \) or
reaches full length.
In the latter case, extend the bridge of \( U \) to meet \( W \).

If \( U \) is in a target then it is already at its right end: we extend the bridge of \( W \) to meet it.
\end{prooof} % step:stitching.front
\end{Proof}


\subsection{The healing procedure}\label{sec:healing-proc}

Structure repair will be split into two procedures.
The first one, called \df{healing}, performs
only local repairs of the structure: for a given (locally) admissible configuration,
it will attempt to compute a satisfying (locally) healthy configuration.
If it fails---having encountered a configuration that is not admissible, or
a new burst---then the \df{rebuilding} procedure is called, which is designed
to repair a larger interval.
On a higher level of simulation, 
this corresponds to the implementation of the ``cleaning'' trajectory properties.
The healing procedure runs in \( O(\beta) \) 
steps, whereas rebuilding needs \( O(\Q^{2}) \) steps. \Pnote{maybe even \( \Q^{3} \)?}

The description of the procedures looks as if we assumed that there is no noise or disorder.
The rules described here, however (as will be proved later), will clean an area locally under the 
appropriate conditions, and will also work under appropriately moderate noise.

The healing procedure does not even protect itself against any possible noise during it.
The only protection is that any one call of the healing procedure will change only 
a small part of the tape, essentially one cell: so a noise burst will have limited impact even if it
happens during healing.

One possible outcome of the healing procedure is \df{failure}. 
In this case the plan is to mark a ``germ'' of \( 4\beta \) cells and call rebuilding.
The healing procedure carries  out only one step of this plan, then it calls itself again.

Recall the parameters \( \Delta,\E \) introduced in Definition~\ref{def:substantial}.
Suppose that \( \rHeal \) is called at some position \( \z \).
Then it sets
\begin{align*}
\Mode\gets\Healing,\ \Heal.\Sweep \gets 1,\ \Heal.\Addr \gets 0.
 \end{align*} 
The healing procedure starts by surveying an interval \( \R \) of at least \( 2\E \) cells 
around its starting point \( z \).
(It will go a little farther than \( \E \) cells, in search of an allowed turning point, due to feathering.
If such a point is not found within \( 3\beta \) steps, healing fails.)
Whenever the head steps on a stem cell or creates a new cell, 
\( \Drift \) is set to point to \( \z \) (to make sure that the head does not get
lost in a homogenous segment of stem cells).

Suppose that in the survey a pair of substantial segments separated by an ambiguous area is found,
that is at least \( \Delta \) removed from the complement of \( R \). 
Choose the ambigous area closest to the center (of healing).
Then the first needed ``stitching'' operation (a single step) as defined in Lemma~\ref{lem:stitching}
is determined, and is performed.
If the ambigous area still remains, go to its cell closest to \( z \) and restart healing.
In case that we started from a clean admissible configuration, this operation creates a new admissible
configuration that is one step closer to being healthy.

If the required stitching operation is not found then, 
the surveyed area is found inadmissible, and healing fails.
Otherwise, if the head is at the front, healing is declared completed.
Otherwise, the healing center is moved one step closer to the front, and healing is restarted.

The healing operation defined this way has the following property.

\begin{lemma}\label{lem:combined-heals}
  Assume that the head moves in a noise-free and clean space-time rectangle \( \lint{a}{b}\times \lint{u}{v} \),
with \( b-a>3\E\B \), \( v-u>2\E\Tu \),
touching every cell of \( \lint{a}{b} \) at least once, and never in rebuilding mode.
Then at time \( v \), the area \( \lint{a+1.5\E\B}{b-1.5\E\B} \) is healthy.
\end{lemma}
\begin{proof}
In healing, the head will not move away from an ambiguous area before eliminating it, creating a healthy interval
\( I \) containing the two substantial intervals that originally bordered it.
If the head leaves this interval in normal mode and the zigging does not start new healing then the
healthy interval covered by zigging can be added to \( I \) to form an even larger healthy interval,
and this continues until new healing starts.
One of the substantial intervals of the new healing will be inside \( I \), and when it is completed,
\( I \) will be extended further.
The head may later return into the healthy interval \( I \), but it will then just continue the simulation without
affecting health while staying inside.  
\end{proof}

\subsection{Rebuilding}\label{sec:rebuilding}

If healing fails, it calls the rebuilding procedure.
This indicates that the colony structure is ruined in an interval of size larger than 
what can be handled by local healing.
Just as with healing, we will be speaking here only about a situation with no 
mention of disorder---but the final analysis will take disorder into account.

Since rebuilding makes changes on an interval of the length of several colonies,
it is important not only that it is invoked when it is needed, but that it is not
invoked otherwise!
A failed healing ends on a ``germ'' of cells marked for rebuilding,
with the state in rebuild mode.
Rebuilding starts by extending the germ to the left, in a zigging way,
expecting rebuild-marked cells on the backward zig.
Since the zig is larger than a burst, if rebuilding started from just a burst then this
will trigger healing, thus leaving the rebuilding mode.

The notion of front, of Definition~\ref{def:front} will be extended also to the rebuilding mode.

Here is an outline, for rebuilding that started from a cell \( z \) in the middle
of the germ.
The goal is that
\begin{itemize}
\item the process does not destroy any healthy colony more than
  \( 3\beta\B \) to the left of \( z \) or to the right of \( z \).
\item we end up with a new decodable area
  (see Definition~\ref{def:super-health}) extending at least one colony to the
  left and one colony to the right of \( z \).
\end{itemize}
Recall the stitching operation from Section~\ref{sec:stitching}.
\begin{description}
 \item[Mark] Starting from the germ, extend a rebuilding area  over \( 3\Q \) cells to the left 
and \( 3\Q \) cells to the right from \( z \).
Mark the area using the track \( \Rebuild.\Addr_{j} \) for \( j=\pm 1 \),
where addresses are counted from both 
ends of the rebuilding area, and the track \( \Rebuild.\Sweep \).
False rebuilding started by a burst will be recognized since
zigging must see an at least germ-sized marked area.
In what follows, every step that changes the configuration must be accompanied by zigging, to check that
the rebuilding is indeed going on.

 \item[Survey and Create]
More details of this stage will be given below.
It looks for existing colonies (possibly needing minor repair) in the rebuilding area, and 
possibly creates some.
As a result, we will have one colony called \( C_{\Left} \) 
on the left of \( z \), one called \( C_{\Right} \) on the right of \( z \),
and possibly some colonies between them.
Make all newly created colonies represent stem cells.
Declare \( C_{\Left} \) the base colony, direct all the others
with drifts and bridges towards it.
(The creation of a bridge may result also in the creation of a new 
colony if the bridge becomes \( \Q \) cells long.)
% Survey and possibly change additional \( 0.5\Q \) cells on the left of \( C_{\Left} \) and \( 0.5\Q \) cells
% on the right of \( C_{\Right} \), making the whole interval healthy.
The interval covering \( C_{\Left} \) and \( C_{\Right} \)
will be called the \df{output interval} of rebuilding.

\item[Mop] Remove the rebuild marks, shrinking the rebuilding area onto the left end of \( C_{\Left} \).
\end{description}

\subsubsection*{Details of the Survey and Create stage}

\begin{varenum}{s}
\item\label{i:survey.left}
Search in the marked area on left of \( z \) for a colony \( C \),
or a set of cells that looks stitchable by up to 3 healings into a colony.
The first substantial segment should be at least \( 3\beta \) cells to the left of \( z \),
to make sure that \( C \) is \df{manifestly} to the left of \( z \).
If the search and the stitching attempt are successful, mark \( C \) as \( C_{\Left} \).

Repeat this search for a colony manifestly to the right of \( z \): if found, call it \( C_{\Right} \).

\item Suppose that only \( C_{\Left} \) is found, then create \( C_{\Right} \).
The other case is symmetrical.

\item Suppose that neither \( C_{\Left} \) nor \( C_{\Right} \) have been found.
Then search the whole area, starting from the left, for a colony.
If no such colony is found then create \( C_{\Left} \) and \( C_{\Right} \).

\item\label{i:survey.both} Suppose that both \( C_{\Left} \) and \( C_{\Right} \) 
have been determined (found or created).
Then search between them, from the left, for (stitcheable) colonies, one-by-one.

\item Suppose that only a colony has been found that 
is not manifestly to the left or right of \( z \); then either the left end is manifestly to the left or
the right end is manifestly to the right of \( z \).

Suppose that the left end it is manifestly to the left of \( z \), then call the colony \( C_{1} \).
Then create \( C_{\Left} \) on the left of  \( C_{1} \).
Now search for another one on its right (it is not manifestly on the right).
If not found, create \( C_{\Right} \), manifestly on the right of \( z \).
If found call it \( C_{2} \), and create \( C_{\Right} \) on its right.
The other case is symmetrical.

\end{varenum}

The steps above requiring the creation of colonies and bridges are destructive
(the stitching ones are not).
Therefore before a creation step, the whole survey preceding it is performed twice, 
marking the result in all rebuilding cells.
The required actions (erasing cells in order to build new ones in their place)
are then only performed if the two survey results are identical---otherwise alarm is called.

To defend against the effects of a burst during the rebuild procedure,
any decision on creation is made twice.
The two surveys on which the decision is based note their results everywhere on two different tracks.
These results must agree, otherwise rebuilding fails and new healing (followed probably by new rebuilding)
is started.
(Marked cells from some interrupted rebuilding may remain even after the mop-up operation.
These may trigger new healing-rebuilding later.)


\section{Scale-up}

\subsection{Annotated history}

It is convenient to introduce some additional structures when discussing
the effects of moderate noise and their repair.

The following definitions help to extend the notion of annotation to histories.

\begin{definition}[Distress and relief, safety]\label{def:distress}
Consider a sequence of annotated configurations over a certain time interval.
If the head is free (see Definition~\ref{def:annotation}), then
the time (and the configuration) will be called \df{distress-free}.
A time that is not distress-free and is preceded by
a distress-free time will be called a \df{distress event}.
This can be of two kinds: the head steps onto an island, or a burst occurs
(creating an island and leaving the head in it).

% The direction of the last \( \Z \) non-turning moves of the front before the distress event
% will be called the \df{pre-distress sweeping direction}.

Consider a time interval \( K \) starting with a distress event and
ending with a distress-free configuration.
Let \( J \) be the interval of tape where the head passed during \( K \), then 
we will call \( J\times K \) a \df{relief event}, if the following holds.
% the free head making \( \Z \) non-turning moves.
% Their direction is called the \df{post-distress sweeping direction}.
% If it coincides with the pre-distress sweeping direction then we will say that
% \df{no turn} occurred.

\begin{alphenumIn}
\item Any new islands occurring in \( J \times K \) are due to some new burst.
\item The island that started the distress event disappears by the end of \( K \).
% \item Provided no turn occurred, the only possible island intersecting \( J \) 
% at the end of \( K \) belongs to some island caused by a burst during \( K \).
% (In other words, the \emph{old} islands will be eliminated in \( K \).
% This is always the case if the distress occurs in the interior of a colony.)
\end{alphenumIn}
\end{definition}

In a relief event, it is possible to leave behind some islands other than the one initiating the distress.

% We will call, informally now, any area affected by a single burst of faults an \df{island}.
% Let us argue (informally, without replacing any later, formal argument), that the simulation
% does not have to correct more than three islands in any area of size \( \Q\B \). 
% We assume that the simulated program obeys the
% feathering condition: so if, say, the head comes from the left neighbor colony
% then at the end of the work period can turn back to the left only if 
% the previous time it visited the colony, it has passed it over from right to left.

\begin{example}[Three islands]\label{xmp:3-islands}
  Suppose that the head has arrived at \( C \) from the left,
performs a work period and then passes to the right.
In this case, if no new noise burst occurs then we expect that
all islands found in \( C \) will be eliminated by the healing procedure.
On the other hand, a new island \( I_{1} \) can be deposited (in the last pass).
We can assume that there is no island on the left of  \( I_{1} \) 
within distance \( \Q\B \), since the noise burst
causing it would have been too close to the noise burst causing \( I_{1} \).

Consider the next time (possibly much later), when the head arrives (from the right).
If it later continues to the left, then the situation is similar to the above.
Island \( I_{1} \) will be eliminated, but a new one may be deposited.
But what if the head turns back right at the end of the work period?
If \( I_{1} \) is close to the left end of \( C \), then due to the feathering
construction, the head may never reach it to eliminate it; moreover,
it may add a new island \( I_{2} \) on the right of \( I_{1} \).
We can also assume, similarly to the above, 
that there is no island on the right of  \( I_{2} \) within distance \( \Q\B \).

When the head returns a third time (possibly much later), 
from the right, it will have to leave on the left.
The islands \( I_{1},I_{2} \) will be eliminated as they are passed over
but a possible new island
\( I_{3} \), created by a new burst (before, after or during the elimination), may remain.
We can also assume, similarly to the above, 
that there is no island on the right of  \( I_{3} \) within distance \( \Q\B \).

In the work period where the head deposits island \( I_{2} \) near the left colony
end, it may repeatedly dip into \( I_{2} \) at the descending end of a zig at a
right turn.
During this dip it can expand the islands
\( I_{1}, I_{2} \) and then emerge on the right, with the healing unfinished.
 \end{example}

We will consider the annotation of histories over a limited space-time region, but will
not point this out repeatedly.

\begin{definition}[Annotated history]\label{def:annotated-hist}
An \df{annotated history} of a generalized Turing machine
    \begin{align*}
        M= (\Sigma, \tau, \Adj,\B, \Tu, \passno)
     \end{align*}
is a sequence of super-annotated configurations such that
the sequence of underlying configurations is a trajectory, and 
every distress event is followed by a relief event \( J\times K \) with 
\( \abs{J}=O(\beta\B) \) and \( \abs{K}= O(\beta^{2}\Tu) \). %?
\end{definition}

In what follows we will show that for any trajectory \( (\eta, \Noise) \) of a
generalized Turing machine \( M \) on any space-time rectangle on which the
noise is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse, if at the
beginning the configuration was super-healthy then the history can be annotated.
In the rest of the section we always rely on the assumption of  this 
sparsity property of the noise.

The main part of the proof is about obtaining relief  after a distress event.
Unlike in~\cite{burstyTuring13}, now islands 
may have their cell structure damaged: may contain disorder.
However, since \( \eta \) is a trajectory, as we will see the islands will be cleaned out.
So, relief will happen in two stages: cleaning, and correcting the structure.
This division is only possible for an observer: the machine has no
``disorder-detector'', we just rely on the cleanness-extending properties of a
trajectory introduced in Definition~\ref{def:traj}.

\subsection{Super-health}

\begin{definition}[Super-health]\label{def:super-health}
  Let \( \xi  \) be a clean configuration on an interval \( I \).
  We say that \( \xi \) is \df{super-healthy} if both ends of \( I \) coincide with the end of a 
  colony, and in each colony in \( I \), whenever the head is not in the last sweep, the \( \Info \)
  contains a valid codeword as defined in Section~\ref{sec:coding}.
  A tape configuration \( \xi \) is \df{super-healthy}
  if it can be extended to a super-healthy configuration.
\end{definition}

As indicated in Section~\ref{sec:model}, when dealing with the behavior
of machine \( M \) over some space-time rectangle, we will assume that the noise
over this rectangle is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse.
With Definition~\ref{def:Tu} of \( \Tus \) this means 
in simpler terms that at most one noise \df{burst} affecting an
area of size at most \( \beta\B \) can occur in any \( \gamma \) consecutive work periods.
In the present section, histories will always be assumed to have this property.

Some histories lend themselves to be viewed as a healthy development
that is disturbed only in some well-understood ways.
Added to such histories the information pointing out these disturbances will be called an annotation.
The proof of the error-correcting behavior of machine \( M \) (essentially the proof of the Transition Function
property of trajectories of Definition~\ref{def:traj} for the simulated machine \( M^{*} \))
will take the form of showing the possibility of annotation under sparse noise.
An annotation, as per Defnition~\ref{def:annotation}, marks some ways in which the health 
of a tape configuration has been be affected.
Now we extend annotation in order to deal with damage not only to health but also to information.

\begin{definition}[Super-annotation]\label{def:super-annotation}
  A \df{super-annotated configuration} is a tuple
  \begin{align*}
    (\xi,\chi, \cI, \cS),
  \end{align*}
  with the following meaning.

\( (\xi,\chi,\cI) \) is an annotated configuration.

\( \cS \) is a set of disjoint intervals called \df{stains}.
All islands are contained in stains.

We can change \( \chi \) into a super-healthy configuration by changing it only in the stains.
  \end{definition}

Recall Definition~\ref{def:admissible} of admissibility.
  \begin{definition}[Super-admissibility]\label{def:super-admissible}
A super-annotation is \df{super-admissible} on an interval \( K \) if it is admissible on \( K \),
further consider any interval \( J\subseteq K\) of size \( \le\Q\B \).
\begin{varenum}[resume=admissible]{a}
\item At most 3 stains intersect \( J \).
\item If \( J \) contains \( k \) stains, surrounded by some healthy area
(we already know \( k\le 3 \)), while the head is at a distance \( >2\B \) within the clean area,
then the total size of these stains is at most \( k\cdot\cns{stain}\B \), where
\begin{align}\label{eq:stain}
   \cns{stain} = (\f_{1}+2)\beta.
 \end{align}
\item\label{i:super-annotated.two-stains} If an outer colony in \( J \)
  intersects two stains then it simulates 
  a cell state with \( \Turned=1 \).
\end{varenum}
\end{definition}

Let us motivate requirement~\eqref{i:super-annotated.two-stains}.  
One stain can arise and remain somewhere for example if a burst occurs in the
last sweep of a work period at the bottom of a zig.
A second stain can remain at the end of a work period in which the simulated
machine makes a turn.
If for example the turn was to the left, then the head might not visit the end of the colony
where this stain is.
But this sets \( \Turned\gets 1 \) in the simulated cell.
Requirement~\eqref{i:super-annotated.two-stains} says that in an annotated
configuration, this is the only way for two stains to remain outside the
workspace.

A configuration may allow several possible super-annotations;
however, the valid codewords (referred to in the definition
of super-health) that can be recovered from it do not depend on the choice of the
annotation.

The following is an immediate consequence of the definition of rebuilding, namely the fact that
in a clean area and the absence of noise, if rebuilding is needed then it will be started and will succeed.

\begin{lemma}\label{lem:rebuild-health}
Suppose that a rebuilding procedure runs noiselessly from start to finish in a clean interval \( I \),
starting on the boundary of a subinterval \( J\subset I \) that is super-admissible at the beginning.
If \( K \) denotes the output interval of rebuilding, then \( J\cup K \) will be super-admissible.
\end{lemma}



\subsection{The simulation codes}\label{sec:sim-codes}

Let us now define formally the codes \( \varphi_{*k},\Phi_{k}^{*} \) needed
for the simulation of history \( (\eta^{k+1},\Noise^{(k+1)}) \) by history \( (\eta^{k},\Noise^{(k)}) \).
Omitting the index \( k \) we will write \( \varphi_{*},\Phi^{*} \).
To compute the configuration encoding \( \varphi_{*} \) we proceed first as
done in Section~\ref{sec:hier-codes}, using the code \( \psi_{*} \) there,
and then add some initializations:
In cells of the base colony and its left neighbor  colony,
the sweep and drift fields \( \Sweep \) and \( \Drift \) are set 
to \( \Last(1)-1 \),  \( 1 \), and \( \Last(1) \),  \( 1 \) respectively.
In the right neighbor colony, these values are \( \Last(-1) \) and \( -1 \) respectively.
In all other cells, these values are empty.
The \( \Addr \) fields of each colony are filled properly:
the \( \Addr \) of the \( j \) cell of a colony
is \( j \bmod \B^{*} \). \Pnote{Picture?}

The value \( \Noise^{(k+1)} \) is obtained by a residue operation
just as in Definition~\ref{def:sparsity} of sparsity.
It remains to define \( \eta^{*}=\eta^{(k+1)} \) when \( \eta=\eta^{k} \).
Parts of the history that are locally super-annotated will be called \df{clean}.
In the clean part,
if no colony has its starting point at \( x \) at time \( t \), set \( \eta^{*}(x,t)=\Vacant \).
Otherwise \( \eta^{*}(x,t) \) will be decoded from
the \( \Info \) track of this colony, at the beginning of its work period 
containing time \( t \).
More precisely:

\begin{definition}[Scale-up]\label{def:scaleup}
Let \( (\eta,\Noise) \) be a history of \( M \), where \( \Noise=\Noise^{(k)} \)
as in Definition~\ref{def:sparsity}.
We define \( (\eta^{*},\Noise^{*})=\Phi^{*}(\eta,\Noise) \) as follows.
Let \( \Noise^{*}=\Noise^{(k+1)} \).
Consider position \( x \) at time \( t \), and \( J=\rint{t-\Tus}{t} \).
If \( x \) is not contained in some interval \( I \) such that \( \eta \) is super-admissible over \( I\times J \)
then \( \eta^{*}(x,t)=\Bad^{*} \).
Assume now that it is contained, and let
\( \chi(\cdot,u) \) be some super-clean history satisfying \( \eta \) over \( I\times J \).
If \( x \) is not the start of a colony in \( \chi \) then let \( \eta^{*}(x,t)=\Vacant \); assume now that it is.
Then let \( t'\in J \) be the starting time in \( \chi \) of the work period of \( C \)
containing \( t \), and let \( \eta^{*}(x,t) \) be the value decoded from \( \eta(C,t') \).
In more detail, as said at the end of Section~\ref{sec:coding}, we apply the decoding
\( \psi^{*} \) to the interior of the colony it to obtain \( \eta(x,t) \).
\end{definition}

This definition decodes super-admissible intervals and histories for \( \eta \) into super-clean intervals and histories
of \( \eta^{*} \), while preserving the the property~\eqref{i:config.sharp-ends} of cleanness in Definition~\ref{def:config}.

% Recall the definition of a clean hole in Definition~\ref{def:clean-hole}.

% \begin{lemma}\label{lem:rebuild-clean}
%   Let  \( I \) be clean interval  of size \( 5\Q\B \).
%   Consider a noise-free time interval \( J \) during which the head spends time \( \ge 5\Tu \) in \( I \)
%   or passes through \( I \).
%   Then during \( J \) at some time the head will be found in a hole intersecting \( I \) that is
%   clean for \( M^{*} \).
% \end{lemma}
% \begin{proof}
% Assume, without loss of generality, that the head passes \( I \) from left to right.
% In the absence of noise, and inside a clean area,
% the healing procedure can only fail if it starts a rebuilding process.
% And a rebuilding process never fails.

% If the head passes \( I \) in normal mode, then
% the simulation makes \( I \) clean for \( M^{*} \).
% It may encounter a colony that is only healthy, not super-healthy;
% however, then the simulation will turn it into a healthy one.
% If healing is invoked but no rebuilding then we can follow the proof of Lemma~\ref{lem:combined-heals}:
% the result of each successful healing is consistent with the the continuation of a simulation.

% If a rebuilding is invoked whose whole range falls within \( I \) then it succeeds.
% Then simulation continues, with possible healings thrown in, until new rebulding starts.
% Lemma~\ref{lem:rebuild-health} shows that rebuilding will only extend the super-admissible area.  
% \end{proof}
% \Pnote{Coordinate these two lemmas, check and elaborate.}

\section{Isolated bursts}\label{sec:1-level-noise}

Here, we will prove that the healing procedure indeed deals with isolated bursts.
Our goal is to show that it provides relief, as required in an
admissible annotated trajectory.

Bursts can create disorder.
For its elimination we will rely on the Escape, Spill Bound and the Attack Cleaning
properties of a trajectory, see Definition~\ref{def:traj}.

Isolated bursts don't create disorder larger than \( 3\beta \).
The head can escape a disorder interval \( I \) via the Escape property; while it is inside the
spreading of this interval is limited by the Spill Bound property.
Every subsequent time when the head enters and exits \( I \) this gets decreased
via the Attack Cleaning property, so it disappears after \( O(\beta) \) such interactions---see
Lemma~\ref{lem:healing} below.

% \begin{lemma}[Local escape]
% Let \( G \) be an interval of size \( n\B \) where \( n<\Z \).
% Then in the absence of noise, the head will either escape \( G \) within time \( O(n\Tu) \),
% or it will be inside a clean hole at some point during this time, as per Definition~\ref{def:clean-hole}.
% \end{lemma}
% \begin{proof}
%   Let \( c = (\CAtt+2\CSpill) \), as used in the Dwell Cleaning property of Definition~\ref{def:traj}.
% Let us cover \( G \) by consecutive intervals of size \( c\B \) called \df{blocks}, let \( m \) be the
% number of these blocks.
% Assume that the head does not escape \( G \) within time \( m\cdot\CEsc\Tu \).
% Then there is a block \( K \) in which it spends cumulative time \( \CEsc\Tu \),
% and the Dwell Cleaning property of trajectories implies that at some point during this time, it
% will be inside a clean hole in \( K \).
% \end{proof}
 
In a clean configuration, whenever healing started with an alarm, the procedure
will be brought to its conclusion as long as no new burst occurred.
The trajectory properties, however, do not allow any conclusion about the
state of the cell to which the head emerges from disorder.
This complicates the reasoning, and may require several restarts of the healing procedure.
By design, the healing procedure can change the \( \Core \) track only in one cell,
even if the head emerged from disorder, with wrong information.
The following lemma limits even this kind of damage:
it says that the head with the wrong information may increase some existing island, but will
not create any new one.

\begin{lemma}
In the absence of noise, no new island will arise.
\end{lemma}
\begin{proof}
The islands are defined only by the \( \Core \) track.
In normal mode, this track changes only at the front.
If this is not the real front, then we are already in or next to an island.

The healing procedure's change of \( \Core \) is part of a stitching operation.
Looking at the different cases of the proof of Lemma~\ref{lem:stitching}, we see that 
inside a healthy area, healing can only change the \( \Core \) track in two ways.

The first way is case~\ref{step:stitching.mergeable}, when the \( \Sweep \) values were changed
in a segment not belonging to the interior.
Such an operation can be applied to any healthy configuration without affecting health.

The second case is when the front is moved left or right.
This does not affect health either.
\end{proof}

The following lemma is central to the analysis of the behavior of the machine under the condition that 
bursts are isolated.

\begin{lemma}[Healing]\label{lem:healing}
In the absence of noise in \( M^{*} \), the history can be super-annotated.
Also, the decoded history \( (\eta^{*},\Noise^{*}) \) satisfies the Transition Function property 
of trajectories (Definition~\ref{def:traj}).
\end{lemma}
\begin{proof}
The proof of super-annotation is by induction on time, extending the super-annotation into the future.
The extension is straightforward as long as no distress event is encountered.
The Transition Function property is observed, as no obstacle arises to the simulation.
Stains do not cause problems: the computation stage of the simulation cycle eliminates them
using the error-correcting code.

Consider now the occurrence of a distress event.
This can be due to either a burst or the encounter with an island.
In the latter case, before the relief a burst can still hit.
In the analysis below, then we will just cut our losses and restart, knowing that 
burst cannot hit again before relief.

At the time of the distress event, let us draw an interval \( I \) of size \( \Q\B \) centered 
around the head.
The head will not leave it before relief, so we will consider the changes of 
the configuration inside it.
Let \( S_{1},\dots,S_{m} \) be the list of substantial segments of \( I \), 
and \( A_{1},\dots,A_{m-1} \) the ambiguous segments between them.
Note that \( m=O(1) \).
Let \( R \) be the set of those \( i \) for which \( S_{i}\cup A_{i}\cup S_{i+1} \) is clean.
For \( i\in R \) let \( n_{i} \) be the number of 
stitching steps by the algorithm of Lemma~\ref{lem:stitching} needed to stitch them 
together.
Let \( N=\sum_{i\in R} n_{_{i}} \).
Whenever the head enters disorder, we will mark the edge where it entered as \df{attacked}.

We introduce a few variables for the proof.
\begin{itemize}
\item\( D= \) be the total size of disorder, divided by \( \B \).
\item\( E= \) the number of un-attacked edges of disorder where the head can exit without
  entering (because it is on the side of the disorder).
  This number never increases.
\item \( F= \) the number of un-attacked edges where the head cannot exit without entering (because
  it is away from the disorder).
\item \( P= \) be the number of steps that the front moves forward (including the ones after possibly
changing the meaning of forward at a regular turn).
\end{itemize}

The following kinds of event may occur:
\begin{varenum}{h}

\item\label{i:heal.N-decr} With a stitching done, \( N \) decreases by 1 while possibly 
moving the front backward.

\item\label{i:heal.enter-carpet} The head enters disorder, decreasing \( F \).

\item\label{i:heal.leave-carpet.E}
The head leaves disorder on a non-attacked edge, decreasing \( E \).

\item\label{i:heal.leave-carpet.F}
The head leaves disorder on an attacked edge, decreases the disorder by at least \( \B \) and
increases \( F \) by 1.

% \item\label{i:heal.hole}
% The head creates a clean hole, by the Dwell Cleaning property of Definition~\ref{def:traj}.
% This may increase \( E \) by 2, but also decreases the disorder by \( \Omega(\B) \).

\item\label{i:heal.clean}
A set \( A_{i} \) becomes clean: then \( i \) gets added to \( R \); 
this can happen only \( m \) times.

\item\label{i:heal.N-incr.healing}
\( N \) increases in the healing mode by \( 1 \).
This can only happen after the head entered the clean area in healing mode (with the wrong information).
At the exit, either \( E \) or \( D \) had to decrease.

\item\label{i:heal.approach-front} 
The head approaches the front: if it does not reach there then it gets closer by  
 \( \Omega(\beta)\B \).

\item\label{i:heal.front-move} The front moves forward in normal mode
(possibly after making a regular turn, and thus changing the forward direction).
This may also increase \( N \) by 1 (and is the only way to do so), 
say by decreasing a segment \( S_{i} \).
But it is followed by zigging.
The zigging may hit disorder, leading to case~\eqref{i:heal.enter-carpet},
or find something wrong---and trigger new healing, which leads to some of the other events.
Otherwise the island around the front will be deleted, and the head becomes distress-free.
\end{varenum}

The above possibilities suggest a potential function
\begin{align*}
   N + c_{D}D + c_{E}E + c_{F}F + c_{m}m - c_{P}P
 \end{align*}
where \( c_{D},\dots \) are appropriate positive constants.
Let us look at what each possibility does to the potential.

Cases~(\ref{i:heal.N-decr}-\ref{i:heal.leave-carpet.E}) decrease the potential if \( c_{P}<1 \).
Case~(\ref{i:heal.leave-carpet.F}) decreases the potential if \( c_{D}>c_{F} \).
There are only \( O(1) \) cases of type~(\ref{i:heal.clean}), increasing \( N \) by a total of \( O(\beta) \).

In case~(\ref{i:heal.N-incr.healing}), if \( c_{D} \) and \( c_{E} \) are large enough,
the increase in \( N \) can be charged to a decrease in \( D \) or \( E \).

Case~(\ref{i:heal.approach-front}) will not change the potential but there are at most \( O(1) \)
consecutive such cases.
 
Consider case~(\ref{i:heal.front-move}).
If the zigging hits new disorder and \( c_{F}>1 \) then the potential decreases.
If it triggers new healing then this will lead to one of the other cases,
compensating for the increase of \( N \) if the constants (other than \( c_{P} \)) are large.

These considerations show that relief indeed follows distress in time \( O(\beta^{2}\Tu) \).
At that point a normal-mode step moving the front has been made, followed by zigging.
Therefore the island causing the distress must have been eliminated, allowing the 
simulation to continue.
The stain caused by the island remains, but as discussed after Definition~\ref{def:super-admissible},
the simulation guarantees that the
size and number of stains remains bounded as required by that definition.
The simulation also continues to deliver the Transition Function property of trajectories.
\end{proof}

\section{Cleaning}\label{sec:cleaning}

This section will scaled the Spill Bound, Escape, Attack Cleaning
and Pass Cleaning properties of trajectories.

Let
\begin{align}\label{eq:pi-star}
 \passno^{*}= \passno +\theta+3.
\end{align}


\subsection{Cleaning the current level}

The following lemma will be used in the the scale-up of the Dwell Cleaning and Pass Cleaning properties.

Let
\begin{align}\label{eq:r-def}
  \r = 4,
  \lambda = 3<\passno.
\end{align}

\begin{lemma}\label{lem:pass-clean-1}
  Let \( P \) be a space-time path of the head  with at most \( \lambda\passno^{2} \) bursts
  occurring during the passes and intrusions of \( P \) into \( I \).
  Let \( J_{1}\subset I \) be an interval of size \( \theta\B/2 \) that \( P \)
  passes \( \passno+\theta \) times,
  at a distance at least \( \r\theta\B \) from the bursts of \( P \).
  Then there is a time during \( P \) when \( \Int(J_{1},\CPass\B) \) becomes clean.
\end{lemma}
The proof of this lemma does not rely on any details of the simulation program.
\begin{proof}
  Assuming the statement is not true we will arrive at a contradiction.
  Assume that \( J_{1} \) is the leftmost interval with the required properties that doees not become clean
  at any time during \( P \).
  
If during the first \( \passno \) passes
all the intrusions of \( P \) into \( J_{1} \) are covered by fewer than \( \passno^{2} \) rectangles,
then the Pass Cleaning property for \( M \) implies that the interior of
\( J_{1} \) becomes clean by their end time (denote it by \( t_{2}<t_{1} \).
Suppose now that this is not the case, so for example the left intrusions need more than \( \passno^{2}/2 \)
rectangles to cover.
Between each two passes, all but one of the rectangles are maximal, so at least
\begin{align*}
 K_{2}=\passno^{2}/2 -\passno
\end{align*}
of them are.
We will call these \df{maximal intrusion rectangles}.

It some interval of size \( 2\B \) within distance \( \theta\B/2 \)
of intersecting \( J_{1} \) becomes clean by time \( t_{1} \) then the Attack property 
will extend this interval to the right in the remaining \( \theta \) passes after \( t_{1} \)
and clean \( J_{1} \) as well.
\Pnote{Elaborate!}
But we assumed this does not happen.
If one of the rectangles intersecting \( J_{1} \) is max height,
then the Dwell Cleaning property implies an interval of size \( 2\B \) in it that becomes
clean before time \( t_{2} \),
which again leads to contradiction by the Attack property and the remaining \( \theta \) passes.
So all the \( K_{2} \) rectangles have maximum width.
Let \( J_{2} \) be the interval of length \( \theta\B/2 \) adjacent to \( J_{1} \) on the left.
Since the intrusion rectangles
don't reach the right end of \( I \), they extend to a distance \( \theta\B/2 \) to the left
of \( I \), so each of them passes \( J_{2} \).

Let us iterate this reasoning for \( i=2,\dots,r \).
Assume we have \( K_{i} \) noise-free passes over \( J_{i} \) before time \( t_{i} \).
We can divide the passes before the the time \( t_{i+1}<t_{i} \) at the beginning of
the last \( \theta \) of them
into groups of \( \passno \) consecutive passes, so we have now at least
\begin{align*}
   K_{i}-\theta-\passno
\end{align*}
passes over \( J_{i} \) before \( t_{i+1} \) divided into consecutive groups of size \( \passno \).
Each group fails to clean \( J_{i} \), so there are at least \( \passno^{2} \) intrusions per group,
altogether at least \( \passno(K_{i}-\theta-\passno) \) intrusions.
Without loss of generality assume that at least half of these are from the left.
Let \( J_{i+1} \) be the interval of size \( |J_{i}| \) adjacent to \( J_{i} \) on the left.
If \( J_{i+1} \) becomes clean before \( t_{i+1} \) then by the Attack property \( J_{i} \) would become
clean by time \( t_{i} \) contrary to the assumption.

Just as in case of \( i=1 \), between each pair of passes, all but one of the intrusions has a maximum-width
covering rectangle.
This gives
\begin{align}\label{eq:K-next}
   K_{i+1}=\passno(K_{i}-\theta-\passno)/2 = K_{i}(\passno /2)-(\theta+\passno)\passno/2
\end{align}
passes over \( J_{i+1} \).
Hence for \( i\ge 3 \)
\begin{align*}
  K_{i} &= (\passno/2)^{i-1}\passno -(\theta+\passno)((\passno/2)+(\passno/2)^{2}+\dots+(\passno/2)^{i-2})
\ge (\passno/2)^{i}
\end{align*}
if \( \passno \) is large.

The situation changes once \( J_{i} \) is such that bursts can happen in it, which can happen if \( i\ge r \),
so we will handle the case \( i=\r \) differently.
By the assumption the total number of bursts is at most \( s=\lambda\passno^{2} \).
Each burst may affect at most one pass, so there remain at least \( K_{i}-s \) burst-free ones of them,
separated into at most \( s+1 \) burst-free groups.
Given the size of \( \passno \), at least one of these groups has size at least
\begin{align*}
  K_{\r+1}&=\frac{K_{\r}-s}{s+1} > \frac{K_{\r}}{2\lambda\passno^{2}} \ge \pi^{r-2}2^{-r-1}/\lambda.
\end{align*}
Let \( t_{r+1} \) be the endtime of the last pass in this group.
With \( \r = 4 \) in~\eqref{eq:r-def} this gives \( K_{5}\ge \passno^{2}/2^{5}\lambda > \passno \).
From here on we can again use~\eqref{eq:K-next}, since no burst appears at all during
the time of this group of passes.
But since \( P \) is finite, sooner or later this process must terminate, leading to a contradiction.
\end{proof}


\subsection{Escape}\label{sec:escape}

This section scales up the Escape property of trajectories.

% The following lemma is similar to Lemma~\ref{lem:combined-heals}, but deals with longer intervals.

% \begin{lemma}\label{lem:rebuild-pass}
% Let \( I=\lint{a}{b} \) be a clean interval of size \( > 13 \Q\B \).
% If the head passes it without a burst then a subinterval of size \( |I|-10\Q\B \)
% becomes clean for \( M^{*} \).
% \end{lemma}
% \begin{proof}
% \end{proof}

\begin{definition}
  A maximal clean interval will be called a \df{clean hole}.
  Let \( \cK(t) \) denote set of all clean holes of size \( \ge\CAtt\B \) at time \( t \),
  let \( K(t)=\bigcup \cK(t) \).
  Let \( \cK^{*}(t) \) be the set of those super-healthy clean holes, and \( K^{*}(t) \) be their union.
\end{definition}

Of course, \( K^{*}(t)\subseteq K(t) \).

Consider a burst-free space-time set \( I\times J \): since 
no disorder can appear during it in the interior of a clean interval,
if \( I_{1} \) is a clean interval at time \( t \) and \( I_{2}, I_{3} \) are clean intervals at time \( t+1 \)
intersecting \( I \) then the smallest interval containing \( I_{2}\cup I_{3} \) is also clean at time \( t+1 \).
So the elements of the set \( \cK(t) \) of clean holes of size \( \ge\CAtt\B \)
will not break up: they can grow, shrink but only slightly, bounded by the Spill condition
(and thus will not disappear) or merge; new elements can appear as well.
We can similarly follow a single clean hole \( I(t) \).

Just as a clean interval cannot break up in the absence of bursts, a super-healthy
interval cannot break up either.
So we can follow the evolution of super-healthy clean holes in \( \cK^{*}(t) \) similarly.
Recall that a super-healthy interval in history \( \eta \) is clean in the decoded history \( \eta^{*} \),
so the simulation of \( \eta^{*} \) will proceed in it.

In what follows we will estimate how much the area of the sets \( K(t) \) and \( K^{*}(t) \)
will grow with the cumulative time spent \( K(t) \).

The following lemma considers the activity of the head while it is inside a clean hole \( I(t) \).
the case that the interval in question is clean.

\begin{lemma}\label{lem:inside-hole}
  Let \( I(t) \) be a clean hole having size \( \le n\B \) with \( n\le\lambda\Q \), \( \lambda\le 3\beta \) at the
  beginning of a burst-free time interval,
  and \( k_{I}(t)=|K^{*}(t)\cap I(t)| \).
  If \( n<\Q/2 \) then the head will leave \( I(t) \) within time \( \CEsc\Z n\Tu \).
  Otherwise, every time interval of its stay that is longer than \( \CEsc\lambda\Tus \) grows
  \( k_{I}(t) \) by at least \( \Q\B \).
\end{lemma}
\begin{proof}
  We will see that every certain amount of time some kind of progress is made.
  \begin{enumerate}
  \item\label{i:inside-hole.rebuild}
Suppose that we are at some time when the rebuild procedure had started,
from a base of rebuild-marked cells large enough not to result in alarm (renewed call for healing)
after the first zigging.
Then the rebuilding will be completed.

\item\label{i:inside-hole.normal}
  Suppose that we are at some time when the mode is normal.
  Then within \( \Z \) steps either a healing call happens, or a step of progress will be
  made in the ordinary work of simulation.
\item\label{i:inside-hole.alarm}
  Suppose that at some time healing will be called.
  Then according to the proof of Lemma~\ref{lem:healing},
  we either arrive at case~\ref{i:inside-hole.rebuild},
  or  in \( O(\beta^{2}<\Z) \) steps or healing finishes in
  normal mode with at least one step made in the ordinary work of simulation.
  
  Indeed if healing succeeds it leaves a healthy area.
  Lemmas~\ref{lem:health-extension} and~\ref{lem:combined-heals} imply that 
  the overlapping successful healing areas can be combined, so healing will not be repeated over the same
  area, and thus can slow down progress only by a 
  factor \( O(\beta^{2}) \) (negligible compared to the \( \Z \) times slowdown by zigging), and even this
  in at most one sweep of simulation over any area.

\item\label{i:long-stay}  Suppose that \( n>\Q\B \) and the head stays longer than \( \CEsc\lambda\Tus \).
  will grow by at least \( \Q\B \).
  In this much time, if any rebuilding starts then it will finish and extend \( K^{*}(t) \) by a colony.
  Without rebuilding, the continued healings add some colonies to the health area; any such colony will be
  turned super-healthy by the first complete work-period of the simulation.
  If the head is already in a super-healthy area then it will perform the simulation of \( M^{*} \).
  The simulated machine \( M^{*} \) has the same program, so it will also make switchbacks
  of at least \( \E>3\beta \) steps (for healing) or even the bigger ones for zigging.
  This extends the super-healthy area unless interrupted by rebuilding, which as seen extends it also.
\end{enumerate}
\end{proof}

\begin{definition}\label{def:super-healthy-end}
  Let \( I \) be a clean interval, with the head on its right.
  We will say that its right end is \df{directed}, if
  the right end, except for a final interval of size \( 2\E \), is covered by a pair of consecutive intervals \( K\cup L \)
  where \( K \) is a super-healthy interval containing at least two complete colonies, \( L \) is
  covered by cells marked for rebuilding and has size \( < 4\Q\B \),
  and the rightmost colony of \( K \) is in the process of transfer to the right.
  Directedness of the left end is defined similarly.
\end{definition}

\begin{lemma}\label{lem:directed-end}
  Let \( I(t) \) be a clean hole having size \( \le n\B \) with \( n\le\lambda\Q \), \( \lambda\le 3\beta \),
during a burst-free time period.
\begin{alphenum}
  \item\label{i:make-directed}
If the head stays in \( I(t) \) longer than 
\( \CEsc\Z n\Tu \) and \( |I(t)| \) did not increase by more than \( \Q\B \),
then the end on which it leaves becomes directed.
\item\label{i:enter-directed}
After consecutive passings of the head into and out of a directed end, this end stays directed.
 If after entering, the head spends a cumulative time of \( 2\Tus \) in \( I(t) \),
the value \( k_{I}(t)+|I(t)| \) increases by at least \( \Q\B \).
\end{alphenum}
\end{lemma}
\begin{proof}
  To~\eqref{i:make-directed}:
  In a clean interval, under burst-free conditions, the head is preparing a healthy area either by
  successive healings or by rebuilding.
  The time spent in \( I(t) \) without increasing it significantly indicates that
  a simulation must have started in a  healthy pair of colonies.
  It can only get interrupted during transfer, either when the head leaves \( I(t) \) or when
  healing/rebuilding starts.
  The rebuilding can also only be interrupted when the head leaves \( I(t) \).

  To~\eqref{i:enter-directed}:
  Assume that the head enters from the right.
  If it starts healing or rebuilding then it does not alter the directedness, since the rebuilding does not encompass
  both healthy colonies of the pair doing the transfer.
  The head
  cannot get past the left healthy colony without either completing its right transfer operation
  or completing a rebuilding action.
  If the transfer or rebuilding finishes then a colony is added to \( K^{*}(t) \), thus increasing it by \( \Q\B \).
  The cumulative time for this is at most \( \Tus \).
  If rebuilding keeps restarting and never finishes during the time \( 2\Tus \) then during the time \( \Tus \)
  spent on these attempts, given that rebuilding takes only \( O(\Z\Q\Tu) \) time,
  the head must have passed the right end so many times that the Attack property increases \( |I(t)| \) by \( \Q\B \).  
\end{proof}

The next lemma considers a situation similar to Lemma~\ref{lem:inside-hole},
but allowing multiple entries and exits.

\begin{lemma}[Expand hole]\label{lem:expand-hole}
  Let \( I(t) \) be a clean hole having size \( \lambda\Q\B \) with \(  1/2\le\lambda\le 3\beta \) at the
  beginning of a burst-free time interval.
  There is a constant \( c \) with the following property.
  If the head spends cumulative time \( c\lambda\Tus \) in \( I(t) \) then at the end of \( J \) the hole grows
  in size by at least \( \Q\B/2-\CSpill\B > \Q\B/3 \).
\end{lemma}
\begin{proof}
If the head spends time \( \le c\Z\Q\Tu \) in \( I(t) \), the constant \( c \)
from Lemma~\ref{lem:directed-end}, then we say that its stay was \df{short}.
If it spends time \( \ge\CEsc\lambda\Tus \) in \( I(t) \) (from Lemma~\ref{lem:inside-hole}),
then we will say that the stay was \df{long}.
Otherwise we will say that the stay was \df{medium}.
\begin{enumerate}

\item\label{i:fast-return} After a short stay, \( I(t) \) expands by at least \( \B/2 \) via the Attack property.

\item\label{i:long-time return}
  After a long stay (even without leaving),, by Lemma~\ref{lem:inside-hole}, the
  value of \( k_{I}(t)= |K^{*}(t)\cap I(t)| \) increases by at least \( \Q\B \).

\item\label{i:medium stay}
  Lemma~\ref{lem:directed-end} implies that after a medium stay and further cumulative time of \( 2\Tus \)
  inside \( I(t) \), the value \( s(t)+|I(t)| \) increases by at least \( \Q\B \).
  \end{enumerate}
    Adding up these possibilities will complete the proof.
\end{proof}

The following lemma is the scale-up of the Escape condition.

\begin{lemma}[Escape]\label{lem:escape}
  In the absence of \( \Noise^{*} \), the
  head will leave any interval \( G \) of size \( n\B \) with \(n\le\lambda\Q \),
  \( 1\le\lambda\le 3\beta \), within time \( \CEsc\lambda\Tus \).
\end{lemma}
\begin{proof}
  If \( J \) is the time interval considered, then we can ignore the at most one
  burst happening during \( J \) by considering the half of \( J \) where it does not occur.

  At any time \( t \), we will partition the interval \( G \) into subintervals as follows.
  First, let \( \cK(t) \) be the set of clean holes of size \( \ge\CAtt\B \) inside \( G \),
  and \( K(t) \) their union.
  Each interval between two such clean holes that is larger
  than \( \theta\B/2 \) will be subdivided into a sequence of subintervals; the first and last one
  have size \( \le\theta\B/2 \), the other ones have size \( \theta\B/2 \).
  Let us call all members of this partition of \( G \) \df{blocks};
  the clean holes among them will be called \df{clean}, the other ones are called \df{short}.

  As time passes new clean blocks may arise, the existing ones may
  increase, and neighboring ones may merge.
  (They may also decrease due to spill of disorder but this decrease is temporary until the next increase
  due to attack.)

  We will consider a sequence of times \( t_{1}<t_{2}<\dots \) defined as follows.
  \( t_{1} \) is our starting time.
  If \( t_{i} \) is defined and at time \( t_{i} \) the head is in some block \( S \)
  then \( t_{i+1} \) is the first time after \( t>t_{i} \) at which 
  \begin{itemize}
  \item if \( S \) is clean then the head is either not in \( S \) or \( t = t_{i}+\Tu \);
  \item if \( S \) is short then the head is not in either \( S \) or in any short block next to \( S \).
  \end{itemize}
  The Escape property of trajectories implies \( t_{i+1}-t_{i}\le e_{1}\Tu \), where
  \begin{align*}
   e_{1}=\CEsc\cdot 3\theta/2,
 \end{align*}
  since the length of a union of three consecutive short blocks is \( (3\theta/2)\B \).
  By definition, between times \( t_{i} \) and \( t_{i+1} \) one of the following events occurs.
  We will show the contribution to the growth of \( |K(t)| \) made by them.
  Clearly \( K(t) \) cannot grow larger than \( \lambda\Q\B=|G| \).
  
  \begin{varenum}{e}

  \item\label{i:cross} A boundary is crossed between clean and short blocks.
    A pair of passes over the same boundary results in an increase of \( K(t) \) by \( \B \).
    The number of un-paired passses over boundaries is at most the upper bound of the number of
    such boundaries, that is \( e_{2}\Q \) where
    \begin{align*}
      e_{2} = \lambda/\CAtt.
 \end{align*}
    So if \( t \) is the time spent on events of this type then there are at least \( t/e_{1}\Tu \)
    such events, and then the growth of \( K(t) \) is at least
    \begin{align*}
      \frac{(t/e_{1}\Tu - e_{2}\Q}{2}\cdot\frac{\B}{2}.
    \end{align*}
    Assuming \( t\ge 2 e_{1}e_{2}\Q\Tu \) this is at least \( t\B/8\Tu \).
    This becomes larger than \( |G|=\lambda\Q\B \) if \( t>8\lambda\Q\Tu \)
    so the time spent on these events is less than \( 8\lambda\Q\Tu\le\Tus \).

  \item\label{i:merge} Two clean blocks merge.
    The number of these events is limited by \( e_{2}\Q \), for a total time limited by \( e_{1}e_{2}\Q\Tu \le\Tus\).

  \item\label{i:cum-incr} The cumulative time spent in \( K(t) \) increases by \( \Tu \).
    Let \( \cns{expand} \) be the constant \( c \) of Lemma~\ref{lem:expand-hole}.
    By that lemma, if the head spends time \( \cns{expand}\lambda\Tu^{*} \)
    on these events then \( K(t) \) increases by at least \( \Q\B/3 \).
    So it cannot spend more than \( 3\cns{expand}\lambda^{2}\Tus \) without escaping \( G \).    
    
  \item\label{i:passes} The number of passes over a short block \( S  \) increases by 1.
    If \( \passno+\theta \) such events occur over a short block \( S \)
    then Lemma~\ref{lem:pass-clean-1} becomes
    applicable (with no bursts!) and \( \Int(S,\CPass\B) \), of size \( \ge \theta\B/6 \), becomes clean,
    increasing \( |K(t)| \) by \( \theta\B/6 \).
    If time \( t \) is spent on such event, then the increase is at least
    \begin{align*}
   \frac{t\theta\B}{6 e_{1}\Tu(\passno+\theta)}.
    \end{align*}
    This becomes larger than \( \lambda\Q\B \) if \( t>6e_{1}\Tu(\passno+\theta)\Q \).
    But \( \Tus/\Tu>6(\passno+\theta)\Q \), so the total time spent on these events is at most \( \Tus \).  
  \end{varenum}

  
\end{proof}





\subsection{Pass cleaning}

The following lemma states the Pass Cleaning property for machine \( M^{*} \).  

\begin{lemma}
Suppose that a path \( P \) has no bursts of \( \eta^{*} \), it passes \( \pi^{*} \) times
the interval \( I \),
and there is a \( (\theta \Q\B, \theta\U\Tu) \)-covering of size \( \le (\pi^{*})^{3} \) for
for \( I \) and \( P \).
Then by end of the \( \pi ^{*} \)th pass the interior
\( \Int(I, 5\Q\B) \) becomes clean for \( \eta^{*} \).
\end{lemma}

Because \( \theta\le\gamma \) and the path \( P \) contains no burst of \( \eta^{*} \),
each rectangle of the \( (\theta \Q\B, \theta\U\Tu \)-covering contains at most one burst;
so the number of bursts during all the intrusions is at most \( (\passno^{*})^{2} \).




The first part of the proof shows that after \( \passno+\theta \) passes, the interior of \( I \) becomes
clean, except for \( O(\passno^{2}) \) islands.
The next part shows that the additional passes build up the next level (three should suffice).

\subsubsection{Building up the next level}

We will show that if an interval \( I \) of size \( \theta \Q\B \) is clean except for
possibly 
\begin{align}\label{eq:islands-bd}
  \kappa=(2\lambda+1)(\passno+\theta+3)=O(\passno^{2}) 
\end{align}
islands then in a constant number of passes it will be populated with healthy colonies, provided
no more than \( \kappa \) bursts happen in it during and between these passes.
Note that for restoring the next-level structure it is allowed to destroy some existing colonies.

% The key to eliminating the rest of \( M \)-level disorder after \( \passno \)
% passes is the following notion.
% In our program, whenever the head moves in the
% direction of its progress (be it in the simulation, or in
% rebuilding), say to the right, then after one step it makes a zigging dip left.
% In the process, it moves back up to \( \Z \) steps from the front of its progress.
% (Healing can have a much shorter range, does not seem to need zigging.)
% The interval of cells between the head and the front can become
% dangerous if the area was actually created by large-scale noise.
% If the head enters it from the bottom then it may just continue the zig, and lose connection with
% the area on the left.
% Therefore we call interval a \df{trap}.

\begin{definition}
  An interval \( J \) not containing the head is called \( \df{directed} \) if
  its cells, possibly with the exception of an island, point towards some possible front (in normal operation
  or rebuilding).
  This front can be inside or outside \( J \).
  If it is to the left of \( J \) then the head must also be on the left of \( J \), similarly for right.
  If it is inside then the head must be inside, with one exception: if, say, the front is within \( Z \) steps
  to the left of \( J \) and moving right then the head may be on the left of \( J \) (say, captured there while
  zigging).
  \end{definition}

  For the lemma below, recall that \( \E \) is the maximum number of cells in a healing area.
It relies on the fact that \( \E \) is much smaller than the size \( \Z \) of zigging.

  \begin{lemma}\label{lem:keep-directed}
  Suppose that \( J \) is directed, the head enters \( J \) and leaves it.
  Let \( J' \) be \( J \) if the intrusion was noiseless, and \( \Int(J,\E\B) \) if it contains a burst.
  Then \( J' \) will be also directed after the intrusion.
\end{lemma}
\begin{proof}
  In normal operation or rebuilding,
  the head moves the front with itself and returns to it from every zig, except when it is
  captured at an end of \( J \) (by a burst or disorder).
  If an island is encountered or a burst occurs then it will be healed, except when the healing interval
  (whose maximum size is \( \E\B \)) intersects
  the boundary of \( J \) (where the disorder may capture the head).
  The decrease of \( J \) to \( J' \) reflects this.
\end{proof}

\begin{lemma}\label{lem:zig-neighbor-int}
  Consider four passes, starting with a right pass,
  over a clean interval \( J \) that gets no burst during this time.
  Let \( J' \) be an interval adjacent to \( J \) on the right that is smaller than \( \Z\B-|J| \),
  and the total amount of disorder that may be created in it during all the passes and intrusions considered is
  less than \( |J|/2 \).
  Then after one of the two left passes, \( J\cup J' \) becomes left-directed. 
\end{lemma}
\begin{proof}
  Suppose after a right pass all cells of \( J \) are directed to the right.
  Then on the next left pass,  disorder on the right within distance \( \Z\B-|J| \)
  will be hit by every right zig, which eliminates a \( \B/2 \)-size piece of it by the Attack property.
  Because of our condition that the amount of disorder in \( J' \) is at most \( |J|/2 \), there will be enough
  right zigs to eliminate all disorder.

  It can only happen that not all cells of \( J \) are directed right after the first right pass
  if at the end of the pass
  the front makes a left turn before reaching the right end of \( J \), and the head reaches it only
during a right zig.
  But then the feathering property implies that 
  during the following rightward pass it cannot make a left turn within \( \Z\B \) of the right end of \( J \),
  therefore \( J \) will become directed right.
\end{proof}

\begin{lemma}\label{lem:left-dir-self}
  Consider an interval \( K \) of size \( \le 3\kappa\E\B \), in \( \Int(I, 2\kappa^{2}\B) \).
  Within four passes (starting from the left) it becomes left-directed.
\end{lemma}
\begin{proof}
(The proof assumes \( \Z>3\kappa^{2} \).)
Since there are at most \( \kappa \) islands (of size \( \le\beta\B \)),
there is a clean interval \( J \) in \( I \) of size \( \kappa\B \)
on the left of \( K \) at a distance at most
\( \kappa(\kappa+\beta)\B<(2\kappa^{2}-\kappa\E)\B \).
  Let \( J' \) be the smallest interval adjacent to \( J \) containing \( K \).
  Lemma~\ref{lem:zig-neighbor-int} implies that \( J' \) becomes left-directed
  during one of the two left passes.
\end{proof}

\begin{lemma}
  After 4 passes,  the interval \( \Int(I,3\kappa^{2}\B) \) will be directed.
\end{lemma}
\begin{proof}
  Consider a subinterval \( J \) of \( \Int(I,2\kappa^{2}\B) \), of size \( \kappa\E\B \).
  Let \( J' \) be the interval of size \( 3\kappa\E\B \) for which \( J=\Int(J',\kappa\E\B) \).
  By Lemma~\ref{lem:left-dir-self}, \( J' \) becomes left-directed within four passes.
  After this, Lemma~\ref{lem:keep-directed} makes sure that besides an island created by the last
  burst, \( J' \) can get islands only at its edges.
  These can decrease \( J' \) by at most \( \E\B \) at a time.
  There are at most \( \kappa \) such times, so they never reach \( J \).
  The fourth pass cleans away all islands but the one created by a burst within the pass itself.
\end{proof}

Now the next pass is going through the directed interval \( \Int(I,3\kappa^{2}\B) \).
If it does not already find healthy colonies it will apply the rebuilding procedure to produce them; 
nothing prevents this in the directed interval.




\subsection{Attack cleaning}

Eventually, we will prove the Attack Cleaning property of trajectories (Definition~\ref{def:traj}) for the 
decoded history \( (\eta^{*},\Noise^{*}) \).
However, first we prove a weaker version, requiring the space-time rectangle of interest to be free
not only of \( \Noise^{*} \) but also of \( \Noise \)---that is burst-free.

\begin{lemma}\label{lem:clean-attack}
Consider a trajectory \( \eta \) in a noise-free space-time rectangle.
Here, the decoded history \( \eta^{*} \) satisfies the Attack Cleaning property.
\end{lemma}
\begin{proof}
The property says the following for the present case.
For current colony-pair \( x,x' \) (where \( x'< x+2\Q\B \)), suppose that the interval
\( \lint{x-\CAtt\Q\B}{x'+\Q\B} \) is clean for \( M^{*} \).
Suppose further that the transition function, applied to \( \eta^{*}(x,t) \), directs the head right.
Then by the time the head comes back to \( x-\CAtt\Q\B \),
the right end of the interval clean in \( M^{*} \)
containing \( x \) advances to the right by at least \( \Q\B \).

The computation phase of the simulation on the colony pair 
\( x, x' \) is completed without the disturbing effect
of noise: even the zigging does not go beyond the boundary.
Then the transfer phase begins which enters the disorder to the right of \( x'+\Q\B \).

We argue that there are only two ways for the head to get back to \( x-\CAtt\Q\B \).
\begin{varenum}{at}
\item\label{i: clean-attack.normal} The transfer into a new colony pair with starting point
\( y\ge x'+\Q\B \) succeeds despite the disorder, and the clean interval extends over it, before
the head moves left to \( x-\CAtt\Q\B \) in the course of the regular simulation.
Some inconsistencies may be discovered along the way, but they are corrected by healing.

\item\label{i: clean-attack.rebuild} The inconsistencies encountered along the way trigger some
rebuilding processes.
Eventually, a complete, clean rebuilding area is created, the rebuilding succeeds, leaving a clean
colony also to the right of \( x'+\Q\B \).
\end{varenum}

The Spill Bound property guarantees that the area to the left
of \( x'+(\Q-\CSpill)\B \) remains clean,
therefore the only way for the head to move left of that is by the rules.
Suppose that rebuilding is not initiated (with creating a substantial germ):
then moving left can only happen by the normal
course of simulation: the transfer stage of the simulation must be carried out, and this requires 
at least as many attacks to the right as the number of sweeps in the transfer stage.
Every attack (followed by return) extends the clean interval further, until the whole target colony becomes clean,
and the transfer completed.
This is the case~\eqref{i: clean-attack.normal}.

Recall the rebuilding procedure in Section~\ref{sec:rebuilding}:
the rebuilding area extends \( 2.5\Q \) cells to the left and right from its initiating cell.
This may become as large as \( 5\Q\B \) to the left and right.
If rebuilding is initiated, its starting position
is necessarily to the right of \( z=x'+(\Q-\PadLen-2)\B \).
It then may extend to the left to at most \( z-5\Q\B \) (this is overcounting,
since the cells of the colony of \( x \) are all adjacent).
Its many sweeps will result in attacks that clean an area to the right of the starting point.
The procedure may be restarted several times, but those restartings will also be initiated
to the right of \( z \).
Therefore the rebuilding area does not extend to the left of \( z-5\Q\B \):
if the head moves to the left of this, then the rebuilding must have succeeded.
The rebuilding also must find or create a colony manifestly to the right of the restarting site: this will be
to the right of \( z \), moving this way the boundary of the area clean in \( M^{*} \) 
by at least \( \Q\B \).
\end{proof}

Note that in the process described in the above proof,
it is possible that the rebuilding finds a competing colony \( C \)
starting at some \( y \in x'+\Q\B + \lint{-\beta\B}{0}\) which
slightly (by the size of an island) overlaps from the right with the colony of \( x \).
The rebuilding may decide to keep \( C \) and to overwrite the rest of the colony of \( x' \) as a bridge
(or even target).
This does not affect the result.

% \subsection{Extended cleaning}\label{sec:extended-cleaning}

% Let us draw some consequences of the Pass Cleaning property of trajectories (Definition~\ref{def:traj}).
% We will extend this property to longer intervals, allowing also  some bursts.
% For the following lemma, let us use the following notation for convenience:
% \begin{align*}
%    \beta'=\beta+2\CSpill.
%  \end{align*}
% Also, for any interval \( I=\lint{a}{b} \) let \( I'=\lint{a+\CSpill\B}{b-\CSpill\B} \).

% \begin{lemma}\label{lem:dirty-passes}
% Assume that the head passes \( n> 2\passno \) times over and interval \( I \) of size \( |I|\ge 2\passno B \) 
% in a burst-free way during some time interval \( J \).
% There could be some other times in \( J \) when the head is subject to a burst inside \( I \).
% Assume that the total number of these bursts is \( < n/4\beta' \).
% (We don't count the times when the head enters and leaves \( I \) without
% passing over and without any burst.)
% Then there is some time during \( J \) when \( I' \) becomes clean.
% \end{lemma}
% \begin{proof}
% We introduce a \df{virtual} time counting.
% We only count the times when the head passes \( I \).
% Let this virtual time interval be denoted by \( K \): we can assume it starts at 0.
% Let \( K_{1}=\rint{0}{\passno} \) and \( K_{2}=\rint{\passno}{n} \), so \( K=K_{1}\cup K_{2} \).

% For each burst (of size \( \le\beta\B \)), let us extend it left and right by \( \CSpill B \) 
% to a size \( \le  \beta' B \).
% Let \( D_{1} \) be the union of all those intervals coming from bursts occurring before virtual time \( \passno \).
% By the end of time interval \( K_{1} \) the set \( I\setminus D_{1} \) 
% becomes clean, due to the Pass Cleaning property.
% In what follows we are looking at how \( D_{1} \) shrinks during \( K_{2} \)---while some new bursts may
% delay the shrinking.

% Consider intervals \( \lint{a}{b}, U \) such that \( \lint{a}{b}\subset U' \),
% and \( U\setminus \lint{a}{b} \) is clean at virtual time \( u>1 \).
% If no burst occurs at the virtual times \( u,u+1 \) (that is during two burst-free passes) then 
% the Attack Cleaning property implies that 
% by the virtual time \( u+2 \), already the interval \( U'\setminus \lint{a+\B}{v-\B} \) will also be clean.
% So let us mount a triangle \( T\subset I\times K \) over \( \lint{a}{b} \)
% which at time \( u+2j \) covers the interval
%  \begin{align*}
%  \lint{a+j\B}{v-j\B},
%  \end{align*}
% and thus its tip is at virtual time \( v=u+(b-a)/\B \).
% We will call \( v-u \) the \df{height} of \( T \), denoted by \( |T| \).
% If no burst occurs until virtual time \( v \), then \( U' \) becomes clean
% by virtual time \( v \).
% In the special case when \( \lint{a}{b} \) reaches, say, to the right end of the interval \( I \),
% we create a triangle (with the same slopes) 
% twice as large, whose tip is at the right end of \( I \) as well.

% \begin{sloppypar}
% Let us mount a triangle of the above type over each interval of the set \( D_{1} \)
% at virtual time \( \passno \), creating a set \( \cT_{0} \) of disjoint triangles.
% While no burst occurs, the Attack Cleaning property confines disorder to these triangles.
% On the other hand, every burst may create a disordered interval 
% \( \lint{x}{x+\beta\B} \), at some virtual time \( u \).
% Let us mount a triangle then over \( \lint{x-\CSpill B}{x+(\beta+\CSpill)\B}\times\{u\} \),
% and add all these triangles to the set \( \cT_{0} \) to get a set of triangles \( \cT \).
% These are not necessarily disjoint anymore.
%   \end{sloppypar}

% If two virtual triangles \( T_{1},T_{2} \) intersect,
% and \( T \) is the smallest virtual triangle containing both,
% then it is easy to see that \( |T|\le|T_{1}|+|T_{2}| \).
% Denote \( T=T_{1}+T_{2} \).

% Let us now perform the following operation that will create a disjoint set
% of triangles.
% We start with \( \cT \) and if we find two intersecting triangles 
% in it, then we replace them with their sum.
% Repeat until the remaining set \( \cT' \) consists of disjoint triangles.
% By the above remark \( \sum_{T\in \cT}|T| = \sum_{T\in \cT'}|T| \).
% By the Attack Cleaning property, the complement of these triangles
% is clean.
% Let us ignore the triangles on the boundary for a moment.
% The sum of the heights of the triangles is at most \( \beta' \) times the number of bursts,
% and thus at most \( n/4 \).
% Taking the boundary triangles into account can increase this by at most a factor of 2, to \( n/2 \).
% Therefore in the interval \( K \) of length \( n-\passno>n/2 \) there will be virtual
% times not intersecting with any element of \( \cT' \).
% At the corresponding real times, the interval \( I \) is clean.
% \end{proof}


% \begin{lemma}\label{lem:burst-density}
% Consider an interval \( K \) of size \( 3 \Q\B \) and a time interval \( J \) in which
% no burst of \( M^{*} \) occurs.
% If at least \( 4\beta'\passno \) bursts occur in \( K \) during \( J \) then at some time in \( J \)
% the interval \( K \) becomes clean in \( M^{*} \).
% \end{lemma}
% \begin{proof} 
% For \( d=8\beta' \), both positive and negative \( i \), and \( j=0,\dots,d-1 \), let
% \begin{align*}
%    K_{ij}=x+3\Q\B\lint{d i+j}{d i+j+1},\ 
% K_{i}=\bigcup_{j=0}^{d-1}K_{ij}=x+3\Q\B\lint{d i}{d(i+1)},
%  \end{align*}
% so \( K=K_{00} \).
% Consider the sweeps corresponding to the \( 4\beta'\passno \) bursts in \( K \).
% If a sweep does not exit \( K \) on either side then Dwell Cleaning and Attack Cleaning
% of \( M^{*} \) becomes applicable.
% Assume this does not happen; then
% either half of these sweeps exits \( K \) on the left, or half of them exits it on the right.
% Without loss of generality assume that they pass on the right.
% Then they will have to pass the whole interval \( K_{0} \) (otherwise again Dwell Cleaning
% and Attack Cleaning of \( M^{*} \)
% applies), and thus each interval \( K_{0,j} \) gets at least this many passes.
% So, the intervals \( K_{0,j} \), \( j>0 \) gets \( 4\beta'\passno/2 \) burst-free passes (since the 
% bursts occurred in \( K_{00} \)).
% Also, none of the \( K_{0j} \) becomes clean for \( M^{*} \) during the first half of these passes,
% since then the Attack Cleaning property would clean \( K_{00} \) as well during the remaining
% passes.

% Now, for \( i=1,2,\dots \) we will show that there is 
% an \( i'  \) with \( |i'|\le i \) such that each \( K_{i'j} \) gets at least 
% \begin{align*}
%   n_{i}= 4\beta'\times 2^{i-1}\passno
% \end{align*}
%  burst-free overpasses  during \( J \), 
% and \( K_{i'j} \) does not become clean during the first half of these.
% This clearly must break down somewhere, leading to a contradiction.

% We have just proved the case \( i=0 \).
% Suppose that the statement was proved up to some \( i \), we will prove it for \( i+1 \).
% In order for the interval \( K_{i'j} \) not to become clean for \( M^{*} \), 
% by Lemmas~\ref{lem:dirty-passes}, \ref{lem:rebuild-clean}
% the total number of bursts happening in it must be at least \( n_{i}/4\beta' = 2^{i-1}\passno \).
% This is true of all \( j \), so the total number of bursts in \( K_{i'} \)
% is at least \( d 2^{i-1}\passno \).
% If a sweep does not exit \( K_{i'} \) on either side then Dwell Cleaning becomes applicable,
% so either half of these sweeps exits \( K_{i'} \) on the left, or half of them exits it on the right.
% If they pass on the right then let \( (i+1)'=i'+1 \), otherwise \( (i+1)'=i'-1 \).
% Then they will have to pass the whole interval \( K_{(i+1)'} \) (again, otherwise Dwell Cleaning
% applies), and thus each interval \( K_{(i+1)',j} \) gets at least \( d 2^{i-2}\passno \) passes.
% Now the inequality~\eqref{eq:cns.traj} implies \( d > 8\beta'=2\cdot 4\beta' \), finishing the proof.
% \end{proof}

% \section{Further analyses}

% \subsection{Trouble with the current plan}

% The current plan seems not to be working. 
% This plan required the trajectory properties
% Transition Function, Attack Cleaning, Spill Bound, Dwell Cleaning, Pass Cleaning.
% But there is a question about the conditions of these properties, for example
% of Pass Cleaning.
% If we require it only during a noise-free time interval, then Lemmas~\ref{lem:dirty-passes} 
% and~\ref{lem:burst-density} are not applicable.
% They may become applicable if we require it in a time interval in which noise is sparse.
% But then it is not clear how to scale it up.
% These same two lemmas are needed to scale it up, but it seems difficult to prove their
% noisy version.




\section{After a large burst}

Our goal is to show that the simulation \( M\to M^{*} \)
defined in Section~\ref{sec:sim-codes} is indeed a simulation.
Section~\ref{sec:1-level-noise} shows this as long as the head operates in an
area that is clean for the simulated machine\( M^{*} \) (can be super-annotated), 
and has no noise for machine \( M^{*} \) (that is its bursts on the level of machine \( M \)
are isolated).
In other words, essentially the Transition Function property of Definition~\ref{def:traj} of
trajectories for the simulated machine \( M^{*} \) has been taken care of.

The new element is the possibility of large areas that cannot be super-annotated: they
may not even be clean, even on the level of machine \( M \).
The Spill Bound, Attack Cleaning, Dwell Cleaning and Pass Cleaning properties still must be proven.

\subsection{Spill bound}

One of the most complex analyses of this work is the proof that 
the simulated machine \( M^{*} \) also obeys the Spill Bound.
Let us outline the problem.

We are looking at the boundary \( z \) of a large area that is clean for the
machine \( M^{*} \): without loss of generality suppose that this is the right boundary.
We will be looking at it in a space-time rectangle \( \lint{a}{b}\times\lint{u}{v} \)
that is noise-free in \( M^{*} \).
The interesting case has \( a<z<b \) with \( z-a,b-z=O(\Q\B) \).
The assumption allows occasional bursts of noise of the trajectory \( \eta \) of \( M \),
but no two of these bursts must occur in a space-time rectangle of size comparable to
the size of a colony work period of \( M \).
The Spill Bound for trajectory \( \eta \) keeps the disorder of \( \eta \) within
\( O(\B) \) on the left of \( z \) while \( \eta \) is noise-free.
If we could assume \( \eta \) noise-free then the heal/rebuild procedures would also keep
the disorder of \( \eta^{*} \) from spilling over by more than \( O(\B^{*})=O(\Q\B) \).
However, nothing is assumed about the length of the time interval \( \lint{u}{v} \).
If the head would spend all the time within \( O(\Q\B) \) of \( z \) then due to 
the Dwell Cleaning property of trajectories,  the area would be cleaned out, again
preventing spilling.
But the head can slide out far to the right of \( b \) fast, since
the disordered area on the right of \( z \) is arbitrarily large.
Then it can come back much later to the left of \( z \), and by
a burst (allowed since much time has passed)
can deposit an island of disorder there.
Repeating this process would produce unlimited spillover, not only 
of the disorder of \( \eta^{*} \) but even that of \( \eta \).

This is where the Pass Cleaning property helps.
If the above process is repeated \( \passno \) times, the \( \passno \) passes
would clean out an interval on the
right of \( z \) whose size is of the order of \( \Q\B \),
while depositing only \( \passno \) islands of disorder to the left of \( z \).
Our construction will have \( \passno\ll \Q \): more precisely, in our hierarchy
of generalized Turing machines we will have \( \passno_{k}=k \), \( \Q_{k}=k^{2} \). %?
And \( \ll \Q \) bursts will still be handled by healing/rebuilding in \( \eta \).

Of course this sketch is very crude, but it should help motivate the reasoning that
follows.



\bibliographystyle{plain}
\bibliography{reli,gacs-publ}

\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
