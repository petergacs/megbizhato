\documentclass[11pt]{memoir}

\pagestyle{plain}
%\pagestyle{simple}
\setlrmargins{*}{*}{1}
\checkandfixthelayout

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{3}
\counterwithout{section}{chapter}
\counterwithin{equation}{section}
%\numberwithin{equation}{section} % in amsmath
%\counterwithout{figure}{chapter}
\counterwithin{figure}{section}

\makeatletter
% To correct a memoir bug:
\renewcommand{\@memmain@floats}{%
  \counterwithin{figure}{section}
  \counterwithin{table}{section}}
\makeatother

\firmlists

% If you do not want the bibliography on a separate page:
\renewcommand{\bibsection}{% 
\section*{\bibname} 
\prebibhook}

\usepackage[backref,hyperindex,colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage[numbered]{bookmark} % Allows to place a bookmark, see the title. Shows section numbers.
% \usepackage[all]{hypcap} % After hyperref, to anchor floats correctly.
% \usepackage{float}
 % After hyperref:
\usepackage[algo2e,algosection,tworuled,noend,noline]{algorithm2e}
%\usepackage[narrow-bold]{gacs}
%\usepackage[baskerville]{gacs}
%\usepackage[sf-headings]{gacs}
\usepackage[charter]{gacs}
% \usepackage{gacs}
% \usepackage[noBBpl]{mathpazo}
\usepackage{gacs-algo} % After hyperref.
% After gacs.sty

%\usepackage[pagecolor={LightCyan1}]{pagecolor}
%\usepackage[pagecolor={DarkSeaGreen1}]{pagecolor}
%\usepackage[pagecolor={Honeydew1}]{pagecolor}
%\usepackage[pagecolor={Azure1}]{pagecolor}
%\usepackage[pagecolor={Cornsilk1}]{pagecolor}
%\usepackage[pagecolor={Ivory1}]{pagecolor}

\hyphenation{com-plex-ity des-tin-at-ion co-lon-ies}

\newcommand{\shownotes}{1}
\ifnum\shownotes=1
\newcommand{\authnote}[3]
{\text{{ \textcolor{#3}{\( \langle\hspace{-0.2em}\langle \)\textsf{\footnotesize #1: #2}\( \rangle\hspace{-0.2em}\rangle \)}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\Pnote}[1]{{\authnote{P}{#1}{cyan}}}
\newcommand{\Inote}[1]{{\authnote{I}{#1}{blue}}}

\theoremstyle{definition} % not italicized
\newtheorem{Premark}{\color{cyan}Peter remark}
\newenvironment{premark}{\begin{Premark}\color{cyan}}{\varqed\end{Premark}}

\newtheorem{Iremark}{\color{blue}Ilir remark}[Premark]
\newenvironment{iremark}{\begin{Iremark}\color{blue}}{\varqed\end{Iremark}}

\renewcommand{\Pnote}[1]{\begin{premark}#1\end{premark}}
\renewcommand{\Inote}[1]{\begin{iremark}#1\end{iremark}}

\renewcommand{\le}{\leq}
\renewcommand{\ge}{\geq}

\renewcommand{\vek}[1]{\mathbf{#1}}

\newcommand{\fld}[1]{\ensuremath{\textit{#1\/}}}
%\newcommand{\rul}[1]{\ensuremath{\texttt{\slshape #1\/}}}
\newcommand{\rul}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\tNormal}{e_{\mathrm{normal}}}
\newcommand{\tZig}{e_{\mathrm{zig}}}
\newcommand{\tHeal}{e_{\mathrm{heal}}}
\newcommand{\tRebuild}{e_{\mathrm{rebuild}}}

% Using def for the possibility of switching between LaTeX and XeTeX:
\def\B{B}  
\def\U{U}

\newcommand{\va}{\vek{a}}
\newcommand{\Bad}{\mathrm{Bad}}
\newcommand{\Vacant}{\mathrm{Vac}}
\newcommand{\blank}{\text{\textvisiblespace}}
\newcommand{\Configs}{\mathrm{Configs}}
\newcommand{\D}{D}
\newcommand{\E}{E}
\renewcommand{\f}{f} % what was it?
\newcommand{\F}{F}
\def\G{G}
\newcommand{\h}{h}
\renewcommand{\H}{H}
\newcommand{\hc}{\hat h}
\newcommand{\vhc}{\vek{\hat h}}
\newcommand{\Int}{\mathrm{Int}}
\newcommand{\Noise}{\mathit{Noise}}
\newcommand{\Output}{\mathit{output}}
\newcommand{\passno}{\pi}
\newcommand{\PenetrationLen}{\mathrm{PenetLen}}
\newcommand{\Plus}{\oplus}
\newcommand{\Minus}{\ominus}
\newcommand{\pos}{\mathrm{pos}}
\newcommand{\curcell}{\textrm{cur-cell}}
\newcommand{\Q}{Q}
\newcommand{\R}{R}
\newcommand{\Tu}{T}
\newcommand{\Tus}{T^{*}}
\newcommand{\V}{V}
\newcommand{\Z}{Z}
\newcommand{\z}{z}

\newcommand{\Addr}{\fld{Addr}}
\newcommand{\cAddr}{\fld{cAddr}}
\newcommand{\Turned}{\fld{Turned}}
\newcommand{\Core}{\fld{Core}}
\newcommand{\Drift}{\fld{Drift}}
\newcommand{\Doomed}{\fld{Doomed}}
%\renewcommand{\G}{\fld{NonAdj}}
\newcommand{\CDwell}{\cns{dwell}}
\newcommand{\Adj}{\fld{Adj}}
\newcommand{\cHold}{\fld{Hold}}
\newcommand{\Hold}{\fld{Hold}}
\newcommand{\Index}{\fld{Index}}
\newcommand{\cInfo}{\fld{cInfo}}
\newcommand{\Info}{\fld{Info}}
\newcommand{\Kind}{\fld{Kind}}
\newcommand{\cKind}{\fld{cKind}}
\newcommand{\cLevel}{\fld{cLevel}}
\newcommand{\Mode}{\fld{Mode}}
\newcommand{\Prog}{\fld{Prog}}
\newcommand{\Heal}{\fld{Heal}}
\newcommand{\rHeal}{\rul{Heal}}
\newcommand{\Plan}{\fld{Plan}}
\newcommand{\Rebuild}{\fld{Rebuild}}
\newcommand{\Stage}{\fld{Stage}}
\newcommand{\State}{\fld{State}}
\newcommand{\Sweep}{\fld{Sweep}}
\newcommand{\Work}{\fld{Work}}
\newcommand{\FDepth}{\fld{FeatherDepth}}
\newcommand{\SwDir}{\fld{SwDir}}
\newcommand{\ZigAddr}{\fld{ZigAddr}}

\newcommand{\Bridge}{\mathrm{Bridge}}
\newcommand{\Committing}{\mathrm{Committing}}
\newcommand{\Coordinated}{\mathrm{Coordinated}}
\newcommand{\decode}{\mathrm{decode}}
\newcommand{\dir}{\mathrm{dir}}
\newcommand{\encode}{\mathrm{encode}}
\newcommand{\front}{\mathrm{front}}
\newcommand{\Histories}{\mathrm{Histories}}
\newcommand{\Trajectories}{\mathrm{Trajectories}}
\newcommand{\Last}{\mathrm{Last}}
\newcommand{\Marking}{\mathrm{Marking}}
\newcommand{\Member}{\mathrm{Member}}
\newcommand{\Normal}{\mathrm{Normal}}
\newcommand{\patch}{\mathrm{patch}}
\newcommand{\Planning}{\mathrm{Planing}}
\newcommand{\Rebuilding}{\mathrm{Rebuilding}}
\newcommand{\score}{\mathrm{score}}
\newcommand{\Target}{\mathrm{Target}}

\newcommand{\PadLen}{\mathit{PadLen}}
\newcommand{\Interpr}{\mathit{Interpr}}

\newcommand{\Healing}{\mathrm{Healing}}
\newcommand{\start}{\mathrm{start}}
\newcommand{\state}{\mathrm{state}}
\newcommand{\Stem}{\mathrm{Stem}}
\newcommand{\tape}{\mathrm{tape}}
\newcommand{\TransferSw}{\mathrm{TransferSw}}
\newcommand{\Un}{\mathrm{Univ}}

\newcommand{\increment}[1]{#1\mathord{+}\mathord{+}}
\newcommand{\decrement}[1]{#1\mathord{-}\mathord{-}}


\newcommand{\ruAddrJmp}{\rul{AddrJmp}}
\newcommand{\Alarm}{\rul{Alarm}}
% \newcommand{\Commit}{\rul{Commit}}
\newcommand{\Comp}{\rul{Compute}}
\newcommand{\BigAlarm}{\rul{BigAlarm}}
%\newcommand{\Vacate}{\rul{Vacate}}
% \newcommand{\Mark}{\rul{Mark}}
\newcommand{\Move}{\rul{Move}}
% \newcommand{\Plan}{\rul{Plan}}
\newcommand{\ruSwing}{\rul{Swing}}
\newcommand{\Transfer}{\rul{Transfer}}
\newcommand{\WriteProgramBit}{\rul{WriteProgramBit}}
\newcommand{\Zigzag}{\rul{Zigzag}}

\newcommand{\mrk}{\mathrm{mrk}}
\newcommand{\K}{K}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Zg}{\mathcal{Z}_g}

\newcommand{\cns}[1]{c_{\textrm{\upshape #1}}}
\newcommand{\CAtt}{\cns{attack}}
\newcommand{\CEsc}{\cns{esc}}
\newcommand{\CPass}{\cns{pass}}
%\newcommand{\CDepth}[1]{\cns{depth-#1}}
\newcommand{\CSim}{\cns{sim}}
\newcommand{\CSpill}{\cns{spill}}

\newcommand{\Left}{\text{left}}
\newcommand{\Right}{\text{right}}

\renewcommand{\r}{r} % what was it?

\begin{document}

\title{A reliable Turing machine}
% Why do I need this?  Some people get the title bookmarked even without this.
\bookmark[page=1,level=0]{A reliable Turing machine}

\author{Ilir \c{C}apuni \and Peter G\'acs}
% \\ Boston University
% \\ gacs@bu.edu

\maketitle

\begin{abstract}
The title says it.
\end{abstract}

\tableofcontents*

\section{Introduction}

\subsection{To be written}

\subsection{Turing machines}\label{sec:TM}

Our contribution could use any one of the standard definitions of a Turing machine, but it will
be convenient to modify it.
Let us recall that a one-tape Turing machine is defined by a finite set \( \Gamma \) of \df{internal states},
a finite alphabet \( \Sigma \) of \df{tape symbols}, a transition function
\begin{align*}
             \delta\colon\Sigma\times \Gamma\to \Sigma\times\Gamma\times\{-1,1\},
\end{align*}
and possibly some distinguished states and tape symbols.
Any array \( A\in\Sigma^{\bbZ} \) is called the \df{tape configuration}, or \df{tape content}.
At any time, the head is at some integer position \( \h \), and is observing the tape
symbol \( A(\h) \).
The meaning of \( \delta(a,q)=(a',q',d) \) is that if \( A(\h)=a \) and the state is \( q \) then
the \( A(\h) \) will be rewritten as \( a' \) and \( \h \) will change to \( \h+d \).

We will use a model that is slightly different, but has clearly the same expressing power.
There is no set of internal states, but the head observes and modifies \emph{two}
neighboring tape cells at a time.
Thus, a Turing machine is defined as a pair
\begin{align*}
   (\Sigma,\tau).
 \end{align*}
The tape alphabet \( \Sigma \) contains at least the distinguished
symbols \( \blank,0,1 \) where \( \blank \) is called the \df{blank symbol}.
We have a \df{transition function}
\begin{align*}
             \tau\colon\Sigma^{2}\to \Sigma^{2}\times\{-1,1\}.
\end{align*}
The tape is blank at all but finitely many positions.
A \df{configuration} is a pair 
        \begin{align*}
             (A,\h),
        \end{align*}
\( \h\in\bbZ \) is the current \df{head position}, or \df{observed cell}, or \df{current cell},
and \( A\in\Sigma^{\bbZ} \) is the \df{tape content}, or \df{tape configuration}:
at position \( p \), the tape contains the symbol \( A(p) \).
If \( \xi=(A,\h) \) is a configuration then we will write
        \begin{align}\label{eq:config-1}
             \quad \xi.\tape=A,\quad \xi.\pos=\h.
        \end{align}
Though the tape alphabet may contain
non-binary symbols, we will restrict input and output to binary.

The transition function \( \tau \) tells us how to compute the next
configuration from the present one.
When the head observes the pair of tape cells
with content \( \va=(a_{0},a_{1}) \) at positions \( \h \), \( \h+1 \) then denoting
         \begin{align*}
           (\va',j)=\tau(\va),
         \end{align*}
will change tape content at positions \( \h \), \( \h+1 \) to \( a'_{0} \), \( a'_{1} \),
and move the head to tape position to \( \h+j \).

\begin{definition}[Fault]\label{def:fault}
A \df{fault} occurs at time \( t \) if the output \( (\va',j) \)
of the    transition function at this time is replaced with some other value
(which is then used to compute the next configuration).
\end{definition}


\subsection{Codes, and the result}

\begin{sloppypar}
For fault-tolerant computation, some redundant coding of the information is needed.  
\end{sloppypar}

\begin{definition}[Codes]\label{def:codes}
    Let \( \Sigma_{1},\Sigma_{2} \) be two finite alphabets.
    A \df{block code} is given by a positive integer \( \Q \)---called
    the \df{block size}---and a pair of functions
    \begin{align*}
            \psi_{*} :\Sigma_{2}\to\Sigma_{1}^{\Q},
            \quad
            \psi^{*}:\Sigma_{1}^{\Q}\to\Sigma_{2}
    \end{align*}
    with the property \( \psi^{*}(\psi_{*}(x))=x \).
It is extended to strings by encoding each letter individually:
\( \psi_{*}(x_{1},\dots,x_{n})=\psi_{*}(x_{1})\dotsm\psi_{*}(x_{n}) \).
\end{definition}

For ease of spelling out a result, we consider only computations whose outcome
is a single symbol, at tape position 1.
The machine will be declared \df{halted} if it writes the blank symbol at tape position 0.

With our machine we want to perform some computation, and want the result to be
correct with large probability even if faults can occur with some small probability
independently at each step.
For this, the input will be encoded by some error-correcting code,
to defend against the possibility of losing information from it even at the first reading.
It is natural to allow the redundancy of the code to depend on the size of the input: in our case,
it will depend logarithmically.

We will generally view a tape symbol as a tuple consisting of several \df{fields}.
The output of computation shoud be possible read from one of these fields,
which for symbol \( a \) will be called \( a.\Output \).

Here is one formulation of the main result, about the ability to compute a
function with values in \( \{0,1\} \) in a fault-tolerant way.
The role of ``halting'' is played by the act of writing the blank symbol.

\begin{theorem}\label{thm:main-main}
There is a Turing machine \( M_{1} \) with a 
function \( a\mapsto a.\Output \) defined on its alphabet, such that
for any Turing machine \( G \) with alphabet \( \Sigma \)
there are \( 0\le\eps <1 \) and \( \alpha_{1},\alpha_{2}>0 \) 
with the following property.

For each input length \( n=\abs{x} \) a block code
\( (\varphi_{*}, \varphi^{*}) \) of block size \( \Q=O((\log n)^{\alpha_{1}}) \) can be constructed 
such that the following holds.

Let \( M_{1} \) start its work from the initial configuration \( \xi=(\varphi_{*}(x),0) \).
Assume further that
during its operation, faults occur independently at random with probabilities \( \le \eps \).

Suppose at time \( t \) the machine \( G \) writes writes the blank symbol
at position 0, and value \( y \) at position 1.
Then denoting by \( \eta(u) \) the configuration of \( M_{1} \) at time \( u \),
at any time \( t' \) after
 \begin{align*}
   t\cdot (\log t)^{\alpha_{2}},
 \end{align*}
we have \( \eta(t').\tape(1).\Output= y \) with probability at least \( 1 - O(\eps) \).
\end{theorem}

We emphasize that the actual
code \( \varphi \) of the construction will depend on \( n \) only in a simple way.

\subsection{A shortcut solution}

A fault-tolerant one-dimensional cellular automaton is constructed
in~\cite{GacsSorg01}.
If our Turing machine could just simulate such an automaton, it would become
fault-tolerant.
This can indeed almost be done provided that the size of the computation is known in advance.
The cellular automaton can be made finite, and we could define
a ``kind of'' Turing machine with a \emph{circular tape} simulating it.
But this solution requires input size-dependent hardware.

It seems difficult to define a fault-tolerant sweeping 
behavior on a regular Turing machine needed to 
simulate cellular automaton, without recreating an entire hierarchy---as we are doing here.


\section{Overview of the construction}

A Turing machine that simulates ``reliably'' any other
Turing machine even when it is subjected to isolated bursts of faults of constant size,
is given in~\cite{burstyTuring13}.
By \df{reliably} we mean that the simulated computation can be decoded from the history
of the simulating machine despite occasional damages.
We will use some of the ideas of~\cite{burstyTuring13}, but we will need to add new ones in
order to maintain a hierarchy.


\subsection{Isolated bursts of faults}\label{sec:bursts}

Let us give a brief overview of a machine \( M_{1} \) that
can withstand isolated bursts of faults, as most of its construction will be reused
in the probabilistic setting.

We break up the task of error correction into several 
problems to be solved.
The solution of one problem gives rise to another one, but the process converges.
\begin{description}
\item[Redundant information] The tape information of the simulated Turing machine
will be stored in a redundant form, more precisely in the form of a block code.
\item[Redundant processing] The block code will be decoded, the retrieved information 
will be processed, and the result recorded.
To limit the propagation of faults, the tape will be split
into tracks that can be handled separately, and the major processing steps will be 
carried out three times within one work period.
\item[Local repair] All the above process must resist a local burst of faults.
For this, it is organized into a rigid, locally checkable structure
with the help of local addresses, and some other tools like sweeps and 
short switchbacks (zigzags).
The local healing procedure is the most complex part of the construction.
\item[Disturbed local repair] A careful organization of the healing procedure
makes sure that even if a new burst interrupts it (or jumps into its middle),
one or two new invocations of it will finish the job.
\end{description}

Here is some more detail.
Each tape cell of the simulated machine \( M_{2} \) will be represented by a block of
some size \( \Q \) called a \df{colony}, of the simulating machine \( M_{1} \).
Each step of \( M_{2} \) will be simulated by a computation of \( M_{1} \) called
a \df{work period}.
During this time, the head of \( M_{1} \) makes a number of sweeps over the
current colony and its right neighbor colony, decodes the represented cell symbols,
then computes and encodes the new symbols, and finally moves the head 
to the new position of the head of \( M_{2} \).

In order to protect information from the propagation of errors,
the tape of \( M_{1} \) is split into \df{tracks}: each track corresponds to a 
\df{field} of a cell symbol of \( M_{1} \) viewed as a data record.
Each stage of computation will be repeated three times.
The results will be stored in separate tracks, and a final cell-by-cell majority vote
will read out the result of the work period from them.

All this organization is controlled by a few key fields, for example a field
called \( \Addr \) showing the position of each cell in the colony, and a field
\( \Sweep \) showing the last sweep of the computation (along with its direction)
that has been performed already.
The most technical part is to protect this control information from bursts.

For example, a burst can reverse the head in the middle of a sweep.
We want to discover such structural disruptions locally, so
the head will not be allowed to go far from the place where its sweep was turned, without
looking back.
Therefore the head will make frequent zigzags even during a single sweep.
This will trigger the healing procedure if for example a premature turn-back is detected.

Note that the healing procedure also must resist interruption (or false start) by a burst.

\begin{remark}
This description uses some words in an informal way.
Some of these will get precise definition later.
For example, the word \df{colony} will have at least two formal definitions.
From the point of view of the program (transition function), it is just a 
sequence of addresses from \( 0 \) to \( \Q-1 \).
From the point of view of the analysis, it is a sequence of
actual adjacent tape cells with the address field having theses values.
\end{remark}


\subsection{Hierarchical construction}\label{sec:hier}

In order to build a machine resisting faults 
occurring independently of each other with some small probability,
we take the approach suggested in~\cite{Kurd78},
and implemented in~\cite{Gacs1dim86} and~\cite{GacsSorg01}
for the case of one-dimensional cellular automata, with some ideas
from the tiling application of~\cite{DurandRomashShenTiling12}.
We will build a \df{hierarchy of simulations}:
machine \( M_{1} \) simulates machine \( M_{2} \) which 
simulates machine \( M_3 \), and so on.
To simplify the outline here, assume now
that all these machines have the same program,
and all simulations have the same block size.
(Later, each level \( k \) will have its own cell size \( \B_{k} \) and block size \( \Q_{k} \),
with \( \B_{k+1}=\B_{k}\Q_{k} \).)

One cell of machine \( M_3 \) is simulated by one colony of machine \( M_{2} \).
Correspondingly, one cell of \( M_{2} \) is simulated by
one colony of machine \( M_{1} \).
So one cell of \( M_3 \) is simulated by \( \Q^{2} \) cells of \( M_{1} \).
Further, one step of machine \( M_3 \) is simulated by one
work period of \( M_{2} \) of, say, \( O(\Q^{2}) \) steps.
One step of \( M_{2} \) is simulated by one work period of \( M_{1} \),
so one step of \( M_3 \) is simulated by \( O(\Q^{4}) \) steps of \( M_{1} \).

Per construction, machine \( M_{1} \) can withstand
bursts of faults with size  \( \le \beta \) for some constant parameter \( \beta \),
separated by some \( O(\Q^{2}) \) fault-free steps.
Machines \( M_{2} \), \( M_3 \), \( \dots \) have the same program, so it
would be natural to expect that machine
\( M_{1} \) can withstand also some \emph{additional}, larger bursts
of size \( \le \beta \Q \) if those are separated by at least \( O(\Q^{4}) \) steps.

But a new obstacle arises.
Damage caused by a big burst spans several colonies.
The repair mechanism of machine \( M_{1} \) outlined in Section~\ref{sec:bursts} above 
is too local to recover from such extensive damage.
This cannot be allowed, since then the whole hierarchy would stop working.
So we add a new mechanism to \( M_{1} \) that, more modestly,
will just try to restore a large enough portion of the
tape (of the extent of several colonies), so it can go on with the simulation of \( M_{2} \).
Note that this mechanism does not aim to restore the lost 
original information: that job is left to higher levels.
It just tries to restore enough structure to enable the (simulated) work of \( M_{2} \).

All machines above \( M_{1} \) in the hierarchy are
``virtual'': the only hardware in the construction is machine \( M_{1} \).
Moreover, they will not be ordinary Turing machines, but \df{generalized} ones,
with some new features that are not needed on the lowest level but seem necessary
in a simulated Turing machine: for example they
allow a positive distance between neighboring tape cells.

A tricky issue is ``forced self-simulation'': while we are constructing machine \( M_{1} \)
we want to give it the feature that it will simulate a machine \( M_{2} \) that
works just like \( M_{1} \).
The ``forced'' feature means that this simulation should
work without any written program (since that could be corrupted by faults).

This will be achieved by
a construction similar to the proof of the Kleene's fixed-point 
theorem (also called recursion theorem).
We first fix a (simple) programming language to express the transition
function of a Turing machine.
We write an interpreter for it in this same language (just as compilers for the 
C language are sometimes written in C).
The program of the transition function of \( M_{2} \)
(essentially the same as that of \( M_{1} \))
in this language, is a string that will be
``hard-wired'' into the transition function of \( M_{1} \), 
so that the latter, at the start of each work period, can write
it on a working track of the current colony.
Then the work period will interpret it, applying it to the data found there, resulting
in the simulation of \( M_{2} \).

This gives rise to an infinite sequence of simulations, to withstand
larger and larger but sparser and sparser bursts of faults.

Since the \( M_{1} \) uses the universal interpreter, which in turns
simulates the same program, it is natural to ask
how  machine \( M_{1} \) simulates a given Turing machine \( G \) that does the 
actual useful computation?
For this task, we set aside a separate track 
on each machine \( M_{i} \), on which some arbitrary other Turing machine can be
simulated.
The higher the level of the machine \( M_{k} \) that performs this
``side-simulation'', the higher the reliability.
Thus, only the simulations \( M_{k}\to M_{k+1} \) are forced, without program
(that is with a ``hard-wired'' program):
the side simulations can rely on written programs, since the robust
structure in the hierarchy \( M_{1},M_{2},\dots \) will support them.

\subsection{From combinatorial to probabilistic noise}

The construction outlined above was related to burst increasing in size and decreasing
in frequency.
In essence, that noise model is combinatorial.
To deal with probabilistic noise combinatorially,
we stratify the set of faulty times \( \Noise \) as follows.
For a series of parameters \( \beta_{k}, V_{k} \),
we first remove ``isolated bursts'' of type \( \pair{\beta_{1}}{V_{1}} \) of elements of this set.
(See below the notion of ``isolated bursts'' of type \( \pair{\beta}{V} \).)
Then, we remove isolated bursts of type \( \pair{\beta_{2}}{V_{2}} \) from the remaining set,
and so on.
It will be shown that with the appropriate choice of parameters, with probability 1,
eventually nothing is left over from the set \( \Noise \).

A composition of two reliable simulations is even more reliable.
We will see that a sufficiently large hierarchy of such
simulations resists probabilistic noise.

\subsection{Difficulties}\label{sec:novelties}

We list here some of the main problems that the paper deals with, 
and some general ways they will be solved or avoided.
Some more specific problems will be pointed out later, along with their solution.

\begin{description}

\item[Non-aligned colonies] A large burst of \( M_{1} \) can modify the order of
entire colonies or create new ones with gaps between them.

To overcome this problem conceptually, we 
introduce the notion of a \df{generalized Turing machine}
allowing for non-adjacent cells.
Each such machine has a parameter \( \B \) called the \df{cell body size}.
The cell body size of a Turing machine in Section~\ref{sec:TM} would still remain
\( 1 \).

    \item[No structure] What to do when the head is in a middle of an empty area
       where no structure exists?
To ensure reliable passage across such areas,
we will try to keep everything filled with cells, even if these are
not part of the main computation.

\item[Clean areas]
    Noise can create areas over
which the predictability of the simulated machine is limited.
In these areas the structure
of the underlying simulation (invisible on this level) may be destroyed.
These areas should not simply be considered blank, since
blankness implies predictable behavior.
We call these areas of  ``disordered'', and call the complement \df{clean}.
Transition properties will allow making conclusions from cleanness, not from disorder.
Examples below will show that when the head enters disorder, it can be ``sucked in''.

\item[Extending cleanness]
  The definition of the generalized Turing machine will stipulate some ``magical'' properties
  helping to restore cleanness:
 \begin{Alphenum}
 \item extension of clean intervals as the head passes in and out of them;
 \item the appearance of a clean ``hole'' around the head 
   whenever it passes a certain amount of time in a small interval;
 \item\label{i:many-slides}
   the cleaning of an interval if it is passed noiselessly a certain number of times.
 \end{Alphenum}
(The need to implement these same properties in simulation is one of the
main burdens of the construction.)
While an area is cleaned, it will also be re-populated with cells.
Their content is not important, what matters is the restoration of predictability.

\item[Rebuilding] If local repair fails, a special rule will be invoked that reorganizes a
larger part of the tape (of the size of a few colonies instead of only a few cells).
This is the mechanism implementing the above ``magical'' properties on the next level.
\end{description}

The following examples show the difficulties that necessitate some of the complexities of the
construction and proof.
Some of them may not be completely understandable without the details of the following program.
You can skip these examples safely, and return to them later when you are wondering
about the motivation for some feature.

\begin{example}[Need for zigging]\label{xmp:zig}
  When the head works in a colony of machine \( M_{1} \)
  performing a simulation of a cell of machine \( M_{2} \), it works in sweeps across the colony.
  But a burst of faults could reverse the head in the middle of a sweep, leaving it uncompleted.
  This way a local burst could create non-local inconsistency.
\end{example}

To handle the problem of Example~\ref{xmp:zig}, the head will proceed in zigzags: every
  step advancing the head in the simulation
  is followed by \( \Z \) steps of going backward and forward again, just checking consistency
  (and starting a healing process if necessary).
  The parameter \( \Z \) will be chosen appropriately.

\begin{example}[Need for feathering]\label{xmp:feather}
  Some big noise can create a number of intervals \( I_{1},I_{2},\dots,I_{n} \)
  consisting of colonies of machine \( M_{1} \), each interval with its own simulated head,
  where the neighboring intervals are in no relation to each other.
  When the head is about to return from the end of \( I_{k} \)
  (never even to zig beyond it, see the discussion after Example~\ref{xmp:zig}),
  a burst can carry it over to \( I_{k+1} \) where
  the situation may be symmetric: it will continue the simulation that \( I_{k+1} \) is performing.
  (The rightmost colony of \( I_{k} \) and the leftmost colony of \( I_{k+1} \) need not be complete:
  what matters is only that the simulation in \( I_{k} \) would not bring the head beyond its right end,
  and the simulation in \( I_{k+1} \) would not bring the head beyond its left end.)

  The head can be similarly captured to \( I_{k+2} \), then much later back from \( I_{k+1} \) to \( I_{k} \),
  and so on.
  This way the restoration of structure in \( M_{2} \) may be delayed too long.
\end{example}

The device by which we will mitigate the effect of this kind of capturing is another property of the
the movement of the head which will call \df{feathering}:
if the head turns back from a tape cell then next time it must go beyond.
This requires a number of adjustments to the program (see later).

\begin{example}[Two slides over disorder]
  This example shows the possibility for the head to slide twice over disorder without cleaning it.
  
Consider two levels of simulation as outlined in Section~\ref{sec:hier}: 
machine \( M_{1} \) simulates \( M_{2} \) which simulates \( M_{3} \).
The tape of \( M_{1} \) is subdivided into colonies of size \( \Q_{1} \).
A burst on level 1 has size \( O(1) \), while a burst on level 2 has size \( O(\Q_{1}) \).

Suppose that \( M_{1} \) is performing a simulation in colony \( C_{0} \).
An earlier big burst may have created a large interval \( D \) of disorder
on the right of \( C_{0} \), even reaching into \( C_{0} \).
For the moment, let \( C_{0} \) be called a \df{victim} colony.
Assume that the left edge of \( D \) represents the last stage of a transfer operation to the right neighbor 
colony \( C_{0}+Q_{1} \).
When the head, while performing its work in \( C_{0} \), moves close to its right end, a small burst may 
carry it over into \( D \).
There it will be ``captured'', and continue the (unintended) right transfer operation.
This can carry the head, over several successful colony simulations in \( D \), to some
victim colony \( C_{1} \) on the right from which it will be captured to the right similarly.
This can continue over new and new victim colonies \( C_{i} \) (with enough space between them to allow for
new bursts to occur), all the way inside the disorder \( D \).
So the \( M_{2} \) cells in \( D \) will fail to simulate \( M_{3} \).

After a while the head may return to the left in \( D \)
(performing the simulations in its colonies).
When it gets at the right end of a victim colony \( C_{i} \), a burst might move it back there.
There is a case when \( C_{i} \) now can just continue its simulation and then send the head
further left: when before the head was captured on its right,
it was in the last stage of simulating a left turn of the head of machine \( M_{2} \).

In summary, 
a big burst can create a disordered area \( D \) which can capture the head and on which the head can slide
forward and back without recreating any level of organization beyond the second one.
\end{example}

\begin{example}[Many slides over disorder]\label{xpl:unbounded}
  Let us describe a certain ``organization'' of a disordered area in which an unbounded number of passes
  may be required to restore order.
For some \( n<0 \), let the cells of \( M_{1} \) at positions
\( x_{-\Q_{1}},\dots,x_{n} \), where \( x_{i+1}=x_{i}+\B_{1} \),
represent part of a healthy colony \( C(x_{-\Q_{1}}) \) starting at \( x_{-\Q_{1}} \), where \( x_{n} \)
is the rightmost cell of \( C(x_{-\Q_{1}}) \)
to which the head would come in the last sweep before
the simulation will move to the \emph{left} neighbor colony \( C(x_{-2\Q_{1}}) \).
Let them be followed by cells \( x_{n+1},\dots, x_{\Q_{1}-1},\dots\)
which represent the last sweep of a transfer operation to the \emph{right} neighbor colony \( C(x_{0}) \).
If the head is in cell \( x_{n} \), a burst can transfer it to \( x_{n+1} \).
The cell state of \( M_{2} \) simulated by \( C(x_{-\Q_{1}}) \) need to be in \emph{no relation} to 
the cell state of \( M_{2} \) simulated by \( C(x_{0}) \).
This was a capture of the head by a burst of \( M_{1} \) across the point 0, to the right.

We can repeat the capture scenario, say around points \( i \Q_{1}\Q_{2} \) for \( i=1,2,\dots \),
and this way cells of \( M_{3} \) simulated by \( M_{2} \) (simulated by \( M_{1} \))
can be defined arbitrarily, with no consistency needed between any two neighbors.
(We did not write \( i \Q_{1} \) just in case bursts are not allowed in neighboring colonies.)
In particular, we can define them to implement a \emph{leftward} capture scenario
via level 3 bursts at points \( i \Q_{1}\Q_{2}\Q_{3}\Q_{4} \), allowing to simulate arbitrary cells of \( M_{5} \)
with no consistency requirement between neighbors.
So \( M_{5} \) could again implement a rightward capture scenario, and so on.
In summary, a malicious arrangement of disorder and noise allows \( k \) passes
after which the level of organization is still limited to level \( 2 k + 1 \).
\end{example}

Our construction will ensure that, on the other hand,
\( O(k) \) passes (free of \( k \)-level will restore organization to level \( k \).
This property of the construction will be incorporated into our definition of a generalized
Turing machines as the ``magical'' property~\eqref{i:many-slides} above.


\section{Notation}\label{sec:notation}

Most notational conventions given here are common; some other ones will
also be useful.

\begin{description}

\item [Natural numbers and integers] 
By \( \bbZ \) we denote the set of integers.
\begin{align*}
   \bbZ_{>0}&=\setOf{x}{x\in \bbZ,\;  x>0}, \\
   \bbZ_{\ge 0}&=\bbN=\setOf{x}{x\in \bbZ,\;  x\ge 0}.
\end{align*}

\item [Intervals]
We use the standard notation for intervals:
\begin{align*}
   \clint{a}{b}&=\setOf{x}{a\le x \le b},\quad \lint{a}{b}=\setOf{x}{a\le x < b}, \\
   \rint{a}{b}&=\setOf{x}{a< x \le b}, \quad  \opint{a}{b}=\setOf{x}{a< x < b}.
\end{align*}
We will also write \( \lint{a}{b} \) in place of \( \lint{a}{b}\cap \bbZ \), 
whenever this leads to no confusion.
Instead of \( \lint{x+a}{x+b} \), sometimes we will write 
\begin{align*}x + \lint{a}{b}.\end{align*}

\item [Ordered pairs]
Ordered pairs are also denoted by \( \pair{a}{b} \),
but it will be clear from the context whether we are
referring to an ordered pair or open interval.

\item [Comparing the order of a number and an interval]
For a given number \( x \) and interval \( I \), we
write
\begin{align*} x \ge I \end{align*}
if for every \( y\in I \),  \( x \ge y \).

\item [Distance]
The distance between two real numbers \( x \) and \( y \) is defined
in a usual way:
\begin{align*}
    d(x,y)= \abs{x-y}.
\end{align*}

The \df{distance of a point \( x \) from interval \( I \)}  is
\begin{align*}
    d(x,I)= \min_{y\in I}d(x,y).
\end{align*}

\item [Ball, neighborhood, ring, stripe]
A \df{ball of radius \( r>0 \), centered at \( x \)} is
\begin{align*}
    B(x,r)= \setOf{y}{d(x,y)\le r}.
\end{align*}
An \df{\( r \)-neighborhood of interval } \( I \) is
\begin{align*}
    \setOf{x}{d(x,I)\le r}.
\end{align*}
An \df{\( r \)-ring} around interval \( I \) is
\begin{align*}
    \setOf{x}{d(x,I)\le r \txt{ and } x \notin I}.
\end{align*}
An \df{\( r \)-stripe to the right of interval \( I \)} is
\begin{align*}
    \setOf{x}{d(x,I)\le r \txt{ and } x \notin I \txt{ and } x>I}.
\end{align*}

\item[Logarithms] Unless specified differently,
the base of logarithms throughout this work is 2.

\end{description}


\section{Specifying a Turing machine}\label{sec:specifying}

Let us introduce the tools allowing to describe the reliable Turing machine.

\subsection{Universal Turing machine}\label{sec:UTM}

We will describe our construction in terms of
universal Turing machines,
operating on binary strings as inputs and outputs.
We define universal Turing machines in a way that allows
for rather general ``programs''.

 \begin{definition}[Standard pairing]
For a (possibly empty) binary string\\ \( x=x(1)\dotsm x(n) \) let us introduce the map
 \[
   \ang{x} = 0^{\abs{x}}1 x,
 \]
Now we encode pairs, triples, and so on, of binary strings as follows:
 \begin{align*}
        \ang{s,t} &=\ang{s}t,
\\ \ang{s,t,u} &= \ang{\ang{s,t},u},
 \end{align*}
and so on.

From now on, we will assume that our alphabet \( \Sigma \)
is of the form \( \Sigma=\{0,1\}^{s} \), that is the
tape symbols are viewed as binary strings of a certain length.
Also, if we write \( \ang{i,u} \) where \( i \) is some number, it is understood
that the number \( i \) is represented in a standard way by a binary string.
\end{definition}

\begin{definition}[Computation result, universal machine]
 Assume that a Turing machine \( M \) starting on binary \( x \),
 at some time \( t \) writes the first time the blank symbol at position
 0 (this is interpreted as halting).
 Then we look at the longest (possibly empty) binary string to be found starting at position
 1 on the tape, and call it the \df{computation result} \( M(x) \).
We will write
 \begin{align*}
   M(x,y)=M(\ang{x,y}),\quad M(x,y,z)=M(\ang{x,y,z}),
 \end{align*}
and so on.

A Turing machine \( U \) is called \df{universal} among Turing machines with
binary inputs and outputs, if for every Turing machine \( M \),
there is a binary string \( p_{M} \) such that for all \( x \) we have
\( U(p_{M},x)=M(x) \).
(This equality also means that the computation denoted on the left-hand side 
halts if and only if the computation on the right-hand side does.)
\end{definition}

Let us introduce a special kind of universal Turing machines, to be
used in expressing the transition functions of other Turing machines.
These are just the Turing machines for which the so-called \( s_{mn} \) theorem
of recursion theory holds with \( s(x,y)=\ang{x,y} \).

\begin{definition}[Flexible universal Turing machine]\label{def:univ-TM}
A universal Turing machine will be called \df{flexible} if 
whenever \( p \) has the form \( p=\ang{p',p''} \) then
\begin{align*}
 U(p,x)= U(p',\ang{p'',x}).
 \end{align*}
Even if \( x \) has the form \(x =\ang{x',x''} \), this definition chooses
\( U(p',\ang{p'',x}) \) over \( U(\ang{p,x'},x'') \), that is starts with 
parsing the first argument
(this process converges, since \( x \) is  shorter than \( \ang{x,y} \)).
\end{definition}

It is easy to see that there are flexible universal Turing machines.
On input \( \ang{p,x} \),
a flexible machine first checks whether its ``program'' \( p \) 
has the form \( p=\ang{p',p''} \).
If yes, then it applies \( p' \) to the pair \( \ang{p'',x} \).
(Otherwise it just applies \( p \) to \( x \).)

\begin{definition}[Transition program]
  Consider an arbitrary Turing machine \( M \) with alphabet
\( \Sigma \), and transition function \( \tau \).
A binary string \( \pi \) will be called a \df{transition program} of \( M \) if
whenever \( \tau(\va)=(\va',j) \) we have
 \begin{align*}
 U(\pi,\va)=\ang{\va,j}.
 \end{align*}
We will also require that the computation induced by the program makes
\( O(\abs{\pi}+\abs{a}) \) left-right turns, over a length tape \( O(\abs{\pi}+\abs{a}) \).
\end{definition}

In our simulations of a Turing machine \( M_{2} \) on some other machine \( M_{1} \),
the transition program of \( M_{2} \) will just provide a way to compute
the (local) transition function of \( M_{2} \) by the universal machine;
it does not organize the rest of the simulation.

\begin{remark}
 In the construction of universal Turing machines provided by the textbooks
(though not in the original one given by Turing), the program is generally a string
encoding a table for the transition function \( \tau \) of the simulated machine \( M \).
Other types of program are imaginable: some simple transition functions can
have much simpler programs.
However, our fixed machine is good enough (similarly to the optimal machine
for Kolmogorov complexity).
If some machine \( U' \) simulates \( M \) via a
very simple program \( q \), then
 \begin{align*}
     M(x)=U'(q,x) = U(p_{U'},\ang{q,x}) = U(\ang{p_{U'},q},x),
 \end{align*}
so \( U \) simulates this computation via the program \( \ang{p_{U'},q} \).
\end{remark}

\subsection{Rule language}\label{sec:language}

In what follows we will describe the generalized Turing machines \( M_{k} \) for all \( k \).
They are all similar, differing only in the parameter \( k \); the most important activity
of \( M_{k} \) is to simulate \( M_{k+1} \).
The description will be uniform, except for the parameter \( k \).
We will denote therefore \( M_{k} \) simply by \( M \), and \( M_{k+1} \)  by \( M^{*} \).
Similarly we will denote the block size \( \Q_{k} \) of the block code of the 
simulation simply by \( \Q \).

Instead of writing a huge table describing the transition function \( \tau_{k}=\tau \),
we present the transition function as a set of \df{rules}.
It will be then possible to write one \emph{interpreter} program that carries
out these rules; that program can be written for some fixed flexible 
universal machine \( \Un \).

Each rule consists of some (nested) conditional statements,
similar to the ones seen in an ordinary program:
 ``\textbf{if} \textit{condition} \textbf{then} \textit{instruction}
\textbf{else} \textit{instruction}'', 
where the condition is testing values of some fields of the observed cell pair, and
the instruction can either be elementary, or itself a conditional statement. 
The elementary instructions are an \df{assignment} of a value to a field
of a cell symbol, or a command to move the head.
Rules can call other rules, but these calls will never form a cycle.
Calling other rules is just a shorthand for nested conditions.

Even though rules are written like procedures of a program,
they describe a single transition.
When several consecutive statements are given, then they
%(almost always)
change different fields of the state or
cell symbol, so they can be executed simultaneously.
% Otherwise and in general, even if a field is updated in
% some previous statement, in all following statements that use
% this field, its old value is considered.

Assignment of value \( x \) to a field \( y \) of the state or cell symbol will
be denoted by \( y \gets x \).
We will also use some conventions introduced by the C language:
namely,
\( x\gets x+1 \) and \( x\gets x-1 \) are abbreviated to \( \increment{x} \) and
\( \decrement{x} \) respectively.

Rules can also have parameters, like \( \ruSwing(a,b,u,v) \).
Since each rule is called only a constant number of times in the whole program,
the parametrized rule can be simply seen as a shorthand.

Mostly we will describe
the rules using plain English, but it should always be clear that they
are translatable into such rules.


\subsection{Fields}\label{sec:fields}

\begin{sloppypar}
For the machine \( M \) we are constructing, each tape symbol will 
be a tuple \( q=(q_{1},q_{2},\dots,q_{k}) \),
where the individual elements of the tuple will be called \df{fields}, and will
have symbolic names.
For example, we will have fields \( \Addr \) and \( \Drift \),
and may write \( q_{1} \) as \( q.\Addr \) or just \( \Addr \), 
\( q_{2} \) as \( q.\Drift \) or \( \Drift \), and so on.
\end{sloppypar}

The array of values of the same field of the cells will be called a \df{track}.
Thus, we will talk about the \( \Hold \) track of the tape, carrying the
\( \Hold \) fields of the cells.
 Each field of a cell has a possible value
\( \emptyset \) whose approximate meaning is ``undefined''.

In what follows we describe some of the most important fields we will use;
others will be introduced later.

A properly formatted configuration of \( M \) splits the tape into blocks of \( \Q \)
consecutive cells called \df{colonies}.
One colony of the tape of the simulating
machine represents one cell of the simulated machine.
The two colonies that correspond to the pair of cells that the
simulated machine is scanning is called the \df{base colony-pair}
(a precise definition will refer to the actual history of the work of \( M \)), and its
members will be called the \df{left} and \df{right base colony}.
Sometimes the left base colony will just be called the \df{base colony}.
Most of the computation proceeds over the base colony-pair.
The direction of the simulated head movement, once figured out by the computation,
is called the \df{drift}.
Two neighbor colonies may not be adjacent, in which case the cells will form
a \df{bridge} between them.

The present behavior of the computation will be characterized by
a field called the \df{mode}:
 \begin{align*}
   \Mode\in\{ \Normal,\Healing, \Rebuilding \}.
 \end{align*}
 If its value in the current cell is \( \Normal \) we will say that the
 computation is in \df{normal} mode.
 In this case, the machine is engaged in the regular business of simulation.
The \df{healing} mode tries to correct some local fault due to a couple of neighboring
bursts, while the \df{rebuilding} mode attempts to restore the colony structure
on the scale of a couple of colonies.
The 
\begin{align*}
   \Info
 \end{align*}
track of a colony of \( M \)
contains the string that encodes the content of the simulated cell of \( M^{*} \).
The
\begin{align*}
 \Prog
 \end{align*}
track stores the program of \( M^{*} \), in an appropriate form 
to be interpreted by the simulation .
(As mentioned in Section~\ref{sec:hier}, this field will be subject to
some ``hard-wiring'' manipulation.)
The field 
 \begin{align*}
  \Addr
 \end{align*}
of the cell shows the position of the cell in its colony:
it takes values in \( \lint{0}{\Q} \).
The direction in \( \{-1,1\} \) in which the simulated head moves will be recorded on the track
 \begin{align*}
   \Drift.
 \end{align*}
The
 \begin{align*}
 \Sweep
 \end{align*}
field counts the sweeps that the head makes during the work period.
The number of the last sweep of the work period will depend on the drift \( d \), 
and will be denoted by 
\begin{align}\label{eq:Last}
   \Last(d).
 \end{align}
In calculating parameters, we will make use of  
\begin{align}\label{eq:V}
   \V=\max(\Last(-1),\Last(1)).
 \end{align}
Cells will be designated as belonging to a number of possible \df{kinds}, signaled by the
field 
\begin{align*}
     \Kind
 \end{align*}
with values \(  \Vacant, \Stem, \Member_{0},\Member_{1},\Bridge \).
Here is a description of the role of these cell kinds.

\( \Vacant \) is not really the kind of any cell, it is just used in the transition function
to show that there is no cell or to describe the action of killing a cell.
\( \Stem \) is the kind of newly created cells that do not participate in any known colony structure.
We will also try to keep all areas between colonies filled with (not necessarily adjacent)
stem cells.
For example the computation may find that a colony does not properly encode
a tape cell by the required error-correcting code.
Then we want to ``kill'' the whole colony.
This will happen by turning the kind of each of its cells to \( \Stem \).

Cells of the base colony-pair are of type \( \Member_{0} \) and \( \Member_{1} \) respectively.
If the base colony pair is not adjacent then there will be a \df{bridge} of \( <\Q \)
adjacent cells of type \( \Bridge \) between them.

\begin{definition}\label{def:extends}
We will say that the bridge \df{extends} a colony if its cells are adjacent to it.  
\end{definition}
Under normal conditions,  
if the \( \Drift \) track on the bridge has value \( -1 \) then it is extending the left base
colony, otherwise is extending the right base colony.
Cells of the bridge will continue the addressing (modulo \( \Q \)) of the colony it extends.

During healing, some special fields of the state and cell are used, treated as subfields of the field
 \begin{align}\label{eq:Heal}
   \Heal.
 \end{align} 
In particular, there will be a \( \Heal.\Sweep \) field.
During rebuilding, we will work with subfields of the field
\begin{align*}
 \Rebuild,   
 \end{align*}
and a cell will be called \df{marked for rebuilding} if \( \Rebuild.\Sweep\ne 0 \).


\section{Exploiting structure in the noise}\label{sec:noise}

\subsection{Sparsity}\label{sec:sparsity}
Let us introduce a technique connecting the combinatorial and probabilistic
noise models.

\begin{definition}[Centered rectangles, isolation]
Let \( \vek{r}=\pair{r_{1}}{r_{2}} \), \( r_{1}, r_{2}\ge 0 \),
be a two-dimensional nonnegative vector.
An \df{rectangle} of radius \( \vek{r} \) \df{centered} at \( \vek{x} \) is
\begin{align}\label{eq:ball1}
  B(\vek{x},\vek{r}) = \setOf{\vek{y}}{\abs{y_{i} - x_{i}} \le r_{i}, i=1,2}.
\end{align}  
Let \( E\subseteq \bbZ^{2} \) be a two-dimensional set.
A point \( \vek{x} \) of \( E \) is \df{\( \pair{\vek{r}}{\vek{r}^{*}} \)-isolated} if
\begin{align*}
  E \cap B(\vek{x},\vek{r}^{*})\subseteq B(\vek{x}, \vek{r}).
 \end{align*}
Set \( E \) is \df{\( (\vek{r}, \vek{r}^{*}) \)-sparse} 
if \( D(E, \vek{r}, \vek{r}^{*})=\emptyset \), that is 
it consists of \( (\vek{r}, \vek{r^{*}}) \)-isolated points.
Let
\begin{align}
  D(E,\vek{r}, \vek{r}^{*}) =
     \setOf{\vek{x}\in E}{\vek{x} \txt{ is not } (\vek{r}, \vek{r}^{*})\txt{-isolated
  from } E}.
\end{align}
\end{definition}

\begin{definition}[Sparsity]\label{def:sparsity}
Let
\begin{align}\label{eq:beta}
 \beta\ge 9
 \end{align}
 be a parameter, and let 
\begin{align*}
  0<\B_{1}<\B_{2}<\dotsm,\quad
  \Tu_{1}<\Tu_{2}<\dotsm,\quad
  1\le\gamma_{1}\le\gamma_{2}\le\dotsm
\end{align*}
be sequences of positive integers to be fixed later.
For a two-dimensional set \( E \), let \( E^{(1)} = E \).
For \( k>1 \) we define recursively:
\begin{align}\label{eq:noise^k}
    E^{(k+1)} = D(E^{(k)}, \beta\pair{\B_{k}}{\Tu_{k}}, \gamma_{k}\pair{\B_{k+1}}{\Tu_{k+1}}).
\end{align}
Set \( E^{(k)} \) is called the \df{\( k \)-th residue} of \( E \).
It is \( k \)-\df{sparse} if \( E^{(k+1)}=\emptyset \).
It is simply \df{sparse} if \( \bigcap_{k}E^{(k)}=\emptyset \).

When \( E=E^{(k)} \) and \( k \) is known
then we will denote \( E^{(k+1)} \) simply by \( E^{*} \).
\end{definition}

The following lemma connects the above defined sparsity notions to the requirement
of small fault probability.
It is formulated somewhat redundantly, for easier application.

\begin{lemma}[Sparsity]\label{lem:sparsiness}
Let \( \Q_{k} = \B_{k+1}/\B_{k} \), \( \U_{k} = \Tu_{k+1}/\Tu_{k} \), and
\begin{align}\label{eq:growth-assumption}
  % \lim_{k\rightarrow\infty}\frac{\log(\U_{k} \Q_{k})}{1.5^k}=0.
  \lim_{k\rightarrow\infty}\frac{\log(\gamma_{k}\U_{k} \Q_{k})}{1.5^k}=0.
\end{align}
For sufficiently small \( \eps \), for every \( k\ge 1 \) the following holds.
Let \( E\subseteq \bbZ\times \bbZ_{\ge 0} \)
be a random set with the property that each pair \( \pair{p}{t} \) belongs to \( E \)
independently from the other ones with probability \( \le \eps \).

Then for each point \( \vek{x} \)  and each \( k \),
 \begin{align*}
   \Pbof{B(\vek{x},(\B_{k}, \Tu_{k}))\cap E^{(k)}\neq\emptyset} <2\eps \cdot 2^{-1.5^{k}}.
 \end{align*}
As a consequence, the set \( E \) is sparse with probability 1.
\end{lemma}

\begin{proof}
Let \( k=1 \).
Rectangle \( B(\vek{x},(\B_{1}, \Tu_{1})) \) is a single point, hence
the probability of our event is \( <\eps \).
Let us prove the inequality by induction, for \( k+1 \).

Note that our 
event depends at most on the rectangle \( B(\vek{x},3(\B_{k}, \Tu_{k})) \).
Let
\begin{align*}
   p_{k}=2\eps\cdot 2^{-1.5^{k}}.
\end{align*}
Suppose \( \vek{y} \in E^{(k)}\cap B(\vek{x},\gamma_{k}(\B_{k+1}, \Tu_{k+1})  ) \).
Then, according to the definition of \( E^{(k)} \),  there is a point
\begin{align}\label{eq:sparse-as}
 \vek{z} \in
 B(\vek{y},\gamma_{k}(\B_{k+1},\Tu_{k+1}))\cap E^{(k)}\setminus B(\vek{y},\beta(\B_{k}, \Tu_{k})).
 \end{align}
Consider a standard partition of the (two-dimensional) space-time into
rectangles \( K_{p}=\vek{c}_{p}+\lint{-\B_{k}}{\B_{k}}\times \lint{-\Tu_{k}}{\Tu_{k}} \)
with centers \( \vek{c}_{1},\vek{c}_{2},\dots \).
The rectangles \( K_{i},K_{j} \) containing \( \vek{y} \) and \( \vek{z} \)
respectively intersect \( B(\vek{x}, 2\gamma_{k}(\B_{k+1}, \Tu_{k+1})) \).
The triple-size rectangles 
\( K'_{i}=c_{i} + \lint{-3\B_{k}}{3\B_{k}}\times \lint{-3\Tu_{k}}{3\Tu_{k}} \) and
\( K'_{j} \) are disjoint, since \eqref{eq:beta} and~\eqref{eq:sparse-as} imply
 \( \abs{y_{1} - z_{1}}>\beta\B_{k} \) and \( \abs{y_{2} - z_{2}}>\beta\Tu_{k} \).

The set \( E^{(k)} \) must intersect two rectangles \( K_{i} \),
\( K_{j} \) of size \( 2(\B_{k}, \Tu_{k}) \) separated by at least \( 4(\B_{k}, \Tu_{k}) \),
of the big rectangle \( B(\vek{x},2\gamma_{k}(\B_{k+1}, \Tu_{k+1})) \).

By the inductive hypothesis, the event \( \cF_{i} \) that
\( K_{i} \) intersects \( E_{k} \) has probability bound \( p_{k} \).
It is independent of the event \( \cF_{j} \), since these events depend
only on the triple size disjoint rectangles \( K'_{i} \) and \( K'_{j} \).

The probability that both of these events hold is at most \( p_{k}^{2} \).
The number of possible rectangles
\( K_{p} \) intersecting \( B(\vek{x},2\gamma_{k}(\B_{k+1}, \Tu_{k+1})) \) is
at most
\( C_{k}:=((2\gamma_{k}^{2}\U_{k} \Q_{k})+2)^{2} \), so the number of possible pairs of rectangles
is at most \( C_{k}^{2}/2 \), bounding the probability of our event by
 \begin{align*}
   C_{k}^{2}p_{k}^{2}/2
    &=
      2 C_{k}^{2}\eps^{2} 2^{-1.5^{k+1}}\cdot 2^{-0.5\cdot 1.5^{k}}
   \\ &=2\eps 2^{-1.5^{k+1}} \cdot \eps
        C_{k}^{2}2^{-0.5\cdot 1.5{k}}.
 \end{align*}
Since \( \lim_{k}\frac{\log{(\gamma_{k}\U_{k} \Q_{k})}}{1.5^k}=0 \),
%Since \( \lim_{k}\frac{\log{(\U_{k} \Q_{k})}}{1.5^k}=0 \),
the last factor is \( \le 1 \) for sufficiently small  \( \eps \).
\end{proof}

In our construction we will choose
\begin{align*}
  \gamma_{k}=\Omega(k^{2}),
  \quad \Q_{k} = \Omega(\gamma_{k}^{2}),
  \quad \U_{k} = \Omega(\Q_{k}^{2}),
\end{align*}
while still satisfying~\eqref{eq:growth-assumption}.

\subsection{Error-correcting code}\label{sec:coding}

Let us add error-correcting features to the block codes introduced in
Definition~\ref{def:codes}.

\begin{sloppypar}
\begin{definition}[Error-correcting code]\label{def:err-code}
A block code is \( (\beta,t) \)-\df{burst-error-correcting},
if for all \( x\in\Sigma_{2} \), \( y\in\Sigma_{1}^{\Q} \) we
have \( \psi^{*}(y)=x \) whenever \( y \) differs from
\( \psi_{*}(x) \) in at most \( t \) intervals of size \( \le\beta \).
For such a code, we will say that a word \( y\in\Sigma_{1}^{\Q} \) is \( r \)-\df{compliant}
if it differs from a codeword of the code by at most \( r \) intervals of size \( \le\beta \).
\end{definition}
  \end{sloppypar}

\begin{example}[Repetition code]\label{xmp:tripling}
  Suppose that \( \Q\ge 3\beta \) is divisible by 3,
  \( \Sigma_{2}=\Sigma_{1}^{\Q/3} \), \( \psi_{*}(x)=xxx \).
  If \( y=y(1)\dots y(\Q) \), then \( x=\psi^{*}(y) \) is defined by
    \( x(i)=\maj(y(i),y(i+\Q/3),y+2\Q/3) \).
    For all \( \beta\le \Q/3 \), this is a
    \( (\beta,1) \)-burst-error-correcting code.
    If we repeat 5 times instead of 3, we get a \( (\beta,2) \)-burst-error-correcting
    code.
    Let us note that there are much more efficient such codes than just repetition.
 \end{example}

Consider a Turing machine 
\( (\Sigma,\tau) \) (actually a generalized one to be defined later)
simulating some Turing machine \( (\Sigma^{*},\tau^{*}) \).
We will assume that the alphabet \( \Sigma^{*} \) is a subset of the set of  binary strings
\( \{0,1\}^{l} \) for some \( l<\Q \) (we can always ignore some tape
symbols, if we want).

\begin{definition}[Interior]\label{def:interior}
Let 
\begin{align*}
  \PadLen 
\end{align*}
be a parameter to be defined later (in~\eqref{eq:PadLenDef}).
Consider an interval \( I \) of neighbor cells.
A cell belongs to the \df{interior} of \( I \) if there are at least \( \PadLen \) neighbors between it 
and the complement of \( I \).
In particular, we will talk about the interior of a colony.
\end{definition}

We will store the coded information in the interior of the colony, since it is more exposed 
to errors near the boundaries.
So let \( (\upsilon_{*}, \upsilon^{*}) \) be a \( (\beta,2) \)-burst-error-correcting block code
with
\begin{align*}
  \upsilon_{*}: \{0,1\}^{l} \cup \set{\emptyset}
   \to\{0,1\}^{(\Q-2\cdot\PadLen)\B}.
\end{align*}
We could use, for example, the repetition code of Example~\ref{xmp:tripling}.
Other codes are also appropriate, but we require that they have some fixed
programs \( p_{\encode} \), \( p_{\decode} \)
on the universal machine \( \Un \), in the following sense:
 \begin{align*}
   \upsilon_{*}(x)=\Un(p_{\encode},x),\quad
   \upsilon^{*}(y)=\Un(p_{\decode}, y).
 \end{align*}
Also, these programs must work in quadratic time and linear space on a one-tape
Turing machine (as the repetition code certainly does).

Let us now define the block code \( (\psi_{*}, \psi^{*}) \) used below in the
definition of the configuration code \( (\varphi_{*}, \varphi^{*}) \)  
outlined in Section~\ref{sec:hier-codes}:
\begin{equation}\label{eq:psi}
   \psi_{*}(a)  = 0^{\PadLen}\upsilon_{*}(a)0^{\PadLen}.
\end{equation}
The decoded value \( \psi^{*}(x) \) is obtained by first removing \( \PadLen \)
symbols from both ends of \( x \) to get \( x' \), and then computing \(
\upsilon^{*}(x') \).
 It will be easy to compute the configuration code from \( \psi_{*} \),
once we know what fields there need initialization.


\section{The model}\label{sec:model}

Recall the definition of sparsity in Section~\ref{sec:sparsity}: there will be 
a sequence \( 0<\B_{1}<\B_{2}<\dotsm \) of ``scales'' in space and a sequence
\( \Tu_{1}<\Tu_{2}<\dotsm \) of scales in time, and a constant \( \beta \).
We will define a sequence of simulations \( M_{1}\to M_{2}\to\dotsm \) where
each \( M_{k} \) is a machine simulating one on a higher level.
For simplicity, we will use the notation \( M=M_{k} \), \( M^{*}=M_{k+1} \),
and similarly for the other parameters, for example \( \B,\Tu, \Q, \U \).
As indicated in Section~\ref{sec:hier}, these machines will generalize
ordinary Turing machines, with a number of new features.

\begin{notation}
  For an interval \( I=\lint{a}{b} \) and some \( c\ge 0 \) let
  \begin{align*}
   \Int(I,c) = \lint{a+c}{b-c}.
  \end{align*}
  Similarly for intervals \( \rint{a}{b} \), \( \clint{a}{b} \), \( \opint{a}{b} \).
\end{notation}

\subsection{Generalized Turing machine}\label{sec:gen-TM}

Standard Turing machines do not have
operations like ``creation'' or ``killing'' of cells, nor
do they allow for cells to be non-adjacent.
We introduce here a \df{generalized Turing machine}.
It depends on an integer \( \B \ge 1 \) that denotes the cell body size,
and an upper bound \( \Tu \) on the transition time.
These parameters provide the illusion that the different Turing
machines in the hierarchy of simulations all operate on the same linear space and the same time
interval.
Even if the notions of cells and alphabet are different for each machine of the hierarchy, 
at least the notion of a \emph{location on the tape} is the same.
There will also be a \df{ pass number} parameter \( \passno \),
whose meaning will be explained in the definition of trajectories.


\begin{definition}[Generalized Turing machine]\label{def:gen-TM}
    A \df{generalized Turing machine} \( M \) is defined by a tuple
        \begin{align}\label{eq:gen-TM}
             (\Sigma, \tau, \Adj,\B, \Tu, \passno),
       \end{align}
    where \( \Sigma \) is a finite set called the \df{alphabet},
        \begin{align*}
             \tau: \Sigma^{2}\to \Sigma^{2}\times\{-1,1\}
        \end{align*}
    is the \df{transition function},
\( \Adj \) is a function on the alphabet (can be called a ``field'' if a symbol is viewed 
as a piece of data, a record with several fields).
\( \Adj\colon\Sigma\to\{ 0, 1 \} \) will show 
whether the last move to the present cell was from an adjacent cell (it could have been for example
from a non-adjacent neighbor).
The integer \( \B\ge 1 \) is called the \df{cell body size},
and the real number \( \Tu \) is a bound on the transition time.
The integer \( \passno \) will be the number of passes needed to clean an area (see below).
Among the elements of the tape alphabet \( \Sigma \), 
we distinguish the elements \( 0,1,\Bad,\Vacant \).
The role of the symbols \( \Bad \) and \( \Vacant \) will be clarified below.
(They can be viewed to correspond to the ``blank'' symbol of a regular Turing machine.)
\end{definition}

The definition of a configuration below introduces a new concept, the \df{current cell-pair},
a pair of positions \( \vhc = (\hc_{0},\hc_{1}) \).
(We will call \( \hc_{0} \) the \df{current cell}.)
The current cell's position may not exactly be the position of the head: here is some explanation.
A generalized Turing machine \( M^{*} \) may be simulated by some lower-level (possibly
generalized) Turing machine \( M \).
Not all details of the tape content of \( M \) are visible on the level of \( M^{*} \): what is visible
is the content of the cells of \( M^{*} \) decoded from those of \( M \).
The currently observed cell-pair in the simulated machine \( M^{*} \) is \( \vhc \).
On the other hand the true head position \( h \) of \( M \)
may perform a lot of oscillatory movement within and around the current cell pair
not directly relevant to the work of \( M^{*} \).

\begin{definition}[Configuration]\label{def:config}
     Consider a generalized Turing machine~\eqref{eq:gen-TM}.
    A \df{configuration} is a tuple
        \begin{align*}
             (A,\h,\vhc),
        \end{align*}
where \( A:\bbZ\to\Sigma \), \( \h,\hc_{j}\in\bbZ \).
The array \( A \) is the tape configuration.
It is \( \Vacant \) in all but finitely many positions.

As in~\eqref{eq:config-1} before, \( \h \) is the head position, but it may differ from the current cell
\( \hc_{0} \), so now if \(  \xi= (A,\h,\vhc) \) then we write
        \begin{align}\label{eq:config-2}
             \xi.\tape=A, \quad \xi.\pos=\h,\quad \xi.\curcell_{j}=\hc_{j}. 
        \end{align}
A point \( p \) is \df{clean} if  \( A(p)\ne\Bad \).
A set of points is \df{clean} if it consists of clean points.
Whenever the interval \( \h+\lint{4\B}{4\B} \) is clean the current cell-pair
must be within it.

We say that there is a \df{cell} at a position \( p\in\bbZ \) if the interval
\( p+\lint{0}{\B} \) is clean and \( A(p)\ne \Vacant \).
In this case, we call the interval \( p+\lint{0}{\B} \) the \df{body} of this cell.
Cells must be at distance \( \ge\B \) from each other, that is their
bodies must not intersect.
If they are at a distance \( <2\B \) from each other then they are called \df{neighbors}.
They are called \df{adjacent} if the distance is exactly \( \B \).
A configuration will always have the following property: % Do we need this?
\begin{varenum}{C}
\item\label{i:config.sharp-ends}
 Any endpoint of a maximal clean interval is the end of a cell body in that direction.
\end{varenum}
Let
    \begin{align*}
         \Configs_{M}
    \end{align*}
    denote the set of all possible configurations of a Turing machine \( M \).
\end{definition}

All the above definitions can clearly be localized to define a configuration
\df{over a space interval} \( I \), where it is always understood that \( \h\in I \), that is 
\( I \) contains the head.

\begin{definition}[Local configuration, replacement]
\label{def:local-config}
  A \df{local configuration on} a (finite or infinite)
  interval \( I \) is given by values assigned to the cells
  of \( I \), along with the following information: whether
  the head is to the left of, to the right of or inside
  \( I \), and if it is inside, on which cell.

  If \( I' \) is a subinterval of \( I \), then a local configuration
  \( \xi \) on \( I \) clearly gives rise to a local configuration
  \( \xi(I') \) on \( I' \) as well, called its \df{subconfiguration}.
  We can similarly talk about local tape configurations.
  
  Let \( \xi \) be a tape configuration and \( \zeta(I) \) a local
  tape  configuration.
  Then the tape configuration \( \xi\vert\zeta(I) \) is obtained by
  replacing \( \xi \) with \( \zeta \) over the interval \( I \).
\end{definition}

It is natural to name a sequence of configurations that is conceivable as a computation
(faulty or not) of a Turing machine as ``history''.
The histories that obey the transition function then could be called ``trajectories''.
In the following definition we will 
stretch this notion to encompass also some limited violations of the
transition function.
In connection with any underlying Turing machine with a given starting configuration, we will
denote by
\begin{align}\label{eq:noise-first}
   \Noise\subseteq \bbZ\times \bbZ_{\ge 0}
\end{align}
the set of space-time points \( \pair{p}{t} \), such that
a fault occurs at time \( t \) when the head is at position \( p \).

\begin{definition}[History]\label{def:history}
  \begin{sloppyenv}
    For a generalized Turing machine~\eqref{eq:gen-TM}, 
consider a sequence \( \eta = (\eta(0), \eta(1), \dots) \) of configurations with
\( \eta(t) = \) \( (A(\cdot, t), \h(t), \vhc(t)) \), along with a noise set \( \Noise \).
The \df{switching times} of this sequence are the times \( t \) when 
the position or content of the current cell-pair changes.
The interval between two consecutive switching times is the \df{dwell period}.
The pair
      \end{sloppyenv}
    \begin{align*}
       (\eta,\Noise)
    \end{align*}
    will be called a \df{history} of machine \( M \) if the following conditions hold.
        \begin{itemize}
            \item \( \abs{\h(t) - \h(t')} \le \abs{t' - t} \).

            \item In two consecutive configurations, the content \( A(p,t) \) of the positions \( p \)
              not in \( \h(t) + \lint{-2\B}{2\B} \), remains the same: for example
                  \( A(n,t+1) = A(n,t) \) for all \( n \notin \h(t) + \lint{-2\B}{2\B} \).
            \item At each noise-free switching time the head is on the new current cell:
\( \hc_{0}(t)=\h(t) \).
In particular, when at a switching time a current cell becomes
\( \Vacant \), the head must already be on another (current) cell.

            \item The length of any noise-free 
dwell period in which the head is staying on clean positions is at most \( \Tu \).

        \end{itemize}
We say that a cell \df{dies} in a history if it becomes \( \Vacant \).
    Let
        \begin{align*}
            \Histories_{M}
        \end{align*}
    denote the set of all possible histories of \( M \).

The above definition can be \df{localized} to define a history
\df{over a space-time rectangle} \( I\times J \), 
where it is always understood that \( \h\in I \) for all times \( t\in J \),
that is \( I \) contains the head throughout the time interval considered.
\end{definition}

In Section~\ref{sec:traj} below, we will define a certain subset of possible histories, called \df{trajectories}.
The set of trajectories of \( M \) will be denoted by
\begin{align*}
   \Trajectories_{M}.
 \end{align*}
 The definition of generalized Turing machines will be finished only by the definition of
 the set of  trajectories.
 But in order to motivate their choice, we first introduce the notions of simulation and
 a hierarchy of simulations.

 \subsection{Simulation}\label{sec:sim}


Until this moment, we used the term ``simulation'' informally, to denote
a correspondence between configurations of
two machines which remains preserved during the computation.
In the formal definition, this correspondence will essentially be a code
\( \varphi=(\varphi_{*},\varphi^{*}) \).
The \emph{decoding} part of the code is the more important.
We want to say that machine \( M_{1} \) simulates machine \( M_{2} \) via
simulation \( \varphi \) if whenever \( (\eta, \Noise) \) is a trajectory of \( M_{1} \) 
then \( (\eta^{*},\Noise^{*}) \),
defined by \( \eta^{*}(\cdot,t)=\varphi^{*}(\eta(\cdot,t)) \), is a
trajectory of \( M_{1} \).
Here, \( \Noise^{*} \) is computed by an appropriate mapping.

We will make, however, two refinements.
First, we may weaken the condition by requiring it only for
those \( \eta \) for which the initial configuration
 \( \eta(\cdot,0) \) has been obtained by encoding, that is it has the form 
\( \eta(\cdot,0)=\varphi_{*}(\xi) \).
This gives a role to the encoding function in the definition.

But there is a more complex refinement.
When a colony is in transition between encoding one simulated value to encoding another one,
there may be times when the value represented by it before the transition
is already not decodable from it, and the value after the transition is not yet decodable from it.
Our way of dealing with these situations is 
to define the simulation decoding as a mapping \( \Phi^{*} \)
between \emph{histories}, not just configurations.
This allows a certain amount of ``looking back'':
the map \( \Phi^{*} \) can depend on the configurations at the beginning of the ``work period''.

It is the mapping \( \Phi^{*} \) that will also define \( \Noise^{*} \).
A history was defined above in Definition~\ref{def:history} 
as a pair \( (\eta,\Noise) \), so we will have
\( \Phi^{*}(\eta,\Noise)=(\eta^{*},\Noise^{*}) \).
The set  \( \Noise^{*} \) will be defined just as in Definition~\ref{def:sparsity} of sparsity:
by deleting those small isolated parts of \( \Noise \) that the 
error-correcting simulation can deal with.

\begin{definition}[Simulation] \label{def:simulation-central}
Let \( M_{1},M_{2} \) be two generalized Turing machines, and let
\begin{align*}
    \varphi_{*}:\Configs_{M_{2}} \to \Configs_{M_{1}}
\end{align*}
be a mapping from configurations of \( M_{2} \)
to those of \( M_{1} \), such that it maps
starting configurations into starting configurations.
We will call such a map a \df{configuration encoding}.
Let
\begin{align*}
   \Phi^{*}:\Histories_{M_{1}} \to \Histories_{M_{2}}
\end{align*}
be a mapping.
The pair \( (\varphi_{*}, \Phi^{*})  \)
is called a \df{simulation} (of \(  M_{2}  \) by \(  M_{1}  \)) if for every
trajectory \(  (\eta, \Noise)  \) with initial
configuration \(  \eta(\cdot,0)=\varphi_{*}(\xi)  \),
the history \(  (\eta^{*},\Noise^{*})=\Phi^{*}(\eta,\Noise)  \) is
a trajectory of machine \(  M_{2}  \).

We say that \( M_{1} \) \df{simulates} \( M_{2} \) if there is a simulation
\( (\varphi_{*},\Phi^{*}) \) of \( M_{2} \) by \( M_{1} \).
\end{definition}

\subsection{Hierarchical codes}\label{sec:hier-codes}

Recall the notion of a code in Definition~\ref{def:codes}.

\begin{definition}[Code on configurations]\label{def:configuration-code}
\begin{sloppypar}
 Consider two generalized Turing machines \( M_{1},M_{2} \) with the corresponding
alphabets and transition functions, and an integer \( \Q\ge 1 \).
We require \(   \B_{2} = \Q \B_{1} \).
Assume that a block code
\(
   \psi_{*}:\Sigma_{2}\to\Sigma_{1}^{\Q}
\)
is given, with an appropriate decoding function, \( \psi^{*} \).
Symbol \( a\in\Sigma_{2} \), is interpreted as the content of some tape square.
\end{sloppypar}

This block code gives rise to a \df{code on configurations}, that is a pair of functions
    \begin{align*}
        \varphi_{*} :\Configs_{M_{2}} \to \Configs_{M_{1}},
        \quad
        \varphi^{*}:\Configs_{M_{1}} \to \Configs_{M_{2}}
    \end{align*}
    that encodes some (initial) configurations \( \xi \) of \( M_{2} \) into configurations of \( M_{1} \).
    Let \( \xi \) be a configuration of \( M_{2} \) with \( \xi.\curcell_{j}=\xi.\pos+j\B_{2} \).
    We set \( \varphi_{*}(\xi).\pos = \xi.\pos \),
    \( \varphi_{*}.\curcell_{j}= \varphi_{*}(\xi).\pos+j\B_{2} \),
  and
\begin{align*}
 \varphi_{*}(\xi).\tape(i\B_{2}, \dots, (i+1)\B_{2} - \B_{1}) = \psi_{*}(\xi.\tape(i)).
 \end{align*}
A configuration \( \xi \) is called a \df{code configuration} if it has the form \( \xi=\varphi_{*}(\zeta) \).
 \end{definition}


\begin{definition}[Hierarchical code]\label{def:hierarchical-code}
For \( k\ge 1 \), let \( \Sigma_{k} \) be an alphabet, of a generalized Turing machine \( M_{k} \).
Let \( \Q_{k}>0 \) be an integer colony size, let \( \varphi_{k} \)
be a code on configurations defined by a block code
  \begin{align*}
       \psi_{k}: \Sigma_{k+1}\to \Sigma_{k}^{\Q_{k}}
  \end{align*}
as in Definition~\ref{def:configuration-code}.
The sequence \( (\Sigma_{k},\varphi_{k}) \), (\( k\ge 1) \),  is
called a \df{hierarchical code}.
For the given hierarchical code, the configuration \( \xi^{1} \) of \( M_{1} \)
is called a \df{hierarchical code configuration} if a sequence
of configurations \( \xi^{2},\xi^{3},\dots \) of \( M_{2},M_{3},\dots \) exists with
\begin{align*}
 \xi^{k}=\varphi_{*k}(\xi^{k+1})
 \end{align*} 
for all \( k \).
(Of course, then whole sequence is determined by \( \xi^{1} \).)

Let \( M_{1} \), \( M_{2} \), \( \dots\) be a sequence of generalized Turing machines,
let \( \varphi_{1} \), \( \varphi_{2} \) , \(\dots \) be a hierarchical code for this sequence,
let \( \xi^{1} \) be a hierarchical code configuration for it, where \( \xi^{k} \) is an
initial configuration of \( M_{k} \) for each \( k \).
Let further be given a sequence of mappings \( \Phi^{*}_{1} \), \( \Phi^{*}_{2} \), \( \dots \) 
such that for each \( k \), the pair \( (\varphi_{k*},\Phi_{k}^{*}) \),
is a simulation of \( M_{k+1} \) by \( M_{k} \).
Such an object is called a \df{tower}.
\end{definition}

The main task of the work will be the construction of a certain tower.
Since the simulation property is highly nontrivial, so is the existence of towers.
Generalized Turing machines with the particular form of the notion of
a trajectory are introduced since they can be proved to form towers. 

\subsection{Trajectories}\label{sec:traj}

This subsection completes the definition of a generalized Turing machine.
The set of trajectories \( \Trajectories_{M} \) 
is defined in terms of constraints imposed on the burst-free parts of a history.
We discuss these properties first informally.

\begin{description}
\item[Transition Function] This property says (in more precise terms)
that in the absence of noise and in a clean area, the
transition function is obeyed.

\item[The Spill Bound] limits the extent to which a disordered interval can spread while
the head is in it.

\item[Escape] limits the time that the head can spend in a small area.

\item[Attack Cleaning] helps erode the disorder as the head repeatedly enters and leaves it.

\item[Pass Cleaning] limits the number of back-and-forth slides that can happen over a
large disordered area (under certain conditions).
Otherwise every time the head reaches, say, its left end,
a new burst could expand the disorder.
As these bursts are not visible in the simulated trajectory \( \eta^{*} \),
the Spill Bound property of \( \eta^{*} \) could be violated.

\end{description}

\begin{definition}
Suppose that at times \( t' \) before a switching time \( t \) but after 
any previous switch, the current cell-pair has state \( \va \),
further after the switch the same cell pair has state \( \va' \)
(including when one of these cells dies).

We say that the switch is \df{dictated by the transition function} if
\begin{align*}
 (\va',\sign(\hc_{0}(t')-\hc_{0}(t))) =  \tau(\va),
 \end{align*}
further \( a'_{0}.\Adj=1 \) if and only if \( \hc_{0}(t')-\hc_{0}(t)\in\{-\B,0,\B\} \).
\end{definition}

We will need the following constants, to be fixed later.
\begin{align}\label{eq:cns.traj}
  \CAtt<\beta/2,\;
  \CSpill,\;
  5<\theta<\beta,\;
  \CPass\le\theta/6,\;
  \CEsc.
 \end{align}
 % \begin{definition}[Clean hole]\label{def:clean-hole}
% The following definitions will use the constants
% % If in a configuration the head is in a clean interval of size \( \ge 1.5 \CAtt\B \),
% % at a distance \( \ge\B \) from the complement, then we will say that the head is \df{inside a clean hole}.
% \end{definition}

\begin{premark}
  Some details of the following definition need to be adjusted yet.
\end{premark}


\begin{definition}[Trajectory]\label{def:traj}
\begin{sloppypar}
   A history  \( (\eta, \Noise) \) of a generalized Turing 
machine~\eqref{eq:gen-TM} with \(\eta(t) =\)
\( (A(t), \h(t), \vhc(t)) \)
is called a \df{trajectory} of \( M \) if the following conditions hold, in any 
noise-free space-time interval \( I\times J \).
  \end{sloppypar}
\begin{description}

\item[Transition Function]\label{i:def.traj.transition}
Suppose that there is a switch, and the current cell-pair is 
is inside the clean area, by a distance of at least \( 2.5\B \).
Then the new state of the current cell-pair, and
the direction towards the new current head position
are dictated by the transition function.
If a new current cell did not exist before then it is adjacent to one of the
previous ones before the switch.
The only change on the tape occurs on the interval enclosing the new and old current cells.
Further, the length of the dwell period is bounded by \( \Tu \).

\item[Spill Bound]\label{i:spill-bound}
  An interval of disorder can spread by at most \( \CSpill \B \)
  on either side while it contains the head.

\item[Escape] \label{i:def.traj.escape}
  The head will leave any interval of size \( \le \lambda\B \) with \( 1\le\lambda\le 3\beta \)
  within time \( \CEsc\lambda^{2}\Tu \).

\begin{sloppypar}
\item[Attack cleaning] \label{i:def.traj.attack-cleaning}
Suppose that the current cell-pair is at the end of a clean interval of size
\( \ge (\CAtt+1)\B \).
Suppose further that the transition function directs the head right.
Then by the time the head comes back to \( x-\CAtt \B \), the right end of the clean interval containing it
advances to the right by at least \( \B \).
 \end{sloppypar}

A similar property is required when ``left'' and ``right'' are interchanged.

\item[Pass Cleaning] This property will be defined below in Definition~\ref{def:pass-cleaning}.
It is motivated by Example~\ref{xpl:unbounded}.

\end{description}

\end{definition}

Our construction will set \( \passno=O(k) \) for a machine on level \( k \) of the simulation,
that is \( O(k) \) passes will restore organization to level \( k \).
(Essentially, at most three slides are needed to raise the level.)
But some restriction is needed: if between the noiseless passes there is a large number of
\df{intrusions} that are not passes, then each of these intrusions can bring a little new disorder, possibly
undoing the effect of passes.
Limiting the \emph{number}
of intrusions does not seem feasible, since there can be too many local intrusions.
Instead, we will limit the number of a certain kind of rectangles covering them.
This will help since
in our noise model, if a space-time area is noiseless for the simulated machine \( M^{*} \)
then essentially every \( 2\gamma\cdot (\Q\B)\times (\U\Tu) \) rectangle contains at most one burst
(of size \( \beta\cdot \B\times \Tu \)).

\begin{definition}
  For a space-time path \( P \) of the head and an interval \( I \),
let \( P_{u:v} \) be its part between times \( u \) and \( v \).
We write \( P_{:v} \) for the part from start to \( v \) and \( P_{u:} \)
for the part from \( u \) to end.

A part \( P_{u:v} \)
is a \df{pass} of \( I \) if \( P \) enters \( I \) at time \( u \) and leaves it on the other side
at time \( v \) (without leaving \( I \) before).
\end{definition}

% \begin{definition}
%   % For some \( W,H>0 \), a \( (W,H) \)-\df{rectangle} is a space-time rectangle of width \( \le W \) and
%   % height \( \le H \).
%   % We say that the rectangle has \df{max-height} if its height is \( H \),
%   % and \df{max-width} if its width is \( W \).
%   % It is \df{maximal} if its width or height is maximal.
% % For a path \( P \) let \( R(P) \) be the smallest rectangle containing \( P \).
% % The rectangles \( R(P_{:t}) \) are increasing with \( t \) both in width and height.
% % Let \( R_{W,H}(P) \) be defined as the largest \( R(P_{:t}) \) that is still a \( (W,H) \)-rectangle.
% \end{definition}

\begin{definition}[\( W \)-segments]\label{def:segments}
  Consider a path \( P \) over some time interval \( J=\lint{t}{u} \)
  with the property that no burst occurs on it while it is in space interval \( I \)
  (bursts might occur on it while it is outside \( I \)).
  Break it up into segments in such a way
  that each segment has space projection \( \le W \) and only the last segment
  can have space projection \( <W \).
  Fix such a segmentation and call it the
  \( W \)-\df{segmentation} of \( P \); its parts are called the  \( W \)-\df{segments}.
  In the \( W \)-segmentation of \( P \) we call the \( W \)-segments intersecting \( I \)
  but not belonging to a pass of \( I \) the \( W \)-\df{intrusions} of \( P \) into \( I \).
\end{definition}

Note that if \( P \) is noiseless and \( W=\lambda\B \) with \( 1\le \lambda\le 3\beta \) then
by the Escape property,
the time spent in each \( W \)-segment of \( P \) is at most \( \CEsc\lambda^{2}\Tu \).

Recall the constants defined in~\eqref{eq:cns.traj}.

\begin{definition}[Pass cleaning property]\label{def:pass-cleaning}
  Suppose that a path \( P \) makes at least \( \passno \) pairs of passes over interval \( I \)
  of size \( \le\theta\B \), with at most
  a number \( \passno^{2} \) of \( \theta\B \)-intrusions.
  Then at some time during \( P \) the interior \( \Int(I, \CPass\B) \) of \( I \) becomes clean.
 \end{definition}

The trajectory properties can clearly be localized to some space-time
rectangle just as the definition of history was.


\section{Simulation structure}

In what follows we will describe the program of the reliable Turing machine
(more precisely, a simulation of each \( M_{k+1} \) by \( M_{k} \) as defined above).
Most of the time, we will refer to \( M_{k} \) as \( M \) and to \( M_{k+1} \) as \( M^{*} \).
Cells will be grouped into colonies, where  \( \Q=\B^{*}/\B \) is the colony size.
The behavior of the head on
each colony simulates the head of \( M^{*} \) on the corresponding cell
of \( M^{*} \).
The process takes a number of steps, constituting a \df{work period}.

\begin{definition}\label{def:Tu}
  We will have a constant
   \begin{align}\label{eq:CU-def}
   \CSim 
   \end{align}
   with the property that the maximum number of steps that any simulation work period
   is at most \( \CSim\beta\Q^{2}/2 \). % Why /2?
   Let 
\begin{align*}
 \U=\CSim\beta\Q^{2},\; \Tus=\U\Tu.
\end{align*}

\end{definition}

Machine \( M \) will perform the simulation even if the noise
in which it operates is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse.
By the above definitions, 
sparsity means that noise comes in \df{bursts} that are confined to
rectangles of size \( \beta \) (affecting at most \( \beta \) consecutive tape cells), 
and are separated from each other in time in such a way that there is at most one burst
in any \( \gamma \) neighboring work periods.
A design goal for the program is the following:

\begin{goal}[Local correction]\label{goal:locality}
Correct a burst within space and time comparable to its size, and much smaller than the 
size of a simulation work period.
\end{goal}

The program is rather complex, but there are some limitations to a
modular presentation in which after the presentation of each part it is shown
what properties of the simulation it guarantees.
Indeed, noise occurring during any part of the  program
can change the environment of the head into  
some state corresponding to an arbitrary other part.

\df{Modes} were introduced in Section~\ref{sec:language}.
Ordinary simulation proceeds in the normal mode. 
To see whether the basic structure
supporting this process is broken somewhere, each step will check 
whether the current cell-pair is \df{coordinated}
(see Definition~\ref{def:coordinated}).
If not then the state of the current pair will be changed into the \df{healing} mode.
We will also say that \df{alarm} will be called.
On the other hand, the state my enter into \df{rebuilding} mode on some indications that
healing fails.
% The crudest outline of the main rule of machine \( M \) is given in
% Rule~\ref{alg:main1}; the \( \Comp \) and \( \Transfer \) rules will be
% outlined below.

% \begin{algorule}\caption{\textbf{Main rule}\label{alg:main1}}

%   \If{the mode is normal}{
%     \lIf{\algNot \( \Coordinated \)}{\( \rHeal \)}
%     \lElseIf{\( 1 \le \Sweep < \TransferSw(1) \)}{ \( \Comp \) }
%     \lElseIf{\( \TransferSw(1) \le \Sweep < \Last \)}{ \( \Transfer \)}
%    \lElseIf{\( \Last \le \Sweep  \)}{move the head to the new base.}
%   }
% \end{algorule}


\subsection{Head movement}\label{sec:sweep}

The global structure of a work period is this:
\begin{description}

\item[Computation phase] 
The new state of the simulated cell pair, and the simulated direction (called the drift) is computed.
Then the ``meaningfulness'' of the result is checked.
During this phase the head sweeps, roughly, back-and-forth between
the ends of the base colony-pair.

\item[Transfer phase]
  The head moves into the neighbor colony in the simulated head direction called drift (building or rebuilding
  a bridge if needed).
  Redundancy is used to protect this movement from a burst.  
\end{description}

The number of the current sweeps within a work period is contained by a field \( \Sweep \).

\begin{definition}[Front]\label{def:front}
  During normal computation, as the head sweeps in a certain direction, it increases \( \Sweep \) field by 1.
Let us call the last site in which this increase occured, the \df{front}.
\end{definition}
Globally in a configuration, due to disorder, there may be more than one front, but locally
we can talk about ``the'' front without fear of confusion.
We can read off the direction of the sweep \( s \) from its parity:
 \begin{align}\label{eq:sweep-dir}
       \dir(s)=(-1)^{s + 1}.
 \end{align}

 Due to the zigging and feathering requirements, in Section~\ref{sec:novelties},
 there will be two kinds of turns: \df{small} turns and \df{big} turns.
 The process uses the constants
\begin{align}\label{eq:turn-limit}
   \cns{turn-limit-1}, \cns{turn-limit-2},   \f = 5
\end{align}
to be fixed later, and the parameters,
\begin{align}\label{eq:FDef}
\Z = \passno^{2+\eps}\; \F = \Z\passno^{2+\eps}
\end{align}
with \( \eps>0 \) to be fixed later, which  allows us to define the earlier used parameter
\begin{align}\label{eq:PadLenDef}
  \PadLen &= \F\log\Q.   
 \end{align}
The choices will be motivated in Sections~\ref{sec:feathering} and~\ref{sec:pass-cleaning}.
When the simulation needs the head to turn, this will in general not happen immediately.
Turns will be handled by a field
\begin{align*}
   \Turned\in\{ 0,1,2 \}.
\end{align*}
The value \( 1 \) shows the memory of a recent turn.
The value \( 2 \) shows the memory of a recent big turn within distance \( \F \).
When a cell is passed in which \( \Turned>0 \) was found then this value is set to \( 0 \).

\begin{description}
\item[Small turn]
Move until you found a cell with \( \Turned\ne 1 \), then turn.
If you did not find it in \( \cns{turn-limit-1} \) steps then call alarm.
Set \( \Turned\gets 1 \) in this cell.

Small turns will be done during zigging and healing (see later).

\item[Big turns]
Move until you found \( \F \) consecutive cells with \( \Turned<2 \)
the last one of which has \( \Turned=0 \), then make a turn.
On the cell after the turn, set \( \Turned\gets 1 \),
and also on the first interval of  \( \F \) cells after the turn, set \( \Turned\gets 2 \).
If you did not find it in \( \cns{turn-limit-2}\Q \)
steps then call alarm: such an event will be called a \df{turn-starvation}.
The control can be accomplished by a counter, say called \( \FDepth \).

Big turns will be done during the ends of sweeps in simulation and rebuilding.
\end{description}
     

\subsubsection{Zigging}\label{sec:zigging}

A burst could turn the head back in the middle of its sweep,
as pointed out in Example~\ref{xmp:zig}.
To detect such an event promptly (in accordance with Goal~\ref{goal:locality}),
a \df{zigzag} movement will be superimposed on the sweeping,
so that the head can check the consistency of a few cells  around the front.
(Their number is not constant, it will depend on \( \pi \). )
The process creates a \df{frontier zone} of about \( 2\Z \) cells in both directions around the front,
where \( \Z \) was defined in~\eqref{eq:FDef}.
The field
\begin{align*}
  \ZigAddr\in\lint{-2\Z}{2\Z}.
\end{align*}
of the current cell shows the cell's address position in the frontier zone.
The zone itself is the interval of cells in which \( \ZigAddr \) is defined.
After every \( \f \) steps of progress, the head will perform a forward-backward-forward
zigzag checking the zone.
For this it uses \df{small turns} defined above, so it may need a few more steps to find
a turning point.
Due to the spacing of all turns, under normal conditions, this will need
no more than \( \f \) steps (as will be proved).

On backward zig (with respect to the sweep direction), we expect the zig addresses to
grow continuously from \( -2\Z \) to \( 2\Z \); the head does not change them.
On forward zig, the addresses will look like \( \dots, i-1,i,i+j,i+j+1,\dots \), for some \( 0<j\le\f \).
If \( \SwDir>0 \) then \( i+j \) will be changed to \( i+1 \), otherwise
\( i \) will be changed to \( i+j-1 \).

\begin{remarks}\label{rem:zigging-choices}
  \begin{enumerate}
  \item The need for backward zigging was explained in Example~\ref{xmp:zig}.
  \item The forward-zigging property and the frontier zone
    will be used in the poof of Lemma~\ref{lem:pass-clean}.
    We will need to show there that
    in the absence of new noise, after a constant number of passes, a clean interval \( J \)
    of size, say, \( \ge 6\Z\B \) will extend to the right until it reaches
    the end of a colony pair or of a rebuilding area.
    Forward zigging helps prevent the disorder from turning the head back prematurely.
  \item The choice \( \Z=\passno^{\eps} \) will be justified in the same proof:
    it lower-bounds the size of new noise capable of turning back the head.
  \end{enumerate}
\end{remarks}

\subsubsection{Feathering}\label{sec:feathering}

Here we motivate the definition of \df{small turns} and \df{big turns} introduced above.
Example~\ref{xmp:feather} above pointed out the need for the
following feature of the simulation program.

\begin{definition}[Feathering]\label{def:feathering}
A Turing machine execution is said to have the \( c \)-\df{feathering} property if the following holds.
If the head turned back at a position \( x \) at some time, then next time it comes within distance \( c \)
of \( x \), it can turn back only at least \( c \) steps beyond \( x \).
\end{definition}

(The name ``feathering'' refers to the picture of the path of the head in a space-time diagram.)
The following example suggests that any computation can be reorganized to accomodate feathering,
without too much extra cost.

\begin{example}[1-feathering]\label{xmp:feathering}
Suppose that, arriving from the left at position 1, the head decides to turn left again.
In repeated instances, it can then turn back at the following sequence of positions:
\begin{align*}
 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, 5, 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, \dots
 \end{align*}
\end{example}

If in the original execution the head turned back \( t \) consecutive
times to the left from position \( p \), then now it will 
turn back from somewhere in a zone of size \( O(\log t) \) to the right of \( p \) in 
each of these times.
(Computing the exact turning point is not necessary.)

Normally, a small turn attempt would take longer than \( O(1) \) steps and
a big turn attempt would take longer than \( O(\F\log\Q) \) steps only in the presence of
large disorder (those cases will be analyzed in the context of large disorder).
We give here only an informal argument justifying this statement; a
formal proof must wait until 
a complete definition of the simulation and of the notion of large disorder.

For the moment, let us call cells  \df{red} with by \( \Turned=1 \)
and \df{orange} with \( \Turned=2 \).
Red cells can delay small turns;
they can only arise in the forward turn of a zig in the previous sweep.
Zigs are by the definition spaced by \( \ge\f \) cells apart.
Big turns are delayed by orange cells; they are also the ones to create orange cells.
But they will be organized as in Example~\ref{xmp:feathering},
therefore a big turn attempt will be delayed by at most \( \F \) times the logarithm
of the total number of big turns inside a colony or a rebuild area.

The turns at the end a rebuilding area are more problematic since
there may be nothing known about the cells over which it is extended---so it may indeed
happen that a place to turn is not reached before alarm is called: this is what we called
\df{turn starvation} above.

The simulated Turing machine will also have the feathering property,
therefore the simulation will not turn back 
from one and the same colony repeatedly, without having passed it in the meantime.

\begin{sloppypar}
\begin{remark}\label{rem:big-turns}
  The size of the parameter \( \F \) is motivated by the proof of Lemma~\ref{lem:pass-clean}.
  Here is a sketch of the argument (it can be safely skipped now).
   At some time \( t_{0} \) in some interval \( I \)
  we will have clean subintervals \( J_{k}(t_{0}) \), \( k=1,2,\dots \)
  of size about \( 6\Z\B \) in which no burst will appear, and which are separated from each other by areas of
  size \( O(\passno^{2}\Z\B)\ll\F\B \).
  For times \( t>t_{0} \) we will consider the maximal clean intervals \( J_{k}(t) \) containing
  the middle of \( J_{k}(t_{0}) \).

  Assume that the head passes over \( I \) noiselessly left to right and later from right to left
  with no bursts.
  If the head moves in a zigging way to the
  right then the attack property will clean out the area between \( J_{i}(t) \) and \( J_{i+1}(t) \),
  joining them.
  This does not happen only in case of a programmed turn
  from the right end of \( J_{i}(t) \), in the course of the simulation or rebuilding.
  But then in the next pair of passes over \( I \), the feathering property implies that
  the programmed turn from the end of \( J_{i}(t) \) is at least a distance \( \F\B \) to the right.
  Our choice of \( \F \) implies that then \( J_{i}(t) \) will be joined to \( J_{i+1}(t) \).
  So two noise-free passes would join all the intervals \( J_{i}(t) \) into a clean area.  
\end{remark}  
\end{sloppypar}

We introduce a convenient notation.
\begin{notation}\label{not:plus}
Suppose that within a procedure a turn attempt is made after a move of \( n \) cells in some direction.
Then the head may not be able to turn right away, the turn attempt moves it a few more steps in the same
direction.
In this case we will say that the head moves \( n^{+} \) cells and turns.
Thus \( n^{+} \) is greater than \( n \), but by not much: only an amount \( O(1) \)
for small turns, and \( O(\F\log \Q) \) for big turns.
\end{notation}

\subsection{Computation phase}\label{sec:computation-phase}

The first phase of the simulation, called the \( \Comp \) rule,
computes new values for current cell-pair of the
simulated machine \( M^{*} \) represented by the current (base) colony-pair,
and the direction of the move of the head of  \( M^{*} \).
During the execution of this rule, the head sweeps the base colony-pair.  
The cell state of \( M^{*} \) will be stored on the track \( \Info \) of the
representing colony.
The move direction of \( M^{*} \) 
will be represented in the \( \Drift \) field of \emph{each} cell of the base colony-pair
(so the whole track must be filled with \( -1 \)'s or \( 1 \)'s).

Recall Definition~\ref{def:err-code}.
The rule \( \Comp \) will rely on a certain fixed \( (\beta,3) \) burst-error-correcting
code, moreover
it expects that each of the words found on the \( \Info \) track
is 2-compliant.  % Probably \( (\beta,2) \) would be sufficient.
The rule \( \rul{ComplianceCheck} \) checks whether a word is \( 2 \)-compliant.

The rule \( \Comp \) essentially repeats 3 times % why was here 5?
the following \df{stages}: decoding, applying the transition, encoding.
Then it calls \( \rul{ComplianceCheck} \); if the latter fails
it will mark the colony for rebuilding.
It uses some additional tracks.
A track called
\begin{align*}
   \Work
 \end{align*}
can be used for auxiliary computation.
A useful track is called
\begin{align*}
   \Adj'.
\end{align*}
At the time of the computation, on this track of the left base colony, each cell
should contain a single symbol in \( \{ 0, 1 \} \), according to whether the
previous left base colony was adjacent or not.
In more detail:
\begin{enumerate}
\item For \( j=1,\dots,3 \)       % Why was here 5 times instead of 3?
  % if \( \Addr \in \set{0, \dots, \Q-1} \)
  do 
       \begin{enumerate}

          \item Calling by \( \va \) the pair of strings found on the \( \Info \) track of
            the interior of the base colonies,
            decode it into the pair of strings \( \tilde\va=\upsilon^{*}(\va) \)
            (this should be the current state of the simulated cell-pair), and
            store it on some auxiliary track in the base colony-pair.
            Do this by simulating the universal machine:
            \( \tilde\va = \Un(p_{\decode}, \va) \).
            Then replace \( \tilde a_{0}.\Adj \) with the value found on the  \( \Adj' \) track.
            % Perform all this computation in a way using no information from outside
            % the address interval \( J \), nor on any 
            % part of the state brought back into this interval other than the address,
            % sweep and zigging fields.

          \item \label{i:comp.trans}
           Compute \( (\va',d)=\tau^{*}(\tilde\va) \).
           Since the program of the transition function \( \tau^{*} \) is not written explicitly anywhere, 
           this ``self-simulation'' step is elaborated in Section~\ref{sec:self-simulation}.

            \item\label{i:comp.write}
              Write the encoded new cell states \( \upsilon_{*}(\va') \) onto the
              \( \Hold[j].\Info \) track of the interior of the base colony-pair.
              Write \( d \) into the \( \Hold[j].\Drift \) field of \emph{each cell} of
              the left base colony.

              If the new state \( a'_{i} \) is a vacant one for \( i=0 \) or 1, that is
              \( a'_{i}.\Kind^{*}=\Vacant^{*} \),
              then write \( 1 \) onto the \( \Hold[j].\Doomed \) track of the corresponding (left or right)
              base colony---else write 0.

        \end{enumerate}

       \item
      % \item Repeat the following twice (hoping that at least
      %   one repetition will be burst-free):  % Why twice?
            Sweeping through the base colony pair,
            at each cell compute the majority of \( \Hold[j].\Info \), \( j=1,\dots,3 \),
            and write it into the field \( \Info \).
            Proceed similarly, and simultaneously, with \( \Drift \).

       \item       For \( j=1,\dots,3 \), call \( \rul{ComplianceCheck} \) on the \( \Info \) track, and
write the resulting bit into the \( \fld{Compliant}_{j} \) track.

Then pass through the colony and turn each cell in which the majority 
of \( \fld{Compliant}_{j} \), \( j=1,\dots,3 \) is false,
into a stem cell (thus destroying the colony if the result was false everywhere).

  \end{enumerate}

It can be arranged---and we assume so---that the total number of sweeps of this
phase, and thus the starting sweep number of the next phase, depends only on \( \Q \).

\subsection{Forced self-simulation}\label{sec:self-simulation}

Here we elaborate step~\ref{i:comp.trans} of Section~\ref{sec:computation-phase}.

\subsubsection{New primitives}

We will make use of the tracks \( \Work \) mentioned above, and the track
\begin{align*}
   \Index
 \end{align*}
that can store a certain address of a colony.

Recall from Section~\ref{sec:language} that the program
of our machine is a list of nested
``\textbf{if} \emph{condition} \textbf{then} \emph{instruction}
\textbf{else} \emph{instruction}''
statements.
As such, it can be represented as a binary string 
 \begin{align*}
   R.
 \end{align*}
If one writes out all details of the construction of the present paper, this string \( R \)
becomes explicit, an absolute constant.
But in the reasoning below, we treat it as a parameter.

Let us provide a couple of \df{extra primitives} to the rules.
First, they have access to the parameter \( k \) of machine \( M=M_{k} \), 
to define the transition function
 \begin{align*}
            \tau_{R,k}(\va).
 \end{align*}
The other, more important, new primitive is a special instruction
 \begin{align*}
   \WriteProgramBit
 \end{align*}
in the rules.
When called, this instruction makes the assignment \( \Work\gets R(\Index) \).
This is the key to self-simulation: \emph{the program has
access to its own bits}.
If \( \Index=i \) then it writes \( R(i) \) onto the current position of the \( \Work \) track.


\subsubsection{Simulating the rules}

By convention, in our fixed flexible universal machine \( \Un \),
program \( p \) and input \( x \) produce an output \( \Un(p,x) \).
The structure of all rules is simple enough that they can be read and
interpreted by \( \Un \) in reasonable time:

\begin{theorem}
There is a constant string called \( \Interpr \) with the property that for
all positive integers \( k \), string \( R \) that is a
sequence of rules, and a pair of bit strings \( \va=(a_{0},a_{1}) \) with \( a_{j}\in\Sigma_{k} \),
 \begin{align*}
  \Un(\Interpr,R,0^{k},\va)=\tau_{R,k}(\va).
 \end{align*}
The computation on \( \Un \) takes time \( O(\abs{R}\cdot \abs{\va}) \).
\end{theorem}

The proof parses and implements the rules in the string \( R \); each of these rules
checks and writes a constant number of fields.

Implementing the \( \WriteProgramBit \) instruction is straightforward:
Machine \( \Un \) determines the number \( i \)
represented by the simulated \( \Index \) field, 
looks up \( R(i) \) in \( R \), and writes it into the simulated \( \Work \) field.

There is no circularity in these definitions:
  \begin{itemize}
  \item 
The instruction \( \WriteProgramBit \) is written \emph{literally}
in \( R \) in the appropriate place, as ``\(\WriteProgramBit \)''.
The string \( R \) is \emph{not part} of the rules (that is of itself).  
  \item On the other hand, the computation in
\( \Un(\Interpr,R,0^{k},\va) \) 
has \emph{explicit} access to the string \( R \) as one of the inputs.
  \end{itemize}

Let us show the computation step invoking the ``self-simulation'' in detail.
In the earlier outline, step~\ref{i:comp.trans} of Section~\ref{sec:computation-phase}
said to compute \( \tau^{*}(\tilde\va) \)
(for the present discussion, we will just consider computing 
\( \tau^{*}(\va)=\tau_{k+1}(\va) \)), where \( \tau=\tau_{k} \),
and it is assumed that \( \va \) is available on an appropriate auxiliary track.
We give more detail now of how to implement this step:

\begin{enumerate}
\item Onto the \( \Work \) track, write the string \( R \).
To do this, for \( \Index \) running from 1 to \( \abs{R} \), 
execute the instruction \( \WriteProgramBit \) and move right.
Now, on the \( \Work  \) track, replace it with \( \ang{\Interpr,0^{k+1},R,\va} \).
Here, string \( \Interpr \) is a constant, so it is just hardwired.
String \( R \) already has been made available.
String \( 0^{k+1} \) can be written since the parameter \( k \) is available.
String \( \va \) is available on the track where it is stored.
\begin{sloppypar}
 \item Simulate the universal automaton \( \Un \) on track \( \Work \):
   it computes \( \tau_{R,k+1}(\va)=\Un(\Interpr,R,0^{k+1}, \va) \)
as needed.  
\end{sloppypar}
\end{enumerate}

This implements the forced self-simulation.
Note what we achieved:

\begin{itemize}
  \begin{sloppypar}
\item On level 1, the transition function \( \tau_{R,1}(\va) \) is defined completely
when the rule string \( R \) is given.
It has the forced simulation property by definition, and
string \( R \) is \emph{``hard-wired''} into it in the following way.
If \( (\va',d)=\tau_{R,1}(\va) \), then
\begin{align*}
  a'_{0}.\Work=R(a_{0}.\Index)
\end{align*}
whenever \( a_{0}.\Index \) represents a number between 1 and \( \abs{R} \),
and the values \( a_{0}.\Sweep \), \( a_{0}.\Addr \) satisfy the conditions
under which the instruction \( \WriteProgramBit \) is 
called in the rules (written in \( R \)).
      \end{sloppypar}

      \begin{sloppypar}
\item The forced simulation property of the \emph{simulated}
transition function \( \tau_{R,k+1}(\cdot) \) is 
achieved by the above defined computation 
step---which \emph{relies on} the forced simulation property of \( \tau_{R,k}(\cdot) \).
              \end{sloppypar}
\end{itemize}

\begin{remark}
  This construction resembles the proof of Kleene's fixed-point theorem, and even more
  some self-reproducing programs (like a program \( p \) in the language C causing the computer
  to write out the string \( p \)).
\end{remark}

\subsection{Transfer phase}\label{sec:TransferPhase}

In the transfer phase the \( \Transfer \) rule takes over: control will be transferred to the
neighbor colony in the direction of the simulated head movement: this is
called the direction of the transfer, or the \df{drift}.
As we will see the transfer phase takes only a constant number of sweeps:
three pairs of sweeps should be sufficient (see the details below).
During this phase, the range of the head
includes the base colony pair and a neighbor colony
in the direction of the drift called \df{target colony}, including possible bridges between them.
The bridges present some extra complication---let us address it.

\begin{sloppypar}
The sweep number in which we start transferring in direction \( \delta \) is called
\( \TransferSw(\delta) \), the \df{transfer sweep}.
We set \( \TransferSw(-1) =\TransferSw(1)+1 \).  
\end{sloppypar}

\begin{definition}[Adjacency of cells]\label{def:adjacent}
  Cells \( a \) and \( b \) are \df{adjacent} if \( \abs{a-b}=\B \).
  Otherwise, if \( \B < \abs{a- b} < 2\B \), then
  \( a \) and \( b \) are two \df{non-adjacent neighbor cells}.
For the sake of the present discussion, a \df{colony} is a sequence of \( \Q \) adjacent
cells whose \( \Addr \) value runs from \( 0 \) to \( \Q-1 \).
It may be \df{extended} by a bridge of up to \( \Q-1 \) adjacent cells.

If the bodies of two cells are not adjacent, but are at a distance \( <\B \) then the space
between them is called a \df{small gap}.
We also call a small gap such a space between the bodies of two colonies.
On the other hand, if the distance of the bodies of two colonies is \( >\B \) 
but \( <\Q\B \) then the space between them is called a \df{large gap}.
\end{definition}

Before the transfer phase, the base colony pair consists of two colonies,
having cells of kind \( \Member_{0} \) and \( \Member_{1} \).
There may be a bridge between them, of \( <\Q \) cells of kind \( \Bridge \)
adjacent to each other, and extending one of the base
colonies (see Definition~\ref{def:extends} and the remark following it).

The transfer phase moves the base colony pair left if \( \Drift=-1 \), and right, if \( \Drift=1 \)
in the left base colony.
Consider \( \Drift=-1 \) first.
The kind of cells of the left base colony will turn from \( \Member_{0} \) to \( \Member_{1} \).
Then a bridge is built to the left (extending the \( -1 \) value on the \( \Drift \) track).
This bridge can override an opposite old bridge (``old'' meaning 
that its \( \Sweep \) is maximal) or move 
into an empty area, kill other bridge cells or stem cells if they are not adjacent, while it extends.
There are two ways that this bridge extension can finish.
\begin{itemize}
\item It stops at the boundary of another colony on the left
  (which consists of cells of type \( \Member_{0} \)).
  (This can happen in the very first step, in which case no bridge is built.)
  Then this colony becomes the new left base colony, and the head moves to its left end
  (extending again the \( -1 \) value on the \( \Drift \)  track).
  
\item The bridge reaches the size of \( \Q \) cells.
  In this case, the bridge will be converted into the new left base colony, the kind of its cells
  becoming \( \Member_{0} \).
  \end{itemize}
Recall that the \( \Adj \) field determines
whether the current cell is adjacent to the cell  where the head came from.
After the transfer stage, we write the \( \Adj' \) track of the
new left base colony: it becomes \( 0 \) if either there is a nonempty bridge,
or there is a gap (found with the help of the \( \Adj \) field) between the old and new left base colony.
This is done again three times, storing candidate values into \( \Hold[j].\Adj' \),
repeating and doing a majority vote.

\begin{sloppypar}
In the last sweep, 
in the old base colony pair, if the majority of \( \cHold[j].\Doomed \), \( j=1,\dots,3 \), 
is 1 then turn the scanned cell into a stem cell: 
in other words, carry out the destruction.
\end{sloppypar}

The case \( \Drift=1 \) is analogous, with the following logical changes.
First the bridge between the base colonies is possibly rebuilt, to extend the left base colony.
Then the kind of cells of the right base colony will turn from \( \Member_{1} \)
to \( \Member_{0} \), and it becomes the new left base colony.
Then a bridge is built to its right (extending the \( 1 \) value on the \( \Drift \) track).
It stops at the boundary of another colony on the right
(which consists of cells of type \( \Member_{1} \)), or becomes itself a colony with elements having
kind \( \Member_{1} \).

\section{Health}            \label{sec:health}

The main part of the simulation uses an error-correcting
code to protect information stored on the \( \Info \) track.
However, faults can ruin the simulation structure and disrupt the simulation itself.
The error-correcting capabilities of the code 
will preserve the content of the \( \Info \) track as long as the coding-decoding
process implemented in the simulation is carried out.
The correctness of the process itself relies only on
the structural integrity of a configuration; this in turn is maintained with the help of a small number
of fields.
Below we outline the necessary relations among them 
allowing the identification and correction of local damage.

A configuration with local structural integrity will be called \df{healthy}.
\begin{sloppypar}
\begin{remark}
In all discussions of the health of a configuration \( \xi=(A,\h,\vhc) \), we can ignore
the current cell position \( \hc_{0}=\xi.\curcell \), since at all switching times 
it agrees with \( \h=\xi.\pos \).
\end{remark}
\end{sloppypar}

All configurations are made up of homogenous domains (interval).

\begin{definition}[Homogenous domain]\label{def:domains}
The tuple of fields
\begin{align*}
   \Core =(\Kind,\Adj',\Drift,\Addr,\Sweep,\ZigAddr)
 \end{align*}
is called the \df{core}.   
 An interval of non-stem adjacent neighbor cells is a \df{homogenous domain} if
 its core variables with the exception of \( \Addr \) and \( \ZigAddr \) have the same value,
 \( \Addr \) increases left to right, and \( \ZigAddr \) values are either
 all undefined or are increasing left to right.
The \df{left end} of a domain is the left edge of its first cell, and its \df{right end} is 
the right edge of its last cell.
\end{definition}

A key property of our definition of health is that it is completely determined by the
types of domains allowed and the types of boundaries allowed between them.
This enables the local detection of failures of health.
The big picture is that cells of a healthy configuration are grouped into gapless colonies, with
certain transitional intervals inside and between them.
\begin{itemize}
\item There is a base colony-pair, with possibly a bridge between them going in the direction
  of its own drift.
  If the sweep inside this colony-pair shows that the \( \Comp \) rule is being performed then
  the sweep boundaries allowed inside the colony-pair are the front, and the edges
  according to past turns at the end of the interior (as in Definition~\ref{def:interior})
  of the base colony-pair.
  Outside each edge is a domain with a lower sweep number.

\item  During the first sweep of transfer, the base colony (of the pair)
  in the direction of the drift is possibly extended by a bridge towards the target colony.
  This bridge may be in the process of consuming an earlier bridge extended in the opposite direction.
  In the later part of the first sweep of transfer, the bridge already reaches
  the target colony.

  If the bridge extends to a full colony then this is converted to the corresponding kinds of member
  cells in the second sweep of the transer.

  In the later sweeps of the transfer, domains ahead and behind the front show the changes done
  by the transfer phase, including the change of \( \Adj' \) in the last sweep.
  
\item Non-base colonies are called \df{outer colonies}.
If an outer colony is not adjacent to its neighbor colony closer to the base,
then it is extended by a bridge that covers this gap.

\end{itemize}
\Pnote{Pictures!}

Let us define outer cells and boundary types in more detail.

\begin{definition}[Outer cells, desert]\label{def:outer-cells}
    Recall the definition of the sweep value  \(  \Last(\delta)  \)  from~\eqref{eq:Last}.
    For \( \delta \in \{ -1,1 \} \), if a cell is stem or has
\( \Drift = \delta \),  \( \Sweep = \Last(\delta) \)
    then it will be called a \df{right outer cell} if
    \( \delta = -1 \), a \df{left outer cell} if \( \delta = 1 \).
\end{definition}

According to this definition, a stem cell is both a left and a right outer cell.
Recall Definition~\ref{def:front} of the front.


\begin{definition}[Boundaries]%\label{def:domains}
% \begin{description}
% \item[Outer domain]
% Outer cells that could belong to the same extended colony.

% \item[Workspace domain]  
%   Part of an extended base colony, or part of a former outer
%   colony that is in the process of being transferred to.
%   The latter is called a \df{target domain}, its colony the \df{target colony}.
%   The cells of the target domain are not outer cells since the their sweep shows the start of transfer.
%   It is not extended by a bridge.
% \end{description}
A boundary of a homogenous domain is called \df{rigid} if its address is the end 
address of a colony in the same direction.
A \df{boundary pair} 
is a right boundary followed by a left boundary at distance \( <\B \).
It is a \df{hole} if the distance is positive.
It is \df{rigid} if at least one of its elements is.
\end{definition}

The health of a configuration \( \xi \) of a generalized Turing machine 
\( M \) is defined over a certain interval \( A \).
It depends on \( \xi_{\tape}(A) \), further on whether \( \xi_{\pos} \) is in \( A \) and
if it is, where.
But we will mention the interval \( A \) explicitly only where it is necessary.
In particular, some of the structures described above may fall partly or fully
outside \( A \).

Defining health formally can be done mechanically on the basis of the above informal description,
but the detailed description would be tedious.

% The definition below adds more detail to the above informal description of a healthy configurations.

% \begin{definition}[Healthy configuration]\label{def:healthy}
% We require that the mode be normal, and the following conditions hold, with \( \delta=\Drift \).

% \begin{description}
% \item[Segments]
%   All non-stem cells belong to full extended colonies.
%   In more detail:
%   \begin{itemize}
%   \item An \df{extended left/right base colony} consisting of cells of kind
%     \( \Member_{0}/\Member_{1} \), possibly extended by bridges.
%     In the first transfer sweep, cells of type \( \Member_{0} \) may be in the process of turning
%     into \( \Member_{1} \) or vice versa, according to the drift.
%   \item \df{Extended outer colonies}.
%   \item Desert filling out the gaps between the above parts.
%   \end{itemize}
%   The following kinds of domain can occur.
%   \begin{itemize}    
%   \item If \( \Sweep = \TransferSw(\delta) \) then the base colony pair is being moved left or right
%     according to \( \delta \).
%     This involves a possibly a bridge to be extended, and also the change of \( \Member_{j} \) to
%     \( \Member_{1-j} \) as the transfer requires.
%   \item If \( \Sweep = \TransferSw(\delta) +1 \) and the transfer did not take place yet,
%     then the whole bridge is being converted into a new base colony
%     as the head is traveling in direction \( -\delta \).
%   \item In the last sweep, the new member cells obtain the \( \Adj' \) values are changed as needed.
%   \end{itemize}
%   These are the only possible domains to be seen in a healthy area.
        
%     \item [The front] 
%       The farthest position \( \front(\xi) \) to which the head has 
%       advanced before starting a new zig is called the \df{front}:
%       it can be computed from the fields \( \xi.\pos \) and \( \Depth \) of the current cell, 
%       but can also be reconstructed from the tape, namely from the \( \Sweep \) track.
%       It is always inside the extended base colony or the target.

%      % \item[Workspace]

%      %   \begin{sloppypar}
%      %      The \df{workspace} is an interval of non-outer cells, such that:         
%      %    \end{sloppypar}
        
%      %    \begin{itemize}
        
%      %    \item For \( \Sweep < \TransferSw(\delta) \), it is equal to the base colony.

%      %    \item In case of \( \Sweep = \TransferSw(\delta) \), it is the smallest interval including
%      %          the base colony and the cell neighboring to \( \front(\xi) \) on the side of the base colony.

%      %    \item If \( \TransferSw(\delta) < \Sweep < \Last(\delta) \),
%      %          then it is equal to the union of the extended base colony and the target.

%      %    \item When \( \Sweep = \Last(\delta) \), it is the smallest interval including the target
%      %          (future  base) colony and \( \front(\xi) \).
%      %    \end{itemize}

%    \end{description}
 % \end{definition}

A tape configuration is called \df{healthy} on an interval \( A \)
when there is a head position (possibly outside \( A \) ) that turns it into a healthy configuration.

Health only depends on the \( \Core \) track
and the lack of heal or rebuild marks on the tape.
Note that in a healthy configuration every cell's \( \Core \) field
determines the direction in which the front is found, from the point of
view of the cell.

A violation of the health requirements can sometimes be noted immediately:

\begin{definition}[Coordination] \label{def:coordinated}
   The current cell pair is \df{coordinated} 
   if it is possible for them to be together in a healthy configuration.
\end{definition}

% Recall that in Rule~\ref{alg:main1},
In normal mode, if lack of coordination is discovered then the healing procedure is called.
The following lemmas show 
how local consistency checking will help achieve longer-range consistency.

\begin{lemma}\label{lem:coordination1}
  In a healthy configuration, each \( \Core \) 
  value along with \( \Z \) determines uniquely the 
  \( \Core \) value of the other cell of the current cell pair, with the following exceptions.
  \begin{itemize}
  \item During the first transferring sweep, while
    creating a bridge, the front can be a stem cell or the first cell of an outer colony.
  \item Every jump backward from a target colony can land on the last cell of a 
    bridge (whose address is not recorded in the cell from which the head is jumping)
    or the last cell of the base colony.
  \end{itemize}
  \end{lemma}
  \begin{proof}
  To compute the values in question, use \( \Z \) to find the address at the front, and
  refer to the properties listed above.
\Pnote{Elaborate!}    
  \end{proof}

\begin{lemma}[Health extension]\label{lem:health-extension}
  Let \( \xi \) be a \emph{tape} configuration that is healthy on intervals \( A_{1}, A_{2} \) 
where \( A_{1}\cap A_{2} \) contains a whole cell body of \( \xi \).
Then \( \xi \) is also healthy on \( A_{1}\cup A_{2} \).
\end{lemma}
\begin{proof}
  The statement follows easily from the definitions.
\Pnote{Elaborate!}    
\end{proof}

In a healthy configuration, the possibilities of finding non-adjacent neighbor
cells are limited.

\begin{lemma}\label{lem:two-domains}
  An interval of size \( <\Q \) over which the configuration \( \xi \) is healthy
contains at most two maximal sequences of adjacent non-stem neighbor cells.
\end{lemma}
\begin{proof}
Indeed, by definition a healthy configuration consists of full extended colonies, with 
possibly stem cells between them.
An interval of size \( <\Q \) contains sequences of adjacent cells 
from at most two such extended colonies.
\end{proof}

% Let us classify the boundary pairs possible in a healthy configuration.
% Rigid pairs:

% \begin{enumerate}[label=\upshape{(r\arabic*)}, ref=r\arabic*]
% \item\label{i:rigid.outer-workspace}
%   Between an outer extended colony or a desert, and a colony closer to the base.
% \item\label{i:rigid.bridge-target} Between the extended base colony 
% and the target or an outer colony.
% \end{enumerate}

% Non-rigid pairs are at the front:
% \begin{enumerate}[label=(nr\arabic*), ref=nr\arabic*]

%   \item\label{i:nr.bridge-bridge} End of a new bridge in direction \( \delta \)
% not within distance \( <\B \) of a rigid boundary of a colony of drift \( -\delta \) or its own target.
% On the other side is either desert or the end of an old bridge.

%   \item\label{i:nr.aligned} Between aligned domains:
%     \begin{enumerate}[label=(\arabic*), ref=nr\arabic{enumi}.\arabic*]
%      \item\label{i:nr.aligned.differ-1} Between sweep values differing by 1,
%      \item\label{i:nr.aligned.bridge-target} between a new bridge and the target it is being converted into,
%      \item\label{i:nr.aligned.target-member} 
% between a target in direction \( \delta \) and the remaining domain of member cells, of drift \( -\delta \),
%      \item\label{i:nr.aligned.member-target}  between a target and the member cells replacing it in the last sweep,
%      \item\label{i:nr.aligned.internal-end} 
% between an internal domain and an end domain (see Definition~\ref{def:domains}).
%     \end{enumerate}

% \end{enumerate}

% The following is worth noting.

% \begin{lemma}
% In a healthy configuration, any interval of size \( <\Q\B \) contains at most 3 boundary pairs,
% only one of which can be  a hole.
% \end{lemma}
% \begin{proof}
% Any interval of size \( <\Q\B \) contains at most one rigid left boundary and
% one rigid right boundary.
% Any nonrigid boundary coincides with the front.
% A hole can exist only at the end of a bridge.
% If there are two bridges, then the hole can only be between their ends.
% \end{proof}

\begin{lemma}\label{lem:infer-between}
In a healthy configuration, 
a cell's state shows whether it is an outer cell, colony cell, bridge cell or target colony cell.
It also shows whether the cell is to the left or to the right of the front.

The \( \Core \) track of a homogenous domain can be reconstructed from any of its cells.
\end{lemma}
\begin{proof}
The information mentioned in the first sentence is explicit in some fields.
About the front: if the cell is outer then \( \Drift \) shows its direction from the front.
In case it is not outer, then the parity of \( \Sweep \) shows it: odd values are on the left, even 
ones on the right. 

The reconstruction is a direct consequence of the definitions.
\end{proof}

\section{Healing and rebuilding}\label{sec:healing}

\subsection{Annotation, admissibility, stitching}\label{sec:stitching}

In this section we show how to correct configurations of machine \( M \)
that are ``almost'' healthy.

\begin{definition}[Annotation]\label{def:annotation}
  An \df{annotated configuration} is a tuple
  \begin{align*}
    (\xi,\chi, \cI),
  \end{align*}
  with the following meaning.

  \( \xi \) is a configuration.

  \( \cI \) is a set of disjoint intervals called \df{islands}.
Their complement is clean.

  \( \chi \) is a healthy configuration differing from \( \xi \) only in the islands.

The \df{base colony} of \( (\xi,\chi, \cI) \) is that of \( \chi \).
The head is \df{free} when it is not in any island,
and the observed cell is in normal mode, coordinated with its pair.
  \end{definition}

\begin{definition}[Admissibility]\label{def:admissible}
An annotation is \df{admissible} on an interval \( K \) if the following holds on any subinterval 
\( J \) of \( K \) of size \( \Q\B \):
\begin{varenum}[series=admissible]{a}
\item The disorder of \( J \) is covered by 3 intervals, each of size \( \le \beta\B \).
\item There are at most 3 islands in \( J \), each of size at most \( \cns{island}\beta\B \).
\end{varenum}
A configuration is \df{admissible} on \( K \) if it can be annotated in a way admissible on \( K \).
\end{definition}

Let us argue (informally, without replacing any later, formal argument), that local correction
does not have to deal with more than three islands in any area of size \( \Q\B \). 

\begin{example}[Three islands]\label{xmp:3-islands}
  Suppose that the head has arrived at a colony \( C \) from the left,
performs a work period and then passes to the right.
In this case, if no new noise burst occurs then we expect that
all islands found in \( C \) will be eliminated by the healing procedure.
On the other hand, a new island \( I_{1} \) can be deposited (in the last pass).
We can assume that there is no island on the left of  \( I_{1} \) 
within distance \( \Q\B \), since the noise burst
causing it would have been too close to the noise burst causing \( I_{1} \).

Consider the next time (possibly much later), when the head arrives (from the right).
If it later continues to the left, then the situation is similar to the above.
Island \( I_{1} \) will be eliminated, but a new one may be deposited.
But what if the head turns back right at the end of the work period?
If \( I_{1} \) is close to the left end of \( C \), then due to the feathering
construction, the head may never reach it to eliminate it; moreover,
it may add a new island \( I_{2} \) on the right of \( I_{1} \).
We can also assume, similarly to the above, 
that there is no island on the right of  \( I_{2} \) within distance \( \Q\B \).
(In the work period where the head deposits island \( I_{2} \) near the left colony
end, it may repeatedly dip into \( I_{2} \) at the descending end of a zig at a
right turn.
During this dip it can expand the islands
\( I_{1}, I_{2} \) and then emerge on the right, with the healing unfinished.)

When the head returns a third time (possibly much later), 
from the right, the feathering requiement implies that it will leave on the left.
The islands \( I_{1},I_{2} \) will be eliminated as they are passed over
but a possible new island
\( I_{3} \), created by a new burst (before, after or during the elimination), may remain.
We can also assume, similarly to the above, 
that there is no island on the right of  \( I_{3} \) within distance \( \Q\B \).
So at least temporarily, it is possible to have three islands in \( C \).
 \end{example}

We will show that a configuration admissible over an interval of a certain size can be locally corrected;
moreover, in case the configuration is clean (no disorder), this correction
can be carried out by the machine \( M \) itself.
For the time being, we will just talk about an admissible configuration, without specifying the interval
\( K \).

\begin{definition}[Substantial domains]\label{def:substantial}
Let \( \xi(A) \) be a tape configuration over an interval \( A \).
A homogenous domain of size at least \( 7\cns{island}\beta\B \)
will be called \df{substantial}.
The area between two neighboring maximal
substantial domains or between an end of \( A \) and the closest substantial domain in \( A \)
will be called \df{ambiguous}.
It is \df{terminal} if it contains an end of \( A \).
  Let
 \begin{align*}
     \Delta &= 39\cns{island}\beta, % ,\quad \Delta'=\Delta+3\cns{island}\beta. %?
\\  \E  &\ge 6\Delta. %?
 \end{align*}
\end{definition}

\begin{lemma}\label{lem:ambiguous}
Consider an admissible configuration.
In a substantial domain, each half contains at least one cell outside the islands.

If an interval of size \( \le  \Q\B \) of a tape configuration \( \xi \) differs from a  healthy tape 
configuration \( \chi \) in at most three islands, then 
the size of each ambiguous area is at most \( \Delta\B \).
\end{lemma}
\begin{proof}
The first statement is immediate from the definition of substantial domains.
There are at most 3 boundary pairs in \( \chi \) at a total size of \( 3\B \), and
3 islands of size \( \le\cns{island}\beta\B \).
There are at most 5 non-substantial domains of sizes \( < 7\cns{island}\beta\B \)
between these: this adds up to
\begin{align*}
 < (3+3\cdot\cns{island}\beta+ 5\cdot 7\cdot\cns{island}\beta)\B<39\cdot\cns{island}\beta\B=\Delta\B.
 \end{align*}
\end{proof}

The following lemma forms the basis of the healing algorithm.

\begin{lemma}[Stitching]\label{lem:stitching}
In an admissible configuration, inside a clean interval,
let \( U,W \) be two substantial domains separated by an ambiguous area \( V \).
It is possible to change the tape on \( U,V,W \) using only information in \( U,W \) in such a 
way that the tape configuration over \( U\cup V\cup W \) becomes healthy.
Moreover, it is possible for a cellular automaton to do so gradually, keeping admissibility, and
changing the tape in \( U \) or \( W \) or enlarging \( U \) or \( W \)
gradually at the expense of \( V \).

The machine in question can make its turns via
``small turns'' as defined in Section~\ref{sec:sweep}.
\end{lemma}
\begin{Proof}
We distinguish several cases, based on the kind of domains involved.
At any step, if we find that \( U\cup V\cup W \) is healthy then we stop.
\begin{step+}{step:stitching.mergeable}
Assume that \( U,W \) both belong to the same extended colony: either an extended base colony, or 
an outer extended colony, or the target.
\end{step+}
\begin{prooof}
  % If both belong to the interior (as in Definition~\ref{def:interior})
  % of the area indicated, then the merging is simple.
In this case, by Lemma~\ref{lem:infer-between}, the content of the \( \Core \) track 
of \( V \) in the healthy configuration is completely determined by that of \( U,W \),
So the intervals \( U \) and \( W \), which will be recognized as the substantial ones as
opposed to \( V \), can be gradually extended towards each other in any order, 
overtaking \( V \).

% Suppose that they do not belong to the interior and are, for example, towards the right from it.
% In this case, the \( \Sweep \) value may be decreasing in both \( U \) and \( W \) to the right.

% Peter: \( \Sweep \) is now constant within a domain.
% The big turns happen far enough from each other that one can require this.

% But since \( U,W \) themselves may overlap with islands, it may happen that the \( \Sweep \) value
% on the right end of \( U \) is smaller than the \( \Sweep \) value on the left end of \( W \).
% In this case, before extending \( U,W \) towards each other, the \( \Sweep \) values in both
% may need to be changed.

% By Lemma~\ref{lem:ambiguous}, the left and right halves of \( U \) contains a cell \( u_{1},u_{3} \) 
% each, not belonging to any island, and similarly with \( w_{1},w_{3} \) in \( W \).
% Let \( u_{2} \) be leftmost cell of the right half of \( U \) and \( w_{2} \) the rightmost cell of the left 
% half of \( W \).
% Then
% \begin{align*}
%  \Sweep(u_{1})\ge \Sweep(u_{2})\ge \Sweep(u_{3})\ge \Sweep(w_{1})\ge \Sweep(w_{2})\ge \Sweep(w_{3}).   
%  \end{align*}
% So the transformation will change \( \Sweep \) to \( \Sweep(u_{2}) \) everywhere in the right half of \( U \),
% and to \( \Sweep(w_{2}) \) everywhere in the left half of \( W \).
% After this, it will extend \( U \) towards \( W \), overtaking \( V \) and keeping \( \Sweep \) constant.

If \( U  \) and \( W \) belonged to different sides of the front, then the
new front will be the new boundary between \( U \) and \( W \).
\end{prooof} % step:stitching.mergeable

\begin{step+}{step:stitching.not-front}
Suppose now that \( U,W \) do not belong to the same extended colony, but they are on 
the same side of the front: without loss of generality, to the left of it.
\end{step+}
\begin{prooof}
In this case, only \( U \) can be outer or a target: suppose that it is outer.
Now \( W \) is either in a target or in the base colony.
The base colony cannot have an extension to the left, because given that the front
is to the right, the same sweep that created the extension would have created already a target, 
separating \( U \) (which being outer is not in the target) from \( W \) too much.
So in both cases, \( W \) is close to its colony's left end, which is in \( V \).
It must be extended to this left end.
Following this, \( U \) must be extended until its reaches \( W \).
The \( \Sweep \) values can be propagated without change, there is no need to coordinate them
between \( U \) and \( W \).

Suppose that \( U \) is a target, then \( W \) belongs to the extended base colony.
Then \( U \) must be extended until its endcell, and then \( W \) must be extended to meet \( U \).
Since both \( U \) and \( W \) are in the interior, the \( \Sweep \) values must be equal, so they
can be extended without change.
\end{prooof} % step:stitching.not-front

\begin{step+}{step:stitching.front}
Suppose that \( U \) and \( W \) belong to different extended colonies, with the front between them.
\end{step+}
\begin{prooof}
The \( \Sweep \) values can be extended without change, there is no need to coordinate them
between \( U \) and \( W \).

Suppose that \( U \) contains no bridge cells: in this case extend it to the right until it hits
a colony endcell.
If you overlap \( W \), decrease \( W \) accordingly.
Due to admissibility, only bridge cells of \( W \) can be overwritten in this way
(colonies don't overlap in a healthy configuration).
Similarly, if \( W \) contains no bridge cells then extend it to the left, until it hits a colony endcell.
Again, only bridge cells of \( U \) can be overwritten in this way.

Only one of \( U \) and \( W \) can be in an outer extended colony, suppose that \( U \) is.
If \( W \) is a target then we already extended it to its limit.
Now extend the bridge of \( U \) to meet it.
If \( W \) is in an extended base colony then extend its bridge until either meets \( U \) or
reaches full length.
In the latter case, extend a bridge of \( U \) to meet \( W \).

If \( U \) is in a target then it is already at its right end: we extend a bridge of \( W \) to meet it.
\end{prooof} % step:stitching.front
\end{Proof}


\subsection{The healing procedure}\label{sec:healing-proc}

Structure repair will be consist of two procedures.
The first one, called \df{healing}, performs
only local repairs of the structure: for a given (locally) admissible configuration,
it will attempt to compute a satisfying (locally) healthy configuration.
If it fails---having encountered a configuration that is not admissible, or
a new burst---then the \df{rebuilding} procedure is called, which is designed
to repair a larger interval.
On a higher level of simulation, 
this corresponds to the implementation of the ``cleaning'' trajectory properties.
The healing procedure runs in \( O(\beta) \) 
steps, whereas rebuilding needs \( O(\Q^{2}) \) steps. \Pnote{maybe even \( \Q^{3} \)?}

The description of the procedures will look as if we assumed that there is no noise or disorder.
The rules described here, however (as will be proved later), will clean an area locally under the 
appropriate conditions, and will also work under appropriately moderate noise.

The healing procedure does not even protect itself against any possible noise during it.
The only protection is that any one call of the healing procedure will change only 
a small part of the tape, essentially one cell: so a noise burst will have limited impact even if it
happens during healing.
One possible outcome of the healing procedure is \df{failure}: this happens when it encounters
an  unstitchable situation.
Failed healing creates a \df{germ} of cells marked for rebuilding.
The size of the gem is larger than \( 3\beta\B \), to make sure that it could not have
been created by a burst.
Proceeding conservatively,
the healing procedure carries  out only one step of this plan, then it calls itself again.
Even at failure, it will only add one cell at a time to the germ.

Where to put the germ?
If no substantial domains are found then the germ is started in the middle of the healing area,
and later steps (if still no substantial domains are found) add to it.
If a substantial domain is found then the germ is started in the middle of one.
This way, the next attempts will not try to eliminate it by stitching, and will add to it on failure.
If healing succeeds then it ends with eliminating any possible started germ.

Recall the parameters \( \Delta,\E \) introduced in Definition~\ref{def:substantial}.
Suppose that \( \rHeal \) is called at some position \( \z \).
Then it sets
\begin{align*}
\Mode\gets\Healing,\; \Heal.\Sweep \gets 1,\; \Heal.\Addr \gets 0.
 \end{align*} 
It starts by surveying an interval \( \R \) of at least \( 2\E \) cells 
around its starting point \( z \).
(It will go a little farther than \( \E \) cells, in search of an allowed turning point, due to feathering.
If such a point is not found within \( 3\beta \) steps, healing fails.)
Whenever the head steps on a stem cell or creates a new cell, 
\( \Drift \) is set to point to \( \z \) (to make sure that the head does not get
lost in a homogenous domain of stem cells).

Suppose that in the survey a pair of substantial domains separated by an ambiguous area is found,
that is at least \( \Delta \) removed from the complement of \( R \). 
Choose the ambigous area closest to the center (of healing).
Then the first needed ``stitching'' operation (a single step) as defined in Lemma~\ref{lem:stitching}
is determined, and is performed.
If the ambigous area still remains, go to its cell closest to \( z \) and restart healing.
 In case that we started from a clean admissible configuration, this operation creates a new admissible
configuration that is one step closer to being healthy.

If the required stitching operation is not found then
the surveyed area is found inadmissible, and healing fails.
Otherwise, if the head is at the front, healing is declared completed.
Otherwise, the healing center is moved one step closer to the front, and healing is restarted.

The healing operation defined this way has the following property.

\begin{lemma}\label{lem:combined-heals}
  Assume that the head moves in a noise-free and clean space-time rectangle
  \( \lint{a}{b}\times \lint{u}{v} \), with \( b-a>3\E\B \), \( v-u>2\E\Tu \),
touching every cell of \( \lint{a}{b} \) at least once, and never in rebuilding mode.
Then at time \( v \), the area \( \lint{a+1.5\E\B}{b-1.5\E\B} \) is healthy.
\end{lemma}
\begin{proof}
  In healing, the head will not move away from an ambiguous area before eliminating it,
  creating a healthy interval
\( I \) containing the two substantial intervals that originally bordered it.
If the head leaves this interval in normal mode and the zigging does not start new healing then the
healthy interval covered by zigging can be added to \( I \) to form an even larger healthy interval,
and this continues until new healing starts.
One of the substantial intervals of the new healing will be inside \( I \), and when it is completed,
\( I \) will be extended further.
The head may later return into the healthy interval \( I \),
but it will then just continue the simulation without
affecting health while staying inside.  
\end{proof}

\subsection{Rebuilding}\label{sec:rebuilding}

If healing fails, it calls the rebuilding procedure.
This indicates that the colony structure is ruined in an interval of size larger than 
what can be handled by local healing.
Just as with healing, we will be speaking here only about a situation with no 
mention of disorder---but the final analysis will take disorder into account.

Since rebuilding makes changes on an interval of the length of several colonies,
it is important not only that it is invoked when it is needed, but that it is not
invoked otherwise!

Rebuilding starts by extending the germ to the left, in a zigging way,
expecting rebuild-marked cells on the backward zig.
Since the zig is larger than a burst, if rebuilding started from just a burst then this
will trigger healing, thus leaving the rebuilding mode.

The notion of front, of Definition~\ref{def:front} will be extended also to the rebuilding mode.
Here is an outline, for rebuilding that started from a cell \( z \) in the middle
of the germ.
The goal is that
\begin{itemize}
\item the process does not destroy any healthy colony more than
  \( 3\beta\B \) to the left of \( z \) or to the right of \( z \).
\item we end up with a new decodable area
  (see Definition~\ref{def:super-health}) extending at least one colony to the
  left and one colony to the right of \( z \).
\end{itemize}
The rebuilding operation uses its own addresses, but in a special way, to avoid being
confused by the occasional deletion or insertion of cells.
So it uses two address tracks: 
\( \Rebuild.\Addr_{-1} \) counts from the left end of the rebuilding area towards the head,
and \( \Rebuild.\Addr_{1} \) from the right end.
Recall the stitching operation from Section~\ref{sec:stitching}.
\begin{description}
 \item[Mark] Starting from the germ, extend a rebuilding area  over \( 3\Q \) cells to the left 
and \( 3\Q \) cells to the right from \( z \).
Mark the area using the tracks \( \Rebuild.\Addr_{j} \) for \( j=\pm 1 \),
and the track \( \Rebuild.\Sweep \).
False rebuilding started by a burst will be recognized since
zigging must see an at least germ-sized marked area.
In what follows, every step that changes the configuration must be accompanied by zigging, to check that
the rebuilding is indeed going on.

Rebuilding cannot assume anything about the content of the area encountered: in particular,
it may represent the traces of some other rebuilding.
Since a single burst (which cannot be completely excluded) could pass
control to this other rebuilding process, this possibility must be handled.
If the front of the marking process encounters the front of a marking process in the opposite direction, then
the marking process moving right gets precedence (see Remark~\ref{rem:rebuild-precedence} below),
and so the area being marked in the left direction gets added to the right-directed one.

There is only one way that the rebuilding process can fail in a clean area: \df{turn-starvation},
as defined in Section~\ref{sec:feathering}.

 \item[Survey and Create]
More details of this stage will be given below.
It looks for existing colonies (possibly needing minor repair) in the rebuilding area, and 
possibly creates some.
As a result, we will have one colony called \( C_{\Left} \) 
on the left of \( z \), one called \( C_{\Right} \) on the right of \( z \),
and possibly some colonies between them.
Make all newly created colonies represent stem cells.
Declare \( C_{\Left} \) the base colony, direct all the others
with drifts and bridges towards it.
(The creation of a bridge may result also in the creation of a new 
colony if the bridge becomes \( \Q \) cells long.)
% Survey and possibly change additional \( 0.5\Q \) cells on the left of \( C_{\Left} \) and \( 0.5\Q \) cells
% on the right of \( C_{\Right} \), making the whole interval healthy.
The interval covering \( C_{\Left} \) and \( C_{\Right} \)
will be called the \df{output interval} of rebuilding.

\item[Mop] Remove the rebuild marks, shrinking the rebuilding area onto the left end of \( C_{\Left} \).
\end{description}

\begin{remarks}\label{rem:rebuild-precedence} 
  \begin{enumerate}
%   \item Why give precedence to the right-directed marking process?
% The issue will come up in the proof of Lemma~\ref{lem:pass-clean}.
% There, we will deal with a clean interval \( J \), initially of size, say,
% \( 6\Z\B \), and the goal is to see that
% in the absence of new noise, after a constant number of passes, \( J \) will extend to the right until it reaches
% the end of a colony pair or of a rebuilding area.
% If marking is unstoppable in both directions then this may not happen: reaching the left end of  \( J \)
% a new rebuilding process may send the head back to the right end, from which a new rebuilding process can send it back
% to the left end, and so on.
% If marking to the right has precedence then a new left-directed marking started on the right end would not stop it.

\item Giving precedence to the right rebuilding has the drawback that one can design a initial configuration
  in which even in the absence of noise, level 1 structure will never arise even locally.
  Namely, we can fill the line with short intervals \( \dots,J_{-1},J_{0},J_{1},\dots \)
  each of which is the start (say of size \( 3\Z \)) of a
  right-directed marking process.
  Then the head, after moving left on \( J_{0} \), will be captured by the process on \( J_{-1} \), then
  later captured by the similar process on \( J_{-2} \), and so on.
  But we will not need to consider such pathological configurations.
  \end{enumerate}
  
\end{remarks}

\subsubsection*{Details of the Survey and Create stage}

\begin{varenum}{s}
\item\label{i:survey.left}
Search in the marked area on left of \( z \) for a colony \( C \),
or a set of cells that looks stitchable by up to 3 healings into a colony.
The first substantial domain should be at least \( 3\beta \) cells to the left of \( z \),
to make sure that \( C \) is \df{manifestly} to the left of \( z \).
If the search and the stitching attempt are successful, mark \( C \) as \( C_{\Left} \).

Repeat this search for a colony manifestly to the right of \( z \): if found, call it \( C_{\Right} \).

\item Suppose that only \( C_{\Left} \) is found, then create \( C_{\Right} \).
The other case is symmetrical.

\item Suppose that neither \( C_{\Left} \) nor \( C_{\Right} \) have been found.
Then search the whole area, starting from the left, for a colony.
If no such colony is found then create \( C_{\Left} \) and \( C_{\Right} \).

\item\label{i:survey.both} Suppose that both \( C_{\Left} \) and \( C_{\Right} \) 
have been determined (found or created).
Then search between them, from the left, for (stitcheable) colonies, one-by-one.

\item Suppose that only a colony has been found that 
is not manifestly to the left or right of \( z \); then either the left end is manifestly to the left or
the right end is manifestly to the right of \( z \).

Suppose that the left end it is manifestly to the left of \( z \), then call the colony \( C_{1} \).
Then create \( C_{\Left} \) on the left of  \( C_{1} \).
Now search for another one on its right (it is not manifestly on the right).
If not found, create \( C_{\Right} \), manifestly on the right of \( z \).
If found call it \( C_{2} \), and create \( C_{\Right} \) on its right.
The other case is symmetrical.

\end{varenum}

The steps above requiring the creation of colonies and bridges are destructive
(the stitching ones are not).
Therefore before a creation step, the whole survey preceding it is performed twice, 
marking the result in all rebuilding cells, on two different tracks.
The required actions (erasing cells in order to build new ones in their place)
are then only performed if the two survey results are identical---otherwise alarm is called
(followed probably by new rebuilding).

Marked cells from some interrupted rebuilding may remain even after the mop-up operation.
These may trigger new healing-rebuilding later.


\section{Scale-up}

\subsection{Super-health}

\begin{definition}[Super-health]\label{def:super-health}
  Let \( \xi  \) be a clean configuration on an interval \( I \).
  We say that \( \xi \) is \df{super-healthy} if both ends of \( I \) coincide with the end of a 
  colony, and in each colony in \( I \), whenever the head is not in the last sweep, the \( \Info \)
  track contains a valid codeword as defined in Section~\ref{sec:coding}.
  A tape configuration \( \xi \) is \df{super-healthy}
  if it can be extended to a super-healthy configuration.
\end{definition}

As indicated in Section~\ref{sec:model}, when dealing with the behavior
of machine \( M \) over some space-time rectangle, we will assume that the noise
over this rectangle is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse.
With Definition~\ref{def:Tu} of \( \Tus \) this means 
in simpler terms that at most one noise \df{burst} affecting an
area of size at most \( \beta\B \) can occur in any \( \gamma \) consecutive work periods.
In the present section, histories will always be assumed to have this property.

Some histories lend themselves to be viewed as a healthy development
that is disturbed only in some well-understood ways.
Annotation adds to such histories the information pointing out these disturbances.
The proof of the error-correcting behavior of machine \( M \)
(essentially the proof of the Transition Function
property of trajectories of Definition~\ref{def:traj} for the simulated machine \( M^{*} \))
will take the form of annotation under sparse noise.
An annotation, as per Defnition~\ref{def:annotation}, marks some ways in which the health 
of a tape configuration has been be affected.
Now we extend annotation in order to deal with damage not only to health but also to information.

\begin{definition}[Super-annotation]\label{def:super-annotation}
  A \df{super-annotated configuration} is a tuple
  \begin{align*}
    (\xi,\chi, \cI, \cS),
  \end{align*}
  with the following meaning.

\( (\xi,\chi,\cI) \) is an annotated configuration.

\( \cS \) is a set of disjoint intervals called \df{stains}.
All islands are contained in stains.

We can change \( \chi \) into a super-healthy configuration by changing it only in the stains.
\end{definition}

Recall Definition~\ref{def:admissible} of admissibility.
  \begin{definition}[Super-admissibility]\label{def:super-admissible}
A super-annotation is \df{super-admissible} on an interval \( K \) if it is admissible on \( K \),
further consider any interval \( J\subseteq K\) of size \( \le\Q\B \).
\begin{varenum}[resume=admissible]{a}
\item At most 3 stains intersect \( J \).
\item If \( J \) contains \( k \) stains, surrounded by some healthy area
(we already know \( k\le 3 \)), while the head is at a distance \( >2\B \) within the clean area,
then the total size of these stains is at most \( k\cdot\cns{stain}\B \), where
\begin{align}\label{eq:stain}
   \cns{stain} = (\f+2)\beta.
 \end{align}
\item\label{i:super-annotated.two-stains} If an outer colony in \( K \)
  intersects two stains then it simulates a cell state with \( \Turned=1 \).
\end{varenum}
\end{definition}

Requirement~\eqref{i:super-annotated.two-stains} is essentially motivated by
the consideration described in Example\ref{xmp:3-islands}.
So a second stain can remain at the end of a work period in which the simulated
machine makes a turn, setting \( \Turned\gets 1 \) in the simulated cell.
Requirement~\eqref{i:super-annotated.two-stains} says that in an annotated
configuration, this is the only way for two stains to remain in an outer colony.

A configuration may allow several possible super-annotations;
however, the valid codewords (referred to in the definition
of super-health) recoverable from it do not depend on the choice of the
annotation.

The following is an immediate consequence of the definition of rebuilding:
in a clean area and the absence of noise, rebuilding will succeed.

\begin{lemma}\label{lem:rebuild-health}
  Suppose that a rebuilding procedure starts on the boundary of a super-admissible subinterval
  \( J\subset I \) of a clean interval \( I \), and runs noiselessly to the finish.
  Denoting by \( K \) the output interval of rebuilding, then \( J\cup K \) will be super-admissible.
\end{lemma}

\subsection{Annotated history}

The following definitions extend the notion of annotation to histories,
in order to discuss the effects of moderate noise and their repair.


\begin{definition}[Distress and relief, safety]\label{def:distress}
Consider a sequence of annotated configurations over a certain time interval.
If the head is free (see Definition~\ref{def:annotation}), then
the time (and the configuration) will be called \df{distress-free}.
A space-time point that is not distress-free and is preceded by
a distress-free time will be called a \df{distress event}.
This can be of two kinds: the head steps onto an island, or a burst occurs
(creating an island and leaving the head in it).

% The direction of the last \( \Z \) non-turning moves of the front before the distress event
% will be called the \df{pre-distress sweeping direction}.

Consider a time interval \( K \) starting with a distress event and
ending with a distress-free configuration.
Let \( J \) be the interval of tape where the head passed during \( K \), then 
we will call \( J\times K \) a \df{relief event}, if the following holds.
% the free head making \( \Z \) non-turning moves.
% Their direction is called the \df{post-distress sweeping direction}.
% If it coincides with the pre-distress sweeping direction then we will say that
% \df{no turn} occurred.

\begin{alphenumIn}
\item Any new islands occurring in \( J \times K \) are due to some new burst.
\item The island that started the distress event disappears by the end of \( K \).
% \item Provided no turn occurred, the only possible island intersecting \( J \) 
% at the end of \( K \) belongs to some island caused by a burst during \( K \).
% (In other words, the \emph{old} islands will be eliminated in \( K \).
% This is always the case if the distress occurs in the interior of a colony.)
\end{alphenumIn}
\end{definition}

In a relief event, it is possible to leave behind some islands other than the one initiating the distress.


We will consider the annotation of histories over a limited space-time region, but will
not refer to this region explicitly every time.

\begin{definition}[Annotated history]\label{def:annotated-hist}
An \df{annotated history} of a generalized Turing machine
    \begin{align*}
        M= (\Sigma, \tau, \Adj,\B, \Tu, \passno)
     \end{align*}
is a sequence of super-annotated configurations such that
the sequence of underlying configurations is a trajectory, and 
every distress event is followed by a relief event \( J\times K \) with 
\( \abs{J}=O(\beta\B) \) and \( \abs{K}= O(\beta^{2}\Tu) \). %?
\end{definition}

In what follows we will show that for any trajectory \( (\eta, \Noise) \) of a
generalized Turing machine \( M \) on any space-time rectangle on which the
noise is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse, if at the
beginning the configuration was super-healthy then the history can be annotated.
In the rest of the section we always rely on the assumption of  this 
sparsity property of the noise.

The main part of the proof is about obtaining relief  after a distress event.
Unlike in~\cite{burstyTuring13}, now islands 
may have their cell structure damaged: may contain disorder.
However, since \( \eta \) is a trajectory, as we will see the islands will be cleaned out.
So, relief maybe said to happen in two stages: cleaning, and correcting the structure.
However, this division is only for the purposese of proof: the machine has no
``disorder-detector'', and the proof just relies on the cleanness-extending properties of a
trajectory introduced in Definition~\ref{def:traj}.


\subsection{The simulation codes}\label{sec:sim-codes}

Let us now define formally the codes \( \varphi_{*k},\Phi_{k}^{*} \) needed
for the simulation of history \( (\eta^{k+1},\Noise^{(k+1)}) \) by history \( (\eta^{k},\Noise^{(k)}) \).
Omitting the index \( k \) we will write \( \varphi_{*},\Phi^{*} \).
To compute the configuration encoding \( \varphi_{*} \) we proceed first as
done in Section~\ref{sec:hier-codes}, using the code \( \psi_{*} \) there,
and then initialize the sweep and drift and address fields appropriately.

The value \( \Noise^{(k+1)} \) is obtained by a residue operation
just as in Definition~\ref{def:sparsity} of sparsity.
It remains to define \( \eta^{*}=\eta^{(k+1)} \) when \( \eta=\eta^{k} \).
Parts of the history that are locally super-annotated will be called \df{clean}.
In the clean part,
if no colony has its starting point at \( x \) at time \( t \), set \( \eta^{*}(x,t)=\Vacant \).
Otherwise \( \eta^{*}(x,t) \) will be decoded from
the \( \Info \) track of this colony, at the beginning of its work period 
containing time \( t \).
More precisely:

\begin{definition}[Scale-up]\label{def:scaleup}
Let \( (\eta,\Noise) \) be a history of \( M \), where \( \Noise=\Noise^{(k)} \)
as in Definition~\ref{def:sparsity}.
We define \( (\eta^{*},\Noise^{*})=\Phi^{*}(\eta,\Noise) \) as follows.
Let \( \Noise^{*}=\Noise^{(k+1)} \).
Consider position \( x \) at time \( t \), and \( J=\rint{t-\Tus}{t} \).
If \( x \) is not contained in some interval \( I \) such that \( \eta \) is super-admissible over \( I\times J \)
then \( \eta^{*}(x,t)=\Bad^{*} \).
Assume now that it is contained, and let
\( \chi(\cdot,u) \) be some super-healthy history satisfying \( \eta \) over \( I\times J \).
If \( x \) is not the start of a colony in \( \chi \) then let \( \eta^{*}(x,t)=\Vacant \); assume now that it is.
Then let \( t'\in J \) be the starting time in \( \chi \) of the work period of \( C \)
containing \( t \), and let \( \eta^{*}(x,t) \) be the value decoded from \( \eta(C,t') \).
In more detail, as said at the end of Section~\ref{sec:coding}, we apply the decoding
\( \psi^{*} \) to the interior of the colony it to obtain \( \eta(x,t) \).
\end{definition}

This definition decodes super-admissible intervals and histories for \( \eta \) into super-healthy
intervals and histories of \( \eta^{*} \), while preserving the the
property~\eqref{i:config.sharp-ends} of cleanness in Definition~\ref{def:config}.

% Recall the definition of a clean hole in Definition~\ref{def:clean-hole}.

% \begin{lemma}\label{lem:rebuild-clean}
%   Let  \( I \) be clean interval  of size \( 5\Q\B \).
%   Consider a noise-free time interval \( J \) during which the head spends time \( \ge 5\Tu \) in \( I \)
%   or passes through \( I \).
%   Then during \( J \) at some time the head will be found in a hole intersecting \( I \) that is
%   clean for \( M^{*} \).
% \end{lemma}
% \begin{proof}
% Assume, without loss of generality, that the head passes \( I \) from left to right.
% In the absence of noise, and inside a clean area,
% the healing procedure can only fail if it starts a rebuilding process.
% And a rebuilding process never fails.

% If the head passes \( I \) in normal mode, then
% the simulation makes \( I \) clean for \( M^{*} \).
% It may encounter a colony that is only healthy, not super-healthy;
% however, then the simulation will turn it into a healthy one.
% If healing is invoked but no rebuilding then we can follow the proof of Lemma~\ref{lem:combined-heals}:
% the result of each successful healing is consistent with the the continuation of a simulation.

% If a rebuilding is invoked whose whole range falls within \( I \) then it succeeds.
% Then simulation continues, with possible healings thrown in, until new rebulding starts.
% Lemma~\ref{lem:rebuild-health} shows that rebuilding will only extend the super-admissible area.  
% \end{proof}
% \Pnote{Coordinate these two lemmas, check and elaborate.}

\section{Isolated bursts}\label{sec:1-level-noise}

Here, we will prove that the healing procedure indeed deals with isolated bursts.
Our goal is to show that it provides relief, as required in an
admissible annotated trajectory.
For the elimination of disorder created by bursts
we will rely on the Escape, Spill Bound and the Attack Cleaning
properties of a trajectory in Definition~\ref{def:traj}.

Isolated bursts don't create disorder larger than \( 3\beta \).
The head escapes a disorder interval \( I \) via the Escape property; while it is inside, the
spreading of this interval is limited by the Spill Bound property.
Every subsequent time when the head enters and exits \( I \) this gets decreased
via the Attack Cleaning property, so it disappears after \( O(\beta) \) such interactions---see
Lemma~\ref{lem:healing} below.

% \begin{lemma}[Local escape]
% Let \( G \) be an interval of size \( n\B \) where \( n<\Z \).
% Then in the absence of noise, the head will either escape \( G \) within time \( O(n\Tu) \),
% or it will be inside a clean hole at some point during this time, as per Definition~\ref{def:clean-hole}.
% \end{lemma}
% \begin{proof}
%   Let \( c = (\CAtt+2\CSpill) \), as used in the Dwell Cleaning property of Definition~\ref{def:traj}.
% Let us cover \( G \) by consecutive intervals of size \( c\B \) called \df{blocks}, let \( m \) be the
% number of these blocks.
% Assume that the head does not escape \( G \) within time \( m\cdot\CEsc\Tu \).
% Then there is a block \( K \) in which it spends cumulative time \( \CEsc\Tu \),
% and the Dwell Cleaning property of trajectories implies that at some point during this time, it
% will be inside a clean hole in \( K \).
% \end{proof}
 
In a clean configuration, whenever healing started with an alarm, the procedure
will be brought to its conclusion as long as no new burst occurs.
The trajectory properties, however, do not allow any conclusion about the
state of the cell to which the head emerges from disorder.
This complicates the reasoning, and may require several restarts of the healing procedure.
By design, the healing procedure can change the \( \Core \) track only in one cell.
The following lemma limits even this kind of possible damage:
it says that the head with the wrong information may increase some existing island, but will
not create any new one.

\begin{lemma}
In the absence of noise, no new island will arise.
\end{lemma}
\begin{proof}
The islands are defined only by the \( \Core \) track.
In normal mode, this track changes only at the front.
If this is not the real front, then we are already in or next to an island.

The healing procedure's change of \( \Core \) is part of a stitching operation.
Looking at the different cases of the proof of Lemma~\ref{lem:stitching}, we see that 
inside a healthy area, healing can only change the \( \Core \) track in two ways.

The first way is case~\ref{step:stitching.mergeable}, when the \( \Sweep \) values were changed
in a domain not belonging to the interior.
Such an operation can be applied to any healthy configuration without affecting health.

The second case is when the front is moved left or right.
This does not affect health either.
\end{proof}

The following lemma is central to the analysis under the condition that bursts are isolated.

\begin{lemma}[Healing]\label{lem:healing}
In the absence of noise in \( M^{*} \), the history can be super-annotated.
Also, the decoded history \( (\eta^{*},\Noise^{*}) \) satisfies the Transition Function property 
of trajectories (Definition~\ref{def:traj}).
\end{lemma}
\begin{proof}
The proof of super-annotation is by induction on time, extending the super-annotation into the future.
The extension is straightforward as long as no distress event is encountered.
The Transition Function property is observed, as no obstacle arises to the simulation.
Stains do not cause problems: the computation stage of the simulation cycle eliminates them
using the error-correcting code.

Consider now the occurrence of a distress event.
This can be due to either a burst or the encounter with an island.
In the latter case, before the relief a burst can still hit.
In the analysis below, then we will just cut our losses and restart, knowing that 
burst cannot hit again before relief.

At the time of the distress event, let us draw an interval \( I \) of size \( \Q\B \) centered 
around the head.
The head will not leave it before relief, so we will consider the changes of 
the configuration inside it.
Let \( S_{1},\dots,S_{m} \) be the list of substantial domains of \( I \), 
and \( A_{1},\dots,A_{m-1} \) the ambiguous domains between them.
Note that \( m=O(1) \).
Let \( R \) be the set of those \( i \) for which \( S_{i}\cup A_{i}\cup S_{i+1} \) is clean.
For \( i\in R \) let \( n_{i} \) be the number of 
stitching steps by the algorithm of Lemma~\ref{lem:stitching} needed to stitch them 
together.
Let \( N=\sum_{i\in R} n_{_{i}} \).
Whenever the head enters disorder, we will mark the edge where it entered as \df{attacked}.

We introduce a few variables for the proof.
\begin{itemize}
\item\( D= \) be the total size of disorder, divided by \( \B \).
\item\( E= \) the number of un-attacked edges of disorder where the head can exit without
  entering (because it is on the side of the disorder).
  This number never increases.
\item \( F= \) the number of un-attacked edges where the head cannot exit without entering (because
  it is away from the disorder).
\item \( P= \) be the number of steps that the front moves forward (including the ones after possibly
changing the meaning of forward at a regular turn).
\end{itemize}

The following kinds of event may occur:
\begin{varenum}{h}

\item\label{i:heal.N-decr} With a stitching done, \( N \) decreases by 1 while possibly 
moving the front backward.

\item\label{i:heal.enter-carpet} The head enters disorder, decreasing \( F \).

\item\label{i:heal.leave-carpet.E}
The head leaves disorder on a non-attacked edge, decreasing \( E \).

\item\label{i:heal.leave-carpet.F}
The head leaves disorder on an attacked edge, decreases the disorder by at least \( \B \) and
increases \( F \) by 1.

% \item\label{i:heal.hole}
% The head creates a clean hole, by the Dwell Cleaning property of Definition~\ref{def:traj}.
% This may increase \( E \) by 2, but also decreases the disorder by \( \Omega(\B) \).

\item\label{i:heal.clean}
A set \( A_{i} \) becomes clean: then \( i \) gets added to \( R \); 
this can happen only \( m \) times.

\item\label{i:heal.N-incr.healing}
\( N \) increases in the healing mode by \( 1 \).
This can only happen after the head exited disorder in healing mode (with the wrong information).
At the exit, either \( E \) or \( D \) had to decrease.

\item\label{i:heal.approach-front} 
The head approaches the front: if it does not reach there then it gets closer by  
 \( \Omega(\beta)\B \).

\item\label{i:heal.front-move} The front moves forward in normal mode
(possibly after making a regular turn, and thus changing the forward direction).
This may also increase \( N \) by 1 (and is the only way to do so), 
say by decreasing a domain \( S_{i} \).
But it is followed by zigging.
The zigging may hit disorder, leading to case~\eqref{i:heal.enter-carpet},
or find something wrong---and trigger new healing, which leads to some of the other events.
Otherwise the island around the front will be deleted, and the head becomes distress-free.
\end{varenum}

The above possibilities suggest a potential function
\begin{align*}
   N + c_{D}D + c_{E}E + c_{F}F + c_{m}m - c_{P}P
 \end{align*}
where \( c_{D},\dots \) are appropriate positive constants.
Let us look at what each possibility does to the potential.

Cases~(\ref{i:heal.N-decr}-\ref{i:heal.leave-carpet.E}) decrease the potential if \( c_{P}<1 \).
Case~(\ref{i:heal.leave-carpet.F}) decreases the potential if \( c_{D}>c_{F} \).
There are only \( O(1) \) cases of type~(\ref{i:heal.clean}), increasing \( N \) by a total of \( O(\beta) \).

In case~(\ref{i:heal.N-incr.healing}), if \( c_{D} \) and \( c_{E} \) are large enough,
the increase in \( N \) can be charged to a decrease in \( D \) or \( E \).

Case~(\ref{i:heal.approach-front}) will not change the potential but there are at most \( O(1) \)
consecutive such cases.
 
Consider case~(\ref{i:heal.front-move}).
If the zigging hits new disorder and \( c_{F}>1 \) then the potential decreases.
If it triggers new healing then this will lead to one of the other cases,
compensating for the increase of \( N \) if the constants (other than \( c_{P} \)) are large.

These considerations show that relief indeed follows distress in time \( O(\beta^{2}\Tu) \).
At that point a normal-mode step moving the front has been made, followed by zigging.
Therefore the island causing the distress must have been eliminated, allowing the 
simulation to continue.
The stain caused by the island remains, but as discussed after Definition~\ref{def:super-admissible},
the simulation guarantees that the
size and number of stains remains bounded as required by that definition.
The simulation also continues to satisfy the Transition Function property of trajectories.
\end{proof}

\section{Cleaning}\label{sec:cleaning}

This section will scale up the Spill Bound, Escape, Attack Cleaning
and Pass Cleaning properties of trajectories.

\subsection{Cleaning the current level}

We need some lemmas in preparation.

The proof of the following lemma does not use any properties of the simulation program.

\begin{lemma}\label{lem:pass-clean-0}
  Let \( P \) be a space-time path of the head that has no bursts, and makes
  at least \( \passno+\theta \) pairs of passes over an interval \( J_{1} \) of size \( \theta\B/2 \).
  Then there is a time during \( P \) when \( \Int(J_{1},\CPass\B) \) becomes clean.
\end{lemma}
Note that the assumption of the lemma do not bound the number of \( \theta\B \)-intrusions.
\begin{proof}  
  Assume that \( J_{1} \) is an interval with the required properties that does not become clean
  at any time during  \( P \); we will derive a contradiction.
  Let \( t_{1} \) be the end time of the first \( \passno \) pairs of passes of \( P \) over \( J_{1} \).

Since \( J_{1} \) did not become clean, the number of \( \theta\B \)-intrusions into it before \( t_{1} \)
is more than \( \passno^{2} \).
The ones that come from left will be called \df{left intrusions}.
Without loss of generality we can assume that the number of left intrusions is 
\begin{align*}
 K_{2}>\passno^{2}/2,
\end{align*}
coming in \( \passno^{2}/4 \) pairs.
Let \( t_{2} \) be the time before the last \( \theta \) pairs of left intrusions.
Let \( J_{2} \) be the interval of length \( |J_{1}| \) adjacent to \( J_{1} \) on the left.
Since the intrusions
don't reach the right end of \( I \) but have length \( \ge\theta\B \),
they extend to a distance \( \theta\B/2 \) to the left
of \( I \), so each of them passes \( J_{2} \), giving at least \( \passno^{2}/4-\theta \) pairs
of passes over it.
If \( J_{2} \) becomes clean before time \( t_{2} \) then the next \( \theta \) passes over \( J_{1} \)
clean interval \( J_{1} \) via the Attack property, contrary to our assumption.

Let us iterate this reasoning for \( i=2,\dots. \)
Assume we have \( K_{i} \) pairs of passes over \( J_{i} \) before time \( t_{i} \).
Let \( t_{i+1} \) be the  time \( t_{i+1}<t_{i} \) before
the last \( \theta \) pairs of them.
We can divide the ones before \( t_{i+1} \)
into groups of \( \passno+\theta \) consecutive pairs of passes, so we have now at least
\begin{align*}
   K_{i}-\theta
\end{align*}
pairs of passes over \( J_{i} \) before \( t_{i+1} \) divided into consecutive groups of size \( \passno+\theta \).
Each group fails to clean \( J_{i} \), so the number of its \( \theta\B \)-intrusions is more than \( \passno^{2} \),
giving a total of more than
\begin{align*}
 \passno^{2}(K_{i}-\theta)/(\passno+\theta)  > 4\passno(K_{i}-\theta)/5
\end{align*}
if \( \passno \) is large.
Without loss of generality, at least half of these intrusions is on the left of \( J_{i} \).
Let \( J_{i+1} \) be the interval of size \( |J_{i}| \) adjacent to \( J_{i} \) on the left, so just as in the reasoning
about \( J_{2} \), we conclude that it is passed over in at least
 \begin{align}\label{eq:K-next}
   K_{i+1}=\passno(K_{i}-\theta)/5  = K_{i}\passno /5- \theta\passno/5
 \end{align}
 pairs.
Hence for \( i\ge 3 \)
\begin{align*}
  K_{i} &= (\passno/5)^{i-1}\passno -\theta((\passno/5)+(\passno/5)^{2}+\dots+(\passno/5)^{i-2})
\ge (\passno/5)^{i}
\end{align*}
if \( \passno \) is large.
Since \( P \) is finite, sooner or later this process must terminate, leading to a contradiction.
\end{proof}

\subsection{Escape}\label{sec:escape}

This section scales up the Escape property of trajectories.
We will frequently measure the length of a path not by the time it takes but its number of segments,
in the sense of Definition~\ref{def:segments}.

\begin{definition}\label{def:segments-spec}
  In the present section, a \df{segment} we will mean a \( \theta\B \)-segment.
\end{definition}

Note that one work period of simulation or rebuilding happens within an
area of size \( \le\theta\Q\B \).
If a path is burst-free then the Escape property allows to estimate the time length of the path
by its number of segments:

\begin{lemma}\label{lem:segment-time}
  A burst-free path with \( s \) segments takes time \( \le \CEsc \theta^{2}(s+1)\Tu \).  
\end{lemma}
\begin{proof}
  Given that \( \theta<\beta \),
  The Escape property says that the path leaves every segment of size \( \le \theta\B \) in
  time \( \CEsc\theta^{2}\Tu \).
\end{proof}

In what follows we will estimate the length of paths mainly by their number of segments.

% The following lemma is similar to Lemma~\ref{lem:combined-heals}, but deals with longer intervals.

% \begin{lemma}\label{lem:rebuild-pass}
% Let \( I=\lint{a}{b} \) be a clean interval of size \( > 13 \Q\B \).
% If the head passes it without a burst then a subinterval of size \( |I|-10\Q\B \)
% becomes clean for \( M^{*} \).
% \end{lemma}
% \begin{proof}
% \end{proof}

\begin{definition}
  For an interval \( I \), the interval \( \Int(I,\CSpill\B) \) be called its \df{spill-interior}.
  A maximal clean interval of size \( \ge\CAtt\B \) will be called a \df{clean hole}.
  Let \( \cK(t) \) denote set of all clean holes at time \( t \),
  let \( K(t)=\bigcup \cK(t) \).
  Let \( \cK^{*}(t) \) be the set of super-healthy clean holes, and \( K^{*}(t)\subseteq K(t) \)
  be their union.
\end{definition}

Consider a burst-free space-time rectangle \( I\times J \).
The Spill Bound property implies that 
no disorder can appear during it in the spill-interior of a clean hole.
If \( I_{1} \) is a clean hole at time \( t \) and \( I_{2}, I_{3} \) are clean holes at time \( t+1 \)
intersecting the spill-interior of \( I_{1} \) then the smallest interval containing \( I_{2}\cup I_{3} \) is
contained in a clean hole at time \( t+1 \).
So the elements of the set \( \cK(t) \) of clean holes of size \( \ge\CAtt\B \)
will not break up: they can grow, shrink but only slightly, bounded by the Spill condition
(and thus will not disappear) or merge; new elements can appear as well.
They may intersect without merging, but only to an extent less than \( \CSpill\B \).
We can similarly follow a single clean hole \( I(t) \).

Just as a clean hole cannot break up in the absence of bursts, a super-healthy
interval cannot break up either.
So we can follow the evolution of super-healthy clean holes in \( \cK^{*}(t) \) similarly.
Recall that a super-healthy interval in history \( \eta \) is clean in the decoded history \( \eta^{*} \),
so the simulation of \( \eta^{*} \) will proceed in it.
In what follows we will estimate how much the area of the sets \( K(t) \) and \( K^{*}(t) \)
will grow with the cumulative time spent by the head in \( K(t) \).
The following lemmas consider the activity of the head while it is inside a clean hole \( I(t) \).

If the head stays in long enough then it will create colonies if they were not there.

\begin{lemma}\label{lem:inside-hole}
  Let \( I(t) \) be a clean hole having size \( \le n\B \) with \( n\le\lambda\Q \), \( \lambda\le 3\beta \) at the
  beginning of a burst-free path \( P \),
  and \( k_{I}(t)=|K^{*}(t)\cap I(t)| \).
  If \( n<\Q/2 \) then the head will leave \( I(t) \) within \( c\Z n \)
  segments for an appropriate \( c \).
  Otherwise, every \( \lambda\U \) segments of its stay increases
  \( k_{I}(t) \) (that is the size of area ``organized up to the next level'') by at least \( \Q\B \).
\end{lemma}
\begin{proof}
  We will see that while the head does not leave \( I(t) \),
  every certain number of segments results in some kind of progress.
  \begin{enumerate}
  \item\label{i:inside-hole.rebuild}
Suppose that we are at some time when the rebuild procedure had started,
from a base of rebuild-marked cells large enough not to result in alarm (renewed call for healing)
after the first zigging.
Then the rebuilding will be completed.

\item\label{i:inside-hole.normal}
  Suppose that we are at some time when the mode is normal.
  Then within \( 2\Z \) steps of the simulation (and hence at most \( 2\Z \) segments)
  either a healing call happens, or a step of progress will be
  made in the ordinary work of simulation.
  
\item\label{i:inside-hole.alarm}
  Suppose that at some time healing will be called.
  Then according to the proof of Lemma~\ref{lem:healing},
  we either arrive at the above case~\ref{i:inside-hole.rebuild} (failed healing)
  in \( O(\beta^{2}<\Z) \) steps or healing finishes in
  normal mode with at least one step made either to move closer to the front or to make a move
  in the ordinary work of simulation.
  
  Indeed, if healing succeeds it leaves a healthy area.
  Lemmas~\ref{lem:health-extension} and~\ref{lem:combined-heals} imply that 
  the overlapping successful healing areas can be combined, so healing will not be repeated over the same
  area, and thus can slow down progress only by a 
  factor \( O(\beta^{2}) \) (negligible compared to the \( \Z \) times slowdown by zigging), and even this
  in at most one sweep of simulation over any area.

\item\label{i:long-stay}  Suppose that \( n>\Q/2 \) and the head stays for more segments than
  \( \lambda\U \).
  Without rebuilding, the continued healings add some colonies to the healthy area; any such colony will be
  turned super-healthy by the first complete work-period of the simulation.
  If the head is already in a super-healthy area then it will perform the simulation of \( M^{*} \).
  The simulated machine \( M^{*} \) has the same program as \( M \), so it will also
  make switchbacks
  of at least \( \E>3\beta \) steps (for healing) or even bigger ones for zigging.
  This extends the super-healthy area unless interrupted by rebuilding.

\item  If any rebuilding starts then it will either finish and extend \( K^{*}(t) \) by a colony,
  or will be interrupted by a turn-starvation, as defined in Section~\ref{sec:feathering}.
  Suppose that turn-starvation happens at an attempted left turn: then the marked healing area is already
  of a size \( \ge\cns{turn-limit-2}\Q \).
  Therefore a next turn starvation cannot be due to an attempted right turn on the left, since
  the marking process does not proceed so far to the left.
  Repeated turn-starvations could only happen to the right, and the head would leave on the right before the
  assumed \( \lambda\U \) segments of stay.
  Since this does not happen, rebuilding must succeed before this.
\end{enumerate}
\end{proof}
\Pnote{Elaborate!}

Now we consider cases where the head exits and enters a clean hole repeatedly.

\begin{definition}\label{def:super-healthy-end}
  Let \( I \) be a clean interval.
  We will say that the right end of \( I \) is \df{advancing} at time \( t \)
  if the rightmost colony of \( K^{*}(t)\cap I \) is in the start of the process of transfer to the right.
  Advancing is defined similarly for the left end.
\end{definition}

\begin{lemma}\label{lem:advancing-end}
  Consider the conditions and notation of Lemma~\ref{lem:inside-hole}.
  \begin{alphenum}
  \item\label{i:make-advancing}
    If the head stays in \( I(t) \) longer than 
    \( c\Z n \) segments and \( k^{*}_{I}(t) \) did not increase by at least \( \Q\B \),
    then the end on which it leaves becomes advancing.
  \item\label{i:enter-advancing}
    If the head enters \( I(t) \) on an advancing end and stays longer than \( c\Z n \) segments
    then \( k^{*}_{I}(t) \) increases by at least \( \Q\B \).
  \end{alphenum}
\end{lemma}

\begin{proof}
  To~\eqref{i:make-advancing}:
  If the head did not continue the simulation on \( K^{*}(t) \) then it would have spent its time in
  healing-rebuilding.
  In this case the time is sufficient to create a new colony, increasing \( k^{*}_{I}(t) \) by at least
  \( \Q\B \).
  Since this did not happen, it must have continued a simulation.
  But then it can leave \( K^{*}(t) \) only via a started transfer operation.

  To~\eqref{i:enter-advancing}:
  Suppose that the head enters on an advancing right end.
  The head cannot get to the left of the colony \( C \) 
  that started the right transfer operation without completing it, and entering \( C \) from
  the right from a new colony (either created by the transfer or by a new rebuilding operation).
  And it spends so much time on the right of \( C \) that at least the rebuilding operation must
  succeed.
\end{proof}

The next lemma considers a situation similar to Lemma~\ref{lem:inside-hole},
but allowing multiple entries and exits.

\begin{lemma}[Expand hole]\label{lem:expand-hole}
  Let \( I(t) \) be a clean hole having size \( n\B \) where \( n=\lambda\Q \)
  with \(  1/2\le\lambda\le 3\beta \) at the
  beginning of a burst-free time interval \( J \).
  There is a constant \( c \) with the following property.
  If the head spends cumulatively
  \( c\lambda\U \) segments in \( I(t) \) then at the end of \( J \) the hole grows
  in size by at least \( \Q\B/2-\CSpill\B > \Q\B/3 \).
\end{lemma}
\begin{proof}\begin{sloppypar}
If the head spends \( \le c\Z n \) segments in \( I(t) \), with the constant \( c \)
from Lemma~\ref{lem:advancing-end}, then we say that its stay was \df{short}.
If it spends \( \ge\lambda\U \) segments in \( I(t) \),
then we will say that the stay was \df{long}.
Otherwise we will say that the stay was \df{medium}.
  \end{sloppypar}
\begin{enumerate}

\item\label{i:fast-return} After a short stay (except possibly the first exit on the left and
  the first exit on the right), \( I(t) \) expands by at least \( \B/2 \) via the Attack property.

\item\label{i:long-time return}
  After a long stay (even without leaving), by Lemma~\ref{lem:inside-hole}, the
  value of \( k_{I}(t)= |K^{*}(t)\cap I(t)| \) increases by at least \( \Q\B \).

\item\label{i:medium stay}
  Lemma~\ref{lem:advancing-end} implies that if a medium stay did not increase
  \( k_{I}(t) \) by at least \( \Q\B \) then it creates an advancing end, and
  the next medium stay will produce such an increase.
  \end{enumerate}
    Adding up these possibilities will complete the proof.
\end{proof}

The following lemma is the scale-up of the Escape condition.

\begin{lemma}[Escape]\label{lem:escape}
  There is a constant \( c \) such that in the absence of \( \Noise^{*} \), the
  head will leave any interval \( G \) of size \( n\B \) with \(n\le\lambda\Q \),
  \( 1\le\lambda\le 3\beta \), within \( c\U \) segments.
\end{lemma}
\begin{proof}
  If \( J \) is the time interval considered, then we can ignore the at most one
  burst happening during \( J \) by considering the half of \( J \) where it does not occur.

  At any time \( t \), we will partition the interval \( G \) into subintervals as follows.
  First, let \( \cK(t) \) be the set of clean holes of size \( \ge\CAtt\B \) inside \( G \),
  and \( K(t) \) their union.
  Each interval between two such clean holes that is larger
  than \( \theta\B/2 \) will be subdivided into a sequence of subintervals; the last one
  has size \( \le \theta\B/2 \), the other ones have size \( \theta\B/2 \).
  Let us call all members of this partition of \( G \) \df{blocks};
  the clean holes among them will be called \df{clean}, the other ones are called \df{short}.

  As time passes new clean blocks may arise, the existing ones may
  increase, and neighboring ones may merge.
  (They may also decrease due to spill of disorder but this decrease is temporary until the next increase
  due to attack.)

  We will consider a sequence of times \( t_{0}<t_{1}<\dots \) defined as follows.
  \( t_{0} \) is our starting time.
  If \( t_{i} \) is defined and then the head is in some block \( S \),
  then \( t_{i+1} \) is the first time coming at which 
  \begin{itemize}
  \item if \( S \) is clean then the head is either not in \( S \) or the path entered a new segment.
  \item if \( S \) is short then the head is not in either \( S \) or in any short block adjacent to it.
  \end{itemize}
 %  The Escape property of trajectories implies \( t_{i+1}-t_{i}\le e_{1}\Tu \), where
 %  \begin{align*}
 %   e_{1}=\CEsc\cdot 3\theta/2,
 % \end{align*}
 %  since the length of a union of three consecutive short blocks is \( (3\theta/2)\B \).
  By definition, between times \( t_{i} \) and \( t_{i+1} \) one of the following events occurs.
  We will show the contribution to the growth of \( |K(t)| \) made by them.
  Clearly \( K(t) \) cannot grow larger than \( |G| \).
  
  \begin{varenum}{e}

  \item\label{i:cross} A boundary is crossed between clean and short blocks.
    A pair of passes over the same boundary results in an increase of \( K(t) \) by \( \B \).
    The number of un-paired passses over boundaries is at most the upper bound of the number of
    such boundaries, that is \( e_{2}\Q \) where
    \begin{align*}
      e_{2} = \lambda/\CAtt.
 \end{align*}
    So if \( s \) segments are spent on events of this type then 
    such events, and then the growth of \( K(t) \) is at least
    \begin{align*}
      \frac{s - e_{2}\Q}{2}\cdot\frac{\B}{2}.
    \end{align*}
    Assuming \( s\ge 2 e_{2}\Q \) this is at least \( s\B/8 \).
    This becomes larger than \( |G|=\lambda\Q\B \) if \( s>8\lambda\Q \)
    so the number of segments spent on these events is less than this.

  \item\label{i:merge} Two clean blocks merge.
    The number of these events is limited by \( e_{2}\Q \).
    
  \item\label{i:cum-incr} The number of segments spent in \( K(t) \) increases by one.
    Let \( \cns{expand} \) be the constant \( c \) of Lemma~\ref{lem:expand-hole}.
    By that lemma, if the head spends time \( \cns{expand}\lambda\U \) segments
    on these events then \( K(t) \) increases by at least \( \Q\B/3 \).
    So it cannot spend more than \( 3\cns{expand}\lambda^{2}\U \) without escaping \( G \).    
    
  \item\label{i:passes} The number of passes over a short block \( S  \) increases by 1.
    If \( \passno+2 \) pairs of passes happen over a short block \( S \)
    then Lemma~\ref{lem:pass-clean-0} becomes
    applicable and \( \Int(S,\CPass\B) \), of size \( \ge \theta\B/6 \), becomes clean,
    increasing \( |K(t)| \) by \( \theta\B/6 \).
    If \( n \) is the number of such events then the increase is at least
    \begin{align*}
   \frac{n\theta\B}{2\passno+4}.
    \end{align*}
    This becomes larger than \( \lambda\Q\B \) if \( n>(2\passno+4)\lambda\Q/\theta \).
  \end{varenum}
\end{proof}

\subsection{Pass cleaning}\label{sec:pass-cleaning}

This section will scale up the Pass Cleaning property to machine \( M^{*} \).
We will consider a path \( P \) with no bursts of \( \eta^{*} \), as
it makes passes over an the interval \( I \) of size \( \theta\Q\B \).
First a strengthening of Lemma~\ref{lem:pass-clean-0}: we allow bursts of \( \eta \).

\begin{lemma}\label{lem:pass-clean-1}
  Let \( \r = 4 \), and suppose that a
  path \( P \) has no bursts of \( \eta^{*} \), it makes \( \passno+\theta \) pairs of passes over
  the interval \( I \) of size \( \theta\Q\B \),
  with the number of \( \theta \Q\B \)-intrusions at most \( (\passno+2\theta)^{2} \).
  Let \( J_{1}\subset \Int(I,\CPass\Q\B/2 ) \) be an interval of size \( \theta\B \)
  at a distance \( \ge\r\theta\B/2 \) from any burst of \( \eta \).
  Then there is a time during \( P \) when \( \Int(J_{1},\CPass\B) \) becomes clean.
\end{lemma}
The proof still uses no properties of the simulation program.
\begin{Proof}
\begin{step+}{step:pass-clean-1.num-bursts}
  There are at most \( 2\passno+(\passno+2\theta)^{2}<2\passno^{2} \) bursts of \( P \) in \( I \),
  and at most \( 2 c \passno^{2}\U \) segments, where the constant \( c \) is the one from
  Lemma~\ref{lem:escape}.
\end{step+}
\begin{pproof}
  The Escape Lemma~\ref{lem:escape} implies that every pass of \( P \) over \( I \)
  leaves using at most \( c\U \) segments, in time \( \Tus \), hence has at most one burst.
  Each \( \theta\Q\B \)-intrusion lasts also at most time \( \Tus \).
  So the total number of bursts is at most the sum of the number of passes and intrusions.
  A bound on the total number of segments is obtained multiplying this number by \( c\U \).
\end{pproof} % step:pass-clean-1.num-bursts

\begin{step+}{step:pass-clean-1:0}
  Since the interval \( J_{1} \) is at a distance \( \ge\r\theta\B/2 \) from any burst, we can
 repeat the reasoning of the proof of Lemma~\ref{lem:pass-clean-0}, for \( i\le\r \).
  We again define a sequence of adjacent intervals \( J_{i} \) such that the path \( P \) makes
  \( K_{i}\ge(\passno/5)^{i} \) pairs of passes over \( J_{i} \)
\end{step+}
\begin{step+}{step:pass-clean-1.r}
We will handle the case \( i=\r \) differently, using
the observation that the total number of bursts is at most \( s= 2\passno^{2} \).
\end{step+}
\begin{prooof}
Each burst may affect at most one pair of passes, so there remain at least \( K_{i}-s \) burst-free ones of them,
separated into at most \( s+1 \) burst-free groups.
At least one of these groups has size at least
\begin{align*}
  K_{\r+1}&=\frac{K_{\r}-s}{s+1} > \frac{K_{\r}-2\passno^{2}}{2\passno^{2}+1}
            \ge \frac{\passno^{r-2}}{3\cdot 5^{\r}}.
\end{align*}
if \( \passno \) is large.
Let \( t_{r+1} \) be the endtime of the last pass in this group.
With \( \r = 4 \) this gives \( K_{5}\ge \passno^{2}/3\cdot 5^{4} > \passno \) if \( \passno  \) is large.
\end{prooof} % step:pass-clean-1.r
\begin{step+}{step:pass-clean-1.end}
From here on we can again use~\eqref{eq:K-next}, since no burst appears at all during
the time of this group of passes, and we get \( K_{i}\ge (\passno/5)^{i-4} \).
The number of \( \theta\B \)-intrusions into \( J_{i} \) grows correspondingly.
Since \( J_{1}\subseteq\Int(I,\CPass\Q\B/2) \), before \( J_{i} \) gets outside \( I \)
we have \( i>c'\Q \) for some constant \( c' \), giving a lower bound exponential in \( \Q \)
on the total number of segments.

But by part~\ref{step:pass-clean-1.num-bursts} above, the total number of segments in \( P \)
is bounded by 
\begin{align*}
  2 c \passno^{2}\U\le 2\CSim c\beta \passno^{2}\Q^{2}
\end{align*}
where we used the definition of \( \U \) in Definition~\ref{def:Tu}.
Since this is only quadratic in \( \Q \), the lower and upper bounds collide, delivering the desired
contradiction.
\end{step+}
\end{Proof}

Under the conditions of Lemma~\ref{lem:pass-clean-1}, the interiors of
burst-free subintervals of \( I \) become clean.
By the Spill Bound property, they essentially also remain clean as long as they remain burst-free.
The following two lemmas will help expanding them.

Recall that \( \E \ll \Z \) is the maximum number of cells in a healing area.

\begin{definition}\label{def:directed}
An interval \( J \) not containing the head is called \df{right-directed}
if the head is to the right of \( J \),
and its cells, maybe with the exception of an island and an area of size \( \E\B \) on the left end
and one of size \( k\E\B \) with \( k<\Z/2 \) on the right end,
point towards a front at the right end of \( J \) (in normal operation or rebuilding),
with the corresponding frontier zone.
We will say that \( J \) has \df{damage size} \( k \).
\end{definition}

\begin{lemma}\label{lem:make-directed}
  If the head passes through a clean interval \( J \) of size \( >4\Z\B \) noiselessly
 from left to right then \( J \) becomes right-directed with no damage.
\end{lemma}
\begin{proof}
  The head is acting in one of three modes: normal, healing or rebuilding.
  Assume that the starting mode is normal.
  If it remains normal then \( J \) naturally becomes directed by the time the head leaves.
  If healing mode gets triggered then as long as healing succeeds it extends a healthy area.
  It moves towards right only if the front is on the right.

  If it does not succeed then rebuilding gets triggered,
  and since it does not touch the left end anymore, it either succeeds or exits on the right
  while trying to extend towards the right.
 The same considerations work if the starting mode is healing, except possibly on
 the part of the left end where the first healing took place, whose purview
 was not completely contained in \( J \).
 
The analysis is similar if the starting mode is rebuilding.
\end{proof}

Recall the definition of the parameter \( \F \) in~\eqref{eq:FDef}.

  \begin{lemma}\label{lem:keep-directed}
    Suppose that a basic interval \( J \) is right-directed, the head enters it and leaves it,
    with at most one burst happening during this.
  \begin{alphenum}
  \item\label{i:stay-directed}  If the head leaves on the right then
    \( J \) will stay right-directed. 
  \item\label{i:other-end}  If the head leaves on the left then the right end
    is a legitimate turning point of the simulation or the rebuilding
    (that is \( J \) has almost \( \F \) cells with \( \Turned=2 \) on its right).
  \end{alphenum}
\end{lemma}
\begin{proof}
  To~\eqref{i:stay-directed}: Both in normal operation and rebuilding,
  the head moves the front with itself and returns to it from every zig, except when it is
  captured at an end of \( J \) (by a burst or disorder).
  If an island is encountered or a burst occurs then it will be healed, except when the healing interval
  (whose maximum size is \( \E\B \)) intersects
  the boundary of \( J \) (where the disorder may capture the head).

  To~\eqref{i:other-end}: the front can turn back only at the legitimate turning points;
  otherwise the simulation will move it forward, and the forward zigging prevents a burst or
  disorder from turning it back prematurely.
\end{proof}

Let
\begin{align}\label{eq:pi-star}
 \passno^{*}= \passno +2\theta.
\end{align}

\begin{lemma}[Pass cleaning]\label{lem:pass-clean}
Suppose that a path \( P \) has no bursts of \( \eta^{*} \), it makes \( \passno^{*} \) pairs of passes over
the interval \( I \),
and the number of \( \theta \Q\B \)-intrusions is \( \le (\passno^{*})^{2} \) for
for \( I \) and \( P \).
Then by end of the \( \passno ^{*} \)th pass the interior
\( \Int(I, \CPass\Q\B) \) becomes clean for \( \eta^{*} \).
\end{lemma}

\begin{Proof}
    Just as in the proof of Lemma~\ref{lem:pass-clean-1}, there are at most
     \begin{align}\label{eq:islands-bd}
 \kappa = 2\passno^{2}
     \end{align}
bursts of \( P \) in \( I \).
That  lemma shows that at some time \( t_{0} \)
after \( \passno+\theta \) passes, the areas of \( I \)
at least a constant distance away from the bursts become clean, so the  interval 
\begin{align*}
 I'=\Int(I,\CPass\Q\B/2 )  
\end{align*}
becomes clean for \( \eta \), except for \( O(\passno^{2}) \) islands.
So the interval \( I \) contains clean subintervals of size \( \ge 4\Z\B \)
separated from each other by areas of size \( O(\passno^{2}\Z\B) \).
Let us call these clean subintervals \df{basic holes}.
\begin{step+}{step:pass-clean.dir-small}
Two pairs of passes make \( I' \Int(I',O(\kappa\E)) \) clean.
\end{step+}
\begin{pproof}
 Lemmas~\ref{lem:make-directed} and~\ref{lem:keep-directed} show that
\begin{itemize}
  \item Passing a basic hole left to right makes it right-directed.
  \item  The limited number of intrusions keep it right-directed with the damage staying
 bounded by \( \kappa\E \ll \Z \).
\item Passing a right-directed basic hole right to left can happen only if the
  right end is a legitimate turning point (of the simulation or rebuilding),
  even if a new rebuilding was triggered at the right end.
  \item Feathering does not allow the following right pass over the same basic hole
to turn back before going a distance at least \( \F\B \) to he right.
As \( \F \gg\kappa\Z \), which is 
the upper bound on the separation between the basic holes,
the the remaining disorder between them must be wiped out before the head can move left over them
in the second pair of passes.
\end{itemize}
 \end{pproof}
  
\begin{step+}{step:pass-clean.dir-large}
The following pass makes \( \Int(I',O(\kappa\E)) \) clean for \( \eta^{*} \).  
\end{step+}
\begin{pproof}
  Before the last pass from right to left, there can be several intrusions.
  Each such intrusion will turn back only at a legitimate turning point.
  It may leave an island if a burst happens on its left end.
  If a next intrusion passes over this island then due to feathering it must pass at a
  significant distance, and clean away the island.
  If it turns back before passing it then it must turn back at a distance of \( \F\B \),
  so the islands left this way are separated by \( \F\B \).
  When finally a left pass comes it can only pass over the whole interval \( I \) while
  cleaning all these islands and finding or rebuilding whole colonies.
\end{pproof} % step:pass-clean.dir-large

\end{Proof}

\begin{remark}
Left and right are not symmetric in the above proof.
Consider a right-directed basic hole with a rebuilding area being extended on the right.
The head may be captured and returned, extending some other rebuilding area towards the left.
Without giving preference to the right direction, this switching between left and right-directed rebuilding
could happen an unbounded number of times.
Using the asymmetry of the definition of rebuilding in Section~\ref{sec:rebuilding},
the right-directed extension of the basic hole will not be overridden by a left-directed rebuilding.  
\end{remark}

\subsection{Spill bound}\label{sec:spill-bound}

Let us prove the scaled-up version of the spill bound property.

\begin{lemma}\label{lem:spill-bound}
Suppose that an interval \( I \) of size \( > 2\CSpill\Q\B \) is clean for \( \eta^{*} \). and
let \( P \) be a path that has no bursts of \( \eta^{*} \).
Then \( \Int(I,\CSpill\Q\B) \) stays clean for \( \eta^{*} \).
\end{lemma}
\begin{Proof}
  The decrease of \( I \) by \( \CSpill\Q\B \) at any of the edges is possible due to some rebuilding
  process starting there.
  Due to the preference to the right expansion of rebuilding, there can be only one rebuilding area on
  the right edge of \( I \), but the left edge may see a sequence of overlapping rebuilding areas.
  Since this is the more complex possibility, let us just concentrate on this one, so all the
  considered exits and entries of the path into \( I \) will be on the left.

  \begin{step+}{step:no-spill.bursts}
  The lack of bursts of \( \eta^{*} \) and the escape property implies that
  each incursion of the path into \( I \) contains at most one burst.
  For simplicity, we will always assume that there is a burst, referring to it as \emph{the} burst.
\end{step+}

When the path first enters into \( I \), which is clean for \( \eta^{*} \) and therefore healthy
for \( \eta \), then if it can work in normal mode, it will continue the simulation.
The burst will be healed unless it is either at a legitimate turning point or is
closer than \( \E\B \) to the edge of \( I \).
In what follows the clean part \( I' \) of \( I \) may become smaller and may contain islands
but only at legitimate turning points.
All colonies except the leftmost and rightmost ones, may contain only one island.
Indeed, if the head passes those in the normal course of simulation then only the
last pass can leave an island.

The path will enter rebuilding mode only if it is within \( \E\B \) of the edge of \( I \),
and a healing did not succeed (or even start).
In this case, it will extend a rebuilding area \( J_{1} \) starting from near the left edge.
If the rebuilding succeeds then it will extend the healthy area of  \( I \) on the left,
and we are done.

But the path may leave it on the left edge before finishing.
It may in this case leave a single island if the burst happened at some legitimate turning point,
or just as above, near the left edge of \( I \).
When it enters next time, it may not continue the work
of rebuilding of \( J_{1} \) but may also start a new rebuilding area \( J_{2} \)
on the left edge of \( J_{1} \) that may overwrite part of \( J_{1} \)
due to the priority of right-extending rebuilding.
If will heal any island from the previous incursion that it encounters, since the place of
this island cannot be a new turning point now.
So now the picture is a disjoint union \( I=D\cup J_{2}\cup J_{1}\cup I' \),
where \( D \) is the possible union of islands formed by the bursts at the left
edge, and \( I' \) is clean.

When the head enters next time, it can only work on \( J_{2} \) (possibly extending it over \( J_{1} \))
or start a new rebuilding area \( J_{3} \) on the left edge of \( J_{2} \).

Generalizing this picture we say that  \( I \) is \df{defended on the left} if
  \begin{varenum}{d}
  \item  It is a disjoint union \( I=D\cup J\cup I' \) where \( I' \) is clean for \( \eta^{*} \),
    further 
  \begin{align*}
   J= J_{1}\cup \cup  J_{2}\dots\cup J_{k}
  \end{align*}
  where each \( J_{i} \) is a rebuilding area.
\item The endpoints of \( J_{i} \) are at legitimate turning points and therefore at a distance of
  at least \( \F\B \) from each other.
\item  Ther is at most one island in each \( J_{i} \), also at legitimate turning points and therefore  also 
  at a distance of at least \( \F\B \) from all other islands.
  \item\label{i:inside-island} There is also at most one island introduced by \( P \) in \( I' \).
  \end{varenum}

  The above reasoning shows that the interval \( I \) will always stay defended on the left.
  Moreover, if \( J \) is non-empty then it can only become empty again when
  all the rebuilding on the left succeeds.
  As we will see then we will be done, since below we will bound \( |D| \).
  If \( J \) stays non-empty then also, since \( |J\cap I| \) is 
is at most as large as the largest possible rebuilding area,
 we only need to bound \( |D| \) to be done.
 For this, we will bound the total number of bursts created by \( P \) in \( I \) by \( 2\pi \).

 The lack of bursts of \( \eta^{*} \) implies that between consecutive bursts the path must
 run long stretches in time.
 Let us show that it must spend these stretches (except for a couple) on the left. 
 
 \begin{step+}{step:spill-bound.left-only}
   If \( J \) stays empty then \( P \) cannot enter more than \( 3\Q\B \) deep into \( I \)
   without extending clean area of \( I \) on the left.
  \end{step+}
  \begin{pproof}
   Let \( C \) be the leftmost colony of \( I \) not involved into a rightward transfer operation
   (so it is the lefmost or second leftmost colony of \( I \)).
   If  \( P \) passes \( C \) from right to left then next time it can enter \( C \) only from another
   colony on its left.
  \end{pproof} % step:spill-bound.left-only

  Let \( I_{0} \) be the interval of size \( \theta\Q\B \) on the left of \( I \).
  From the above it follows that
  every time (but maybe two) when \( P \) leaves a burst, it must make a pass over \( I_{0} \).
  Now an argument similar to the proof of Lemma~\ref{lem:pass-clean-0}, scaled up to \( \eta^{*} \),
  shows that at some time during these passes, \( I'_{0}=\Int(I_{0},\CPass\B\Q) \) becomes
  clean for \( \eta^{*} \).
  But then the subsequent passes from \( I'_{0} \) into \( I \) will clean the area
  between them and unite them.  
\end{Proof}

\subsection{Attack cleaning}

This section will scale up the Attack Cleaning property of trajectories (Definition~\ref{def:traj})
to machine \( M^{*} \).
First we prove a weaker version, requiring the space-time rectangle of interest to be free
not only of \( \Noise^{*} \) but also of \( \Noise \)---that is burst-free.

\begin{lemma}\label{lem:clean-attack}
Consider a trajectory \( \eta \) in a noise-free space-time rectangle.
Here, the decoded history \( \eta^{*} \) satisfies the Attack Cleaning property.
\end{lemma}
\begin{proof}
  \begin{sloppypar}The property says the following for the present case.
For current colony-pair \( x,x' \) (where \( x'< x+2\Q\B \)), suppose that the interval
\( \lint{x-\CAtt\Q\B}{x'+\Q\B} \) is clean for \( M^{*} \).
Suppose further that the transition function, applied to \( \eta^{*}(x,t) \), directs the head right.
Then by the time the head comes back to \( x-\CAtt\Q\B \),
the right end of the interval clean in \( M^{*} \)
containing \( x \) advances to the right by at least \( \Q\B \).
 \end{sloppypar}

  

The computation phase of the simulation on the colony pair 
\( x, x' \) is completed without the disturbing effect
of noise: even the zigging does not go beyond the boundary.
Then the transfer phase begins which enters the disorder to the right of \( x'+\Q\B \).

We argue that there are only two ways for the head to get back to \( x-\CAtt\Q\B \).
\begin{varenum}{at}
\item\label{i: clean-attack.normal} The transfer into a new colony pair with starting point
\( y\ge x'+\Q\B \) succeeds despite the disorder, and the clean interval extends over it, before
the head moves left to \( x-\CAtt\Q\B \) in the course of the regular simulation.
Some inconsistencies may be discovered along the way, but they are corrected by healing.

\item\label{i: clean-attack.rebuild} The inconsistencies encountered along the way trigger some
rebuilding processes.
Eventually, a complete, clean rebuilding area is created, the rebuilding succeeds, leaving a clean
colony also to the right of \( x'+\Q\B \).
\end{varenum}

The Spill Bound property guarantees that the area to the left
of \( x'+(\Q-\CSpill)\B \) remains clean,
therefore the only way for the head to move left of that is by the rules.
Suppose that rebuilding is not initiated (with creating a substantial germ):
then moving left can only happen by the normal
course of simulation: the transfer stage of the simulation must be carried out, and this requires 
at least as many attacks to the right as the number of sweeps in the transfer stage.
Every attack (followed by return) extends the clean interval further, until the whole target colony becomes clean,
and the transfer completed.
This is the case~\eqref{i: clean-attack.normal}.

Recall the rebuilding procedure in Section~\ref{sec:rebuilding}:
the rebuilding area extends \( 2.5\Q \) cells to the left and right from its initiating cell.
This may become as large as \( 5\Q\B \) to the left and right.
If rebuilding is initiated, its starting position
is necessarily to the right of \( z=x'+(\Q-\PadLen-2)\B \).
It then may extend to the left to at most \( z-5\Q\B \) (this is overcounting,
since the cells of the colony of \( x \) are all adjacent).
Its many sweeps will result in attacks that clean an area to the right of the starting point.
The procedure may be restarted several times, but those restartings will also be initiated
to the right of \( z \).
Therefore the rebuilding area does not extend to the left of \( z-5\Q\B \):
if the head moves to the left of this, then the rebuilding must have succeeded.
The rebuilding also must find or create a colony manifestly to the right of the restarting site: this will be
to the right of \( z \), moving this way the boundary of the area clean in \( M^{*} \) 
by at least \( \Q\B \): this is the case of~\eqref{i: clean-attack.rebuild}.

In the process described above,
it is possible that the rebuilding finds a competing colony \( C \)
starting at some \( y \in x'+\Q\B + \lint{-\beta\B}{0}\) which
slightly (by the size of an island) overlaps from the right with the colony of \( x \).
The rebuilding may decide to keep \( C \) and to overwrite the rest of the colony of \( x' \) as a bridge
(or even target).
This does not affect the result.
\end{proof}


% \subsection{Extended cleaning}\label{sec:extended-cleaning}

% Let us draw some consequences of the Pass Cleaning property of trajectories (Definition~\ref{def:traj}).
% We will extend this property to longer intervals, allowing also  some bursts.
% For the following lemma, let us use the following notation for convenience:
% \begin{align*}
%    \beta'=\beta+2\CSpill.
%  \end{align*}
% Also, for any interval \( I=\lint{a}{b} \) let \( I'=\lint{a+\CSpill\B}{b-\CSpill\B} \).

% \begin{lemma}\label{lem:dirty-passes}
% Assume that the head passes \( n> 2\passno \) times over and interval \( I \) of size \( |I|\ge 2\passno \B \) 
% in a burst-free way during some time interval \( J \).
% There could be some other times in \( J \) when the head is subject to a burst inside \( I \).
% Assume that the total number of these bursts is \( < n/4\beta' \).
% (We don't count the times when the head enters and leaves \( I \) without
% passing over and without any burst.)
% Then there is some time during \( J \) when \( I' \) becomes clean.
% \end{lemma}
% \begin{proof}
% We introduce a \df{virtual} time counting.
% We only count the times when the head passes \( I \).
% Let this virtual time interval be denoted by \( K \): we can assume it starts at 0.
% Let \( K_{1}=\rint{0}{\passno} \) and \( K_{2}=\rint{\passno}{n} \), so \( K=K_{1}\cup K_{2} \).

% For each burst (of size \( \le\beta\B \)), let us extend it left and right by \( \CSpill \B \) 
% to a size \( \le  \beta' \B \).
% Let \( D_{1} \) be the union of all those intervals coming from bursts occurring before virtual time \( \passno \).
% By the end of time interval \( K_{1} \) the set \( I\setminus D_{1} \) 
% becomes clean, due to the Pass Cleaning property.
% In what follows we are looking at how \( D_{1} \) shrinks during \( K_{2} \)---while some new bursts may
% delay the shrinking.

% Consider intervals \( \lint{a}{b}, U \) such that \( \lint{a}{b}\subset U' \),
% and \( U\setminus \lint{a}{b} \) is clean at virtual time \( u>1 \).
% If no burst occurs at the virtual times \( u,u+1 \) (that is during two burst-free passes) then 
% the Attack Cleaning property implies that 
% by the virtual time \( u+2 \), already the interval \( U'\setminus \lint{a+\B}{v-\B} \) will also be clean.
% So let us mount a triangle \( T\subset I\times K \) over \( \lint{a}{b} \)
% which at time \( u+2j \) covers the interval
%  \begin{align*}
%  \lint{a+j\B}{v-j\B},
%  \end{align*}
% and thus its tip is at virtual time \( v=u+(b-a)/\B \).
% We will call \( v-u \) the \df{height} of \( T \), denoted by \( |T| \).
% If no burst occurs until virtual time \( v \), then \( U' \) becomes clean
% by virtual time \( v \).
% In the special case when \( \lint{a}{b} \) reaches, say, to the right end of the interval \( I \),
% we create a triangle (with the same slopes) 
% twice as large, whose tip is at the right end of \( I \) as well.

% \begin{sloppypar}
% Let us mount a triangle of the above type over each interval of the set \( D_{1} \)
% at virtual time \( \passno \), creating a set \( \cT_{0} \) of disjoint triangles.
% While no burst occurs, the Attack Cleaning property confines disorder to these triangles.
% On the other hand, every burst may create a disordered interval 
% \( \lint{x}{x+\beta\B} \), at some virtual time \( u \).
% Let us mount a triangle then over \( \lint{x-\CSpill \B}{x+(\beta+\CSpill)\B}\times\{u\} \),
% and add all these triangles to the set \( \cT_{0} \) to get a set of triangles \( \cT \).
% These are not necessarily disjoint anymore.
%   \end{sloppypar}

% If two virtual triangles \( T_{1},T_{2} \) intersect,
% and \( T \) is the smallest virtual triangle containing both,
% then it is easy to see that \( |T|\le|T_{1}|+|T_{2}| \).
% Denote \( T=T_{1}+T_{2} \).

% Let us now perform the following operation that will create a disjoint set
% of triangles.
% We start with \( \cT \) and if we find two intersecting triangles 
% in it, then we replace them with their sum.
% Repeat until the remaining set \( \cT' \) consists of disjoint triangles.
% By the above remark \( \sum_{T\in \cT}|T| = \sum_{T\in \cT'}|T| \).
% By the Attack Cleaning property, the complement of these triangles
% is clean.
% Let us ignore the triangles on the boundary for a moment.
% The sum of the heights of the triangles is at most \( \beta' \) times the number of bursts,
% and thus at most \( n/4 \).
% Taking the boundary triangles into account can increase this by at most a factor of 2, to \( n/2 \).
% Therefore in the interval \( K \) of length \( n-\passno>n/2 \) there will be virtual
% times not intersecting with any element of \( \cT' \).
% At the corresponding real times, the interval \( I \) is clean.
% \end{proof}


% \begin{lemma}\label{lem:burst-density}
% Consider an interval \( K \) of size \( 3 \Q\B \) and a time interval \( J \) in which
% no burst of \( M^{*} \) occurs.
% If at least \( 4\beta'\passno \) bursts occur in \( K \) during \( J \) then at some time in \( J \)
% the interval \( K \) becomes clean in \( M^{*} \).
% \end{lemma}
% \begin{proof} 
% For \( d=8\beta' \), both positive and negative \( i \), and \( j=0,\dots,d-1 \), let
% \begin{align*}
%    K_{ij}=x+3\Q\B\lint{d i+j}{d i+j+1},\ 
% K_{i}=\bigcup_{j=0}^{d-1}K_{ij}=x+3\Q\B\lint{d i}{d(i+1)},
%  \end{align*}
% so \( K=K_{00} \).
% Consider the sweeps corresponding to the \( 4\beta'\passno \) bursts in \( K \).
% If a sweep does not exit \( K \) on either side then Dwell Cleaning and Attack Cleaning
% of \( M^{*} \) becomes applicable.
% Assume this does not happen; then
% either half of these sweeps exits \( K \) on the left, or half of them exits it on the right.
% Without loss of generality assume that they pass on the right.
% Then they will have to pass the whole interval \( K_{0} \) (otherwise again Dwell Cleaning
% and Attack Cleaning of \( M^{*} \)
% applies), and thus each interval \( K_{0,j} \) gets at least this many passes.
% So, the intervals \( K_{0,j} \), \( j>0 \) gets \( 4\beta'\passno/2 \) burst-free passes (since the 
% bursts occurred in \( K_{00} \)).
% Also, none of the \( K_{0j} \) becomes clean for \( M^{*} \) during the first half of these passes,
% since then the Attack Cleaning property would clean \( K_{00} \) as well during the remaining
% passes.

% Now, for \( i=1,2,\dots \) we will show that there is 
% an \( i'  \) with \( |i'|\le i \) such that each \( K_{i'j} \) gets at least 
% \begin{align*}
%   n_{i}= 4\beta'\times 2^{i-1}\passno
% \end{align*}
%  burst-free overpasses  during \( J \), 
% and \( K_{i'j} \) does not become clean during the first half of these.
% This clearly must break down somewhere, leading to a contradiction.

% We have just proved the case \( i=0 \).
% Suppose that the statement was proved up to some \( i \), we will prove it for \( i+1 \).
% In order for the interval \( K_{i'j} \) not to become clean for \( M^{*} \), 
% by Lemmas~\ref{lem:dirty-passes}, \ref{lem:rebuild-clean}
% the total number of bursts happening in it must be at least \( n_{i}/4\beta' = 2^{i-1}\passno \).
% This is true of all \( j \), so the total number of bursts in \( K_{i'} \)
% is at least \( d 2^{i-1}\passno \).
% If a sweep does not exit \( K_{i'} \) on either side then Dwell Cleaning becomes applicable,
% so either half of these sweeps exits \( K_{i'} \) on the left, or half of them exits it on the right.
% If they pass on the right then let \( (i+1)'=i'+1 \), otherwise \( (i+1)'=i'-1 \).
% Then they will have to pass the whole interval \( K_{(i+1)'} \) (again, otherwise Dwell Cleaning
% applies), and thus each interval \( K_{(i+1)',j} \) gets at least \( d 2^{i-2}\passno \) passes.
% Now the inequality~\eqref{eq:cns.traj} implies \( d > 8\beta'=2\cdot 4\beta' \), finishing the proof.
% \end{proof}

% \section{Further analyses}

% \subsection{Trouble with the current plan}

% The current plan seems not to be working. 
% This plan required the trajectory properties
% Transition Function, Attack Cleaning, Spill Bound, Dwell Cleaning, Pass Cleaning.
% But there is a question about the conditions of these properties, for example
% of Pass Cleaning.
% If we require it only during a noise-free time interval, then Lemmas~\ref{lem:dirty-passes} 
% and~\ref{lem:burst-density} are not applicable.
% They may become applicable if we require it in a time interval in which noise is sparse.
% But then it is not clear how to scale it up.
% These same two lemmas are needed to scale it up, but it seems difficult to prove their
% noisy version.




\section{After a large burst}

Our goal is to show that the simulation \( M\to M^{*} \)
defined in Section~\ref{sec:sim-codes} is indeed a simulation.
Section~\ref{sec:1-level-noise} shows this as long as the head operates in an
area that is clean for the simulated machine\( M^{*} \) (can be super-annotated), 
and has no noise for machine \( M^{*} \) (that is its bursts on the level of machine \( M \)
are isolated).
In other words, essentially the Transition Function property of Definition~\ref{def:traj} of
trajectories for the simulated machine \( M^{*} \) has been taken care of.

The new element is the possibility of large areas that cannot be super-annotated: they
may not even be clean, even on the level of machine \( M \).
The Spill Bound, Attack Cleaning, Dwell Cleaning and Pass Cleaning properties still must be proven.

\subsection{Spill bound}

One of the most complex analyses of this work is the proof that 
the simulated machine \( M^{*} \) also obeys the Spill Bound.
Let us outline the problem.

We are looking at the boundary \( z \) of a large area that is clean for the
machine \( M^{*} \): without loss of generality suppose that this is the right boundary.
We will be looking at it in a space-time rectangle \( \lint{a}{b}\times\lint{u}{v} \)
that is noise-free in \( M^{*} \).
The interesting case has \( a<z<b \) with \( z-a,b-z=O(\Q\B) \).
The assumption allows occasional bursts of noise of the trajectory \( \eta \) of \( M \),
but no two of these bursts must occur in a space-time rectangle of size comparable to
the size of a colony work period of \( M \).
The Spill Bound for trajectory \( \eta \) keeps the disorder of \( \eta \) within
\( O(\B) \) on the left of \( z \) while \( \eta \) is noise-free.
If we could assume \( \eta \) noise-free then the heal/rebuild procedures would also keep
the disorder of \( \eta^{*} \) from spilling over by more than \( O(\B^{*})=O(\Q\B) \).
However, nothing is assumed about the length of the time interval \( \lint{u}{v} \).
If the head would spend all the time within \( O(\Q\B) \) of \( z \) then due to 
the Dwell Cleaning property of trajectories,  the area would be cleaned out, again
preventing spilling.
But the head can slide out far to the right of \( b \) fast, since
the disordered area on the right of \( z \) is arbitrarily large.
Then it can come back much later to the left of \( z \), and by
a burst (allowed since much time has passed)
can deposit an island of disorder there.
Repeating this process would produce unlimited spillover, not only 
of the disorder of \( \eta^{*} \) but even that of \( \eta \).

This is where the Pass Cleaning property helps.
If the above process is repeated \( \passno \) times, the \( \passno \) passes
would clean out an interval on the
right of \( z \) whose size is of the order of \( \Q\B \),
while depositing only \( \passno \) islands of disorder to the left of \( z \).
Our construction will have \( \passno\ll \Q \): more precisely, in our hierarchy
of generalized Turing machines we will have \( \passno_{k}=k \), \( \Q_{k}=k^{2} \). %?
And \( \ll \Q \) bursts will still be handled by healing/rebuilding in \( \eta \).

Of course this sketch is very crude, but it should help motivate the reasoning that
follows.



\bibliographystyle{plain}
\bibliography{reli,gacs-publ}

\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t%%% End: 
