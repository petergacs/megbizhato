\documentclass[12pt]{memoir}

\pagestyle{plain}
%\pagestyle{simple}
\setlrmargins{*}{*}{1}
\checkandfixthelayout

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{3}
\counterwithout{section}{chapter}
\counterwithin{equation}{section}
%\numberwithin{equation}{section} % in amsmath
%\counterwithout{figure}{chapter}
\counterwithin{figure}{section}

\makeatletter
% To correct a memoir bug:
\renewcommand{\@memmain@floats}{%
  \counterwithin{figure}{section}
  \counterwithin{table}{section}}
\makeatother

\firmlists

% If you do not want the bibliography on a separate page:
\renewcommand{\bibsection}{% 
\section*{\bibname} 
\prebibhook}

\usepackage[backref,hyperindex,colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage[numbered]{bookmark} % Allows to place a bookmark, see the title. Shows section numbers.
% \usepackage[all]{hypcap} % After hyperref, to anchor floats correctly.
% \usepackage{float}
 % After hyperref:
\usepackage[algo2e,algosection,tworuled,noend,noline]{algorithm2e}
\usepackage{gacs}
\usepackage{gacs-algo} % After hyperref.
% After gacs.sty

%\usepackage[pagecolor={LightCyan1}]{pagecolor}
%\usepackage[pagecolor={DarkSeaGreen1}]{pagecolor}
%\usepackage[pagecolor={Honeydew1}]{pagecolor}
%\usepackage[pagecolor={Azure1}]{pagecolor}
%\usepackage[pagecolor={Cornsilk1}]{pagecolor}
%\usepackage[pagecolor={Ivory1}]{pagecolor}

\hyphenation{com-plex-ity des-tin-at-ion co-lon-ies}

\newcommand{\shownotes}{1}
\ifnum\shownotes=1
\newcommand{\authnote}[3]
{\text{{ \textcolor{#3}{\( \langle\hspace{-0.2em}\langle \)\textsf{\footnotesize #1: #2}\( \rangle\hspace{-0.2em}\rangle \)}}}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\Pnote}[1]{{\authnote{P}{#1}{cyan}}}
\newcommand{\Inote}[1]{{\authnote{I}{#1}{blue}}}

\renewcommand{\le}{\leq}
\renewcommand{\ge}{\geq}

\renewcommand{\vek}[1]{\mathbf{#1}}

\newcommand{\fld}[1]{\ensuremath{\textit{#1\/}}}
\newcommand{\rul}[1]{\ensuremath{\texttt{\slshape #1\/}}}
\newcommand{\maj}{\mathrm{maj}}
\newcommand{\sign}{\mathop\mathrm{sign}}

\newcommand{\tEnd}{f_{\mathrm{end}}}
\newcommand{\tZig}{f_{\mathrm{zig}}}
\newcommand{\tHeal}{f_{\mathrm{heal}}}
\newcommand{\tRebuild}{f_{\mathrm{rebuild}}}

% Using def for the possibility of switching between LaTeX and XeTeX:
\def\B{B}  
\def\U{U}

\newcommand{\Bad}{\mathit{Bad}}
\newcommand{\Vacant}{\mathit{Vac}}
\newcommand{\blank}{\text{\textvisiblespace}}
\newcommand{\Configs}{\mathrm{Configs}}
\newcommand{\D}{D}
\newcommand{\E}{E}
\newcommand{\f}{f}
\def\G{G}
\newcommand{\h}{h}
\renewcommand{\H}{H}
\newcommand{\hc}{\hat h}
\newcommand{\Noise}{\mathit{Noise}}
\newcommand{\Output}{\mathit{output}}
\newcommand{\PenetrationLen}{\mathrm{PenetLen}}
\newcommand{\Plus}{\oplus}
\newcommand{\Minus}{\ominus}
\newcommand{\pos}{\mathrm{pos}}
\newcommand{\curcell}{\textrm{cur-cell}}
\newcommand{\Q}{Q}
\newcommand{\R}{R}
\newcommand{\Tu}{T}
\newcommand{\Tus}{T^{*}}
\renewcommand{\V}{V}
\newcommand{\F}{F}
\newcommand{\Z}{Z}
\newcommand{\z}{z}

\newcommand{\Addr}{\fld{Addr}}
\newcommand{\cAddr}{\fld{cAddr}}
\newcommand{\cCanTurn}{\fld{cCanTurn}}
\newcommand{\Core}{\fld{Core}}
\newcommand{\cCore}{\fld{cCore}}
\newcommand{\Dir}{\fld{Dir}}
\newcommand{\cDir}{\fld{cDir}}
\newcommand{\Drift}{\fld{Drift}}
\newcommand{\Doomed}{\fld{Doomed}}
\newcommand{\cDrift}{\fld{cDrift}}
%\renewcommand{\G}{\fld{NonAdj}}
\newcommand{\CDwell}{\cns{dwell}}
\newcommand{\NonAdj}{\fld{NonAdj}}
\newcommand{\cHold}{\fld{cHold}}
\newcommand{\Index}{\fld{Index}}
\newcommand{\Info}{\fld{Info}}
\newcommand{\cInfo}{\fld{cInfo}}
\newcommand{\Kind}{\fld{Kind}}
\newcommand{\cKind}{\fld{cKind}}
\newcommand{\cLevel}{\fld{cLevel}}
\newcommand{\Mode}{\fld{Mode}}
\newcommand{\cProg}{\fld{cProg}}
\newcommand{\Heal}{\fld{Heal}}
\newcommand{\rHeal}{\rul{Heal}}
\newcommand{\Plan}{\fld{Plan}}
\newcommand{\Rebuild}{\fld{Rebuild}}
\newcommand{\Stage}{\fld{Stage}}
\newcommand{\State}{\fld{State}}
\newcommand{\cState}{\fld{cState}}
\newcommand{\Sweep}{\fld{Sw}}
\newcommand{\cSweep}{\fld{cSw}}
\newcommand{\cWork}{\fld{cWork}}
\newcommand{\ZigDepth}{\fld{ZigDepth}}
\newcommand{\ZigDir}{\fld{ZigDir}}

%\newcommand{\Bridge}{\mathrm{Bridge}}
\newcommand{\Committing}{\mathrm{Committing}}
\newcommand{\Coordinated}{\mathrm{Coordinated}}
\newcommand{\decode}{\mathrm{decode}}
\newcommand{\dir}{\mathrm{dir}}
\newcommand{\encode}{\mathrm{encode}}
\newcommand{\front}{\mathrm{front}}
\newcommand{\Histories}{\mathrm{Histories}}
\newcommand{\Last}{\mathrm{Last}}
\newcommand{\Marking}{\mathrm{Marking}}
\newcommand{\Member}{\mathrm{Member}}
\newcommand{\Normal}{\mathrm{Normal}}
\newcommand{\patch}{\mathrm{patch}}
\newcommand{\Planning}{\mathrm{Planing}}
\newcommand{\Rebuilding}{\mathrm{Rebuilding}}
\newcommand{\score}{\mathrm{score}}
\newcommand{\Target}{\mathrm{Target}}

\newcommand{\PadLen}{\mathit{PadLen}}
\newcommand{\Interpr}{\mathit{Interpr}}

\newcommand{\Healing}{\mathrm{Healing}}
\newcommand{\start}{\mathrm{start}}
\newcommand{\state}{\mathrm{state}}
\newcommand{\Stem}{\mathrm{Stem}}
\newcommand{\tape}{\mathrm{tape}}
\newcommand{\TransferSw}{\mathrm{TransferSw}}
\newcommand{\Un}{\mathrm{Univ}}

\newcommand{\increment}[1]{#1\mathord{+}\mathord{+}}
\newcommand{\decrement}[1]{#1\mathord{-}\mathord{-}}


\newcommand{\ruAddrJmp}{\rul{AddrJmp}}
\newcommand{\Alarm}{\rul{Alarm}}
% \newcommand{\Commit}{\rul{Commit}}
\newcommand{\Comp}{\rul{Compute}}
\newcommand{\BigAlarm}{\rul{BigAlarm}}
%\newcommand{\Vacate}{\rul{Vacate}}
% \newcommand{\Mark}{\rul{Mark}}
\newcommand{\Move}{\rul{Move}}
% \newcommand{\Plan}{\rul{Plan}}
\newcommand{\ruSwing}{\rul{Swing}}
\newcommand{\Transfer}{\rul{Transfer}}
\newcommand{\UsefulComp}{\rul{UsefulComp}}
\newcommand{\WriteProgramBit}{\rul{WriteProgramBit}}
\newcommand{\Zigzag}{\rul{Zigzag}}

\newcommand{\mrk}{\mathrm{mrk}}
\newcommand{\K}{K}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Zg}{\mathcal{Z}_g}

\newcommand{\cns}[1]{c_{\textrm{\upshape #1}}}
\newcommand{\CAtt}{\cns{attack}}
\newcommand{\CRebuild}{\cns{rebuild}}
\newcommand{\CCleanEsc}{\cns{clean-esc}}
\newcommand{\CEsc}{\cns{escape}}
\newcommand{\CCleanS}{\cns{clean-s}}
\newcommand{\CCleanT}{\cns{clean-t}}
%\newcommand{\CDepth}[1]{\cns{depth-#1}}
\newcommand{\cIncr}{\cns{incr}}
\newcommand{\CSpill}{\cns{spill}}

\newcommand{\Left}{\text{left}}
\newcommand{\Right}{\text{right}}

\begin{document}

\title{A reliable Turing machine}
% Why do I need this?  Some people get the title bookmarked even without this.
\bookmark[page=1,level=0]{A reliable Turing machine}

\author{Ilir \c{C}apuni \and Peter G\'acs}
% \\ Boston University
% \\ gacs@bu.edu

\maketitle

\begin{abstract}
The title says it.
\end{abstract}

\tableofcontents*

\section{Introduction}

\subsection{To be written}

\subsection{Turing machines}\label{sec:TM}

Our contribution uses one of the standard definitions of a Turing
machine.

    A Turing machine \( M \) is defined by a tuple
        \begin{align*}
             (\Gamma, \Sigma,\tau, q_{\start},F).
        \end{align*}
    Here, \( \Gamma \) is a finite set of \df{states},
    \( \Sigma \) is a finite alphabet, and
        \begin{align*}
             \tau\colon\Sigma\times \Gamma\to \Sigma\times\Gamma\times\{-1,1\}
        \end{align*}
    is the transition function.
The tape alphabet \( \Sigma \) contains at least the distinguished
symbols \( \blank,0,1 \) where \( \blank \) is called the \df{blank symbol}.
The state \( q_{\start} \) is called the \df{starting state}, and
there is a set \( F \) of \df{final states}.

    The tape is blank at all but finitely many positions.

    A \df{configuration} is a tuple
        \begin{align*}
             (q,A,\h),
        \end{align*}
    where \( q\in\Gamma \) is the \df{current state}, 
\( \h\in\bbZ \) is the current \df{head position}, or \df{observed cell},
and \( A\in\Sigma^{\bbZ} \) is the \df{tape content}: 
at position \( p \), the tape contains the symbol \( A(p) \).
If \( \xi=(q,A,\h) \) is a configuration then we will write
        \begin{align}\label{eq:config-1}
             \xi.\state=q, \quad \xi.\tape=A,\quad \xi.\pos=\h.
        \end{align}
    Here, \( A \) is also called the \df{tape configuration}.
    The cell at position \( \h \) is the  \df{current cell}.
Though the tape alphabet may contain
non-binary symbols, we will restrict input and output to binary.

    The transition function \( \tau \) tells us how to compute the next
    configuration from the present one.
    When the machine is in a state \( q \), at tape position \( \h \), and
    observes tape cell with content \( a \), then denoting
         \begin{align*}
           (a',q',j)=\tau(a,q),
         \end{align*}
    it will change the state to \( q' \), change the
    tape content at position \( \h \) to \( a' \), and move to tape position to \( \h+j \).
    For \( q\in F \) we have \( a'=a \), \( q'\in F \).


\begin{definition}[Fault]\label{def:fault}
    We say that a \df{fault} occurs at time \( t \) if the output \( (a',q',j) \) of the
    transition function at this time is replaced with some other value
    (which is then used to compute the next configuration).
\end{definition}


\subsection{Codes, and the result}

For fault-tolerant computation, some redundant coding of the information is needed.

\begin{definition}[Codes]\label{def:codes}
    Let \( \Sigma_{1},\Sigma_{2} \) be two finite alphabets.
    A \df{block code} is given by a positive integer \( \Q \)---called
    the \df{block size}---and a pair of functions
    \begin{align*}
            \psi_{*} :\Sigma_{2}\to\Sigma_{1}^{\Q},
            \quad
            \psi^{*}:\Sigma_{1}^{\Q}\to\Sigma_{2}
    \end{align*}
    with the property \( \psi^{*}(\psi_{*}(x))=x \).
It is extended to strings by encoding each letter individually:
\( \psi_{*}(x_{1},\dots,x_{n})=\psi_{*}(x_{1})\dotsm\psi_{*}(x_{n}) \).
\end{definition}

For ease of spelling out a result, let us consider only computations whose outcome
is a single symbol, at tape position 0.

\begin{theorem}\label{thm:main-main}
There is a Turing machine \( M_{1} \) with a 
function \( a\mapsto a.\Output \) defined on its alphabet, 
such that
for any Turing machine \( G \) with alphabet \( \Sigma \) and state set \( \Gamma \)
there are \( 0\le\varepsilon <1 \) and \( \alpha_{1},\alpha_{2}>0 \) 
with the following property.

For each input length \( n=\abs{x} \) a block code
\( (\varphi_{*}, \varphi^{*}) \) of block size \( \Q=O((\log n)^{\alpha_{1}}) \) can be constructed 
such that the following holds.

Let \( M_{1} \) start its work from its initial state,
and the initial tape configuration \( \xi=(q_{\start},\varphi_{*}(x),0) \).
Assume further that
during its operation, faults occur independently at random
with probabilities \( \le \varepsilon \).

Suppose that on input \( x \) machine \( G \) reaches a final state at time \( t \) and writes
value \( y \) at position 0 of the tape.
Then denoting by \( \eta(u) \) the configuration machine \( M_{1} \) at time \( u \),
at any time \( t' \) after
 \begin{align*}
   t\cdot (\log t)^{\alpha_{2}},
 \end{align*}
we have \( \eta(t').\tape(0).\Output= y \)
with probability at least \( 1 - O(\varepsilon) \).
\end{theorem}

We emphasize that the actual
code \( \varphi \) of the construction will depend on \( n \) only in a simple way:
it will be the ``concatenation'' of one and the same fixed-size
code with itself, \( O(\log\log n) \) times.


\section{Overview of the construction}

A Turing machine that simulates ``reliably'' any other
Turing machine even when it is subjected to isolated bursts of faults of constant size,
is given in~\cite{burstyTuring13}.
By \df{reliably} we mean that the 
simulated computation can be decoded from the history
of the simulating machine despite occasional damages.


\subsection{Isolated bursts of faults}\label{sec:bursts}

Let us give a brief overview of a machine \( M_{1} \) that
can withstand isolated bursts of faults, as most of its construction will be reused
in the probabilistic setting.

Let us break up the task of error correction into several 
problems to be solved.
The solution of one problem gives rise to another problem one, 
but the process converges.
\begin{description}
\item[Redundant information] The tape information of the simulated Turing machine
will be stored in a redundant form, more precisely in the form of a block code.
\item[Redundant processing] The block code will be decoded, the retrieved information 
will be processed, and the result recorded.
To carry out all this in a way that limits the propagation of faults, the tape will be split
into tracks that can be handled separately, and the major processing steps will be 
carried out three times within one work period.
\item[Local repair] All the above process must be able to recover from a local burst of faults.
For this, it is organized into a rigid, locally checkable structure
with the help of local addresses, and some other tools like sweeps and 
short switchbacks (zigzags).
The major tool of local correction, the local healing procedure, turns out to be the most
complex part of the construction.
\item[Disturbed local repair] A careful organization of the healing procedure
makes sure that even if a new burst interrupts it (or jumps into its middle),
soon one or two new invocations of it will finish the job (whenever needed).
\end{description}

Here is some more detail.
Each tape cell of the simulated machine \( M_{2} \) will be represented by a block of
size \( \Q \) of the simulating machine \( M_{1} \) called a \df{colony}.
Each step of \( M_{2} \) will be simulated by a computation of \( M_{1} \) called
a \df{work period}.
During this time, the head of \( M_{1} \) makes a number of sweeps over the
current colony, decodes the represented cell symbol and state,
computes the new state, and transfers the necessary information to the 
neighbor colony that corresponds to the new position of the head of \( M_{2} \).

In order to protect information from the propagation of errors,
the tape of \( M_{1} \) is subdivided into \df{tracks}: each track corresponds to a 
\df{field} of a cell symbol of \( M_{1} \) viewed as a data record.
Each stage of computation will be repeated three times.
The results will be stored in separate tracks, and a final cell-by-cell majority vote
will recover the result of the work period from them.

All this organization is controlled by a few key fields, for example a field
called \( \cAddr \) showing the position of each cell in the colony, and a field
\( \cSweep \) showing the last sweep of the computation (along with its direction)
that has been performed already.
The technically most challenging part of the construction is the protection of this
control information from bursts.

For example, a burst can reverse the head in the middle of a sweep.
Our goal is that such structural disruptions be discovered locally, so
we cannot allow the head to go far from the place where it was turned back.
Therefore the head's movement will not be straight even during a single
sweep: it will make frequent zigzags.
This will trigger the healing procedure if for example a turn-back is detected.

It is a significant challenge that the healing procedure
itself can be interrupted (or started) by a burst.

\begin{remark}
This description uses some words in an informal way.
Some of these will get precise definition later.
For example, the word \df{colony} will have at least two formal definitions.
From the point of view of the program (transition function), it is just a 
sequence of addresses from \( 0 \) to \( \Q-1 \).
From the point of view of the analysis of the behavior of the machine, it is a sequence of
actual adjacent tape cells with the address field having theses values.
\end{remark}


\subsection{Hierarchical construction}\label{sec:hier}

In order to build a machine that can resist faults 
occurring independently of each other with some small probability,
we take the approach suggested in~\cite{Kurd78},
and implemented in~\cite{Gacs1dim86} and~\cite{GacsSorg01}
for the case of one-dimensional cellular automata, with some ideas
from the tiling application of~\cite{DurandRomashShenTiling12}.
We will build a \df{hierarchy of simulations}:
machine \( M_{1} \) simulates machine \( M_{2} \) which 
simulates machine \( M_3 \), and so on.
For simplicity we assume all these machines have the same program,
and all simulations have the same block size.

One cell of machine \( M_3 \) is simulated by one colony of machine \( M_{2} \).
Correspondingly, one cell of \( M_{2} \) is simulated by
one colony of machine \( M_{1} \).
So one cell of \( M_3 \) is simulated by \( \Q^{2} \) cells of \( M_{1} \).
Further, one step of machine \( M_3 \) is simulated by one
work period of \( M_{2} \) of, say, \( O(\Q^{2}) \) steps.
One step of \( M_{2} \) is simulated by one work period of \( M_{1} \),
so one step of \( M_3 \) is simulated by \( O(\Q^{4}) \) steps of \( M_{1} \).

Per construction, machine \( M_{1} \) can withstand
bursts of faults whose size is \( \le \beta \) for some constant parameter \( \beta \), that
are separated by some \( O(\Q^{2}) \) fault-free steps.
Machines \( M_{2} \), \( M_3 \), \( \dots \) have the same program, so it
would be natural to expect that machine
\( M_{1} \) can withstand also some \emph{additional}, larger bursts
of size \( \le \beta \Q \) if those are separated by at least \( O(\Q^{4}) \) steps.

But a new obstacle arises.
On the first level, damage caused by a big burst spans several colonies.
The repair mechanism of machine \( M_{1} \) outlined in Section~\ref{sec:bursts} above 
is too local to recover from such extensive damage.
This cannot be allowed, since then the whole hierarchy would stop working.
So we add a new mechanism to \( M_{1} \) that, more modestly,
will just try to restore a large enough portion of the
tape, so it can go on with the simulation of \( M_{2} \), even if all 
original information was lost.
For this, \( M_{1} \) may need to rewrite an area as large as a few colonies.

This will enable the low-level healing procedure of 
machine \( M_{2} \) to restore eventually a higher-level order.

All machines above \( M_{1} \) in the hierarchy are
``virtual'': the only hardware in the construction is machine \( M_{1} \).
Moreover, they will not be ordinary Turing machines, but \df{generalized} ones,
with some new features that are not needed on the lowest level but seem necessary
in a simulated Turing machine: for example they
allow a positive distance between neighboring tape cells.

A tricky issue is ``forced self-simulation'': while we are constructing machine \( M_{1} \)
we want to give it the feature that it will simulate a machine \( M_{2} \) that
works just like \( M_{1} \).
The ``forced'' feature means that this simulation should
work without any written program (that could be corrupted).

This will be achieved by
a construction similar to the proof of the Kleene's fixed-point 
theorem (also called recursion theorem).
We first fix a (simple) programming language to express the transition
function of a Turing machine.
We write an interpreter for it in this same language (just as compilers for the 
C language are sometimes written in C).
The program of the transition function of \( M_{2} \)
(essentially the same as that of \( M_{1} \))
in this language, is a string that will be
``hard-wired'' into the transition function of \( M_{1} \), 
so that \( M_{1} \), at the start of each work period, can write
it on a working track of the base colony.
Then the work period will interpret it, 
applying it to the data found there, resulting
in the simulation of \( M_{2} \).

In this way, an infinite sequence of simulations arises, in order
to withstand larger and larger but sparser and sparser bursts of faults.

Since the \( M_{1} \) uses the universal interpreter, which in turns
simulates the same program, it is natural to ask
how  machine \( M_{1} \) simulates a given Turing machine \( G \) that does the 
actual useful computation?
For this task, we set aside a separate track 
on each machine \( M_{i} \), on which some arbitrary other Turing machine can be
simulated.
The higher the level of the machine \( M_{k} \) that performs this
``side-simulation'', the higher the reliability.
Thus, only the simulations \( M_{k}\to M_{k+1} \) are forced, without program
(that is a hard-wired program):
the side simulations can rely on written programs, since the firm
structure in the hierarchy \( M_{1},M_{2},\dots \) will support them reliably.



\subsection{From combinatorial to probabilistic noise}

The construction we gave in the previous subsection
was related to increasing bursts that are not frequent.
In essence, that noise model is combinatorial.
To deal with probabilistic noise combinatorially,
we stratify the set of faulty times \( \Noise \) as follows.
For a series of parameters \( \beta_{k}, V_{k} \),
we first remove ``isolated bursts'' of type \( \pair{\beta_{1}}{V_{1}} \) of elements of this set.
(The notion of ``isolated bursts'' of type \( \pair{\beta}{V} \)
will be defined appropriately.)
Then, we remove isolated bursts of type \( \pair{\beta_{2}}{V_{2}} \) from the remaining set,
and so on.
It will be shown that with the appropriate choice of parameters, with probability 1,
eventually nothing is left over from the set \( \Noise \).

A composition of two reliable simulations is even more reliable.
We will see that a sufficiently large hierarchy of such
simulations resists probabilistic noise.


\subsection{Difficulties}\label{sec:novelties}

Let us spell-out some of the main problems that the paper deals with, 
and some general ways they will be solved or avoided.
Some more specific problems will be pointed out later, along with their solution.



\begin{description}

\item[Non-aligned colonies] A large burst of \( M_{1} \) can modify the order of
entire colonies or create new ones with gaps between them.

To overcome this problem conceptually, we 
introduce the notion of a \df{generalized Turing machine}
allowing for non-adjacent cells.
Each such machine has a parameter \( \B \) called the \df{cell body size}.
The cell body size of a Turing machine in Section~\ref{sec:TM} would still remain
\( 1 \).

    \item[No structure] What to do when the head is in a middle of an empty area
       where no structure exists?
To ensure reliable passage across such areas,
we will try to keep everything filled with cells, even if these are
not part of the main computation.

\item[Clean areas]
    Noise can create areas over
which the predictability of the simulated machine is limited.
In these areas the (on this level) invisible structure
of the underlying simulation may be destroyed.
These areas should not simply be considered blank, since
blankness implies predictable behavior.
We could call these areas ``dirty'', but we prefer to use only a positive terminology,
and talk about the complement, namely \df{clean} intervals.
There is a danger in using the negative terminology, since the transition properties will allow
to make only be from cleanness, not from ``dirtiness''.
The following example shows that 
when the head comes out of a clean area, it can be
``sucked in'', and its state can change.


\item[Extending cleanness]
 The definition of the generalized
Turing machine stipulates a certain ``magical'' extension of clean intervals,
and also a magical appearance of a clean ``hole'' around the head 
whenever it passes a certain amount of cumulative time in a small interval.
(Of course, this property needs to be implemented in simulation, which is one of the
main burdens of the actual construction.)
Once an area is cleaned, it will be re-populated with new cells.
Their content is not important, what matters is the restoration of predictability.

\item[Rebuilding] If local repair fails, a special rule will be invoked that reorganizes a
larger part of the tape (of the size of a few colonies instead of only a few cells).
This is the mechanism enabling the ``magical'' restoration on the next level.

\end{description}

\begin{example}[Uncleanness]
Consider two levels of simulation as outlined in Section~\ref{sec:hier}: 
machine \( M_{1} \) simulates \( M_{2} \) which simulates \( M_{3} \).
The tape of \( M_{1} \) is subdivided into colonies of size \( \Q_{1} \).
The tape content of the 
current colony of level 1 represents not only the content of the currently
observed tape cell of machine \( M_{2} \), but also its state.

A burst on level 1 has size \( O(1) \), while a burst on level 2 has size \( O(\Q_{1}) \).
Suppose that such a burst happens at some time, while the head was
performing a simulation in colony \( C(x)=x+\lint{0}{\Q_{1}} \),
and just intending to leave it on the \emph{left} for a long time.
Let a burst \( \mathbf{b}_{1} \) of level 2 change the right neighbor colony \( C(x+\Q_{1}) \) 
and the state represented by it 
completely, into the last stage of a work period with the 
head on the verge of leaving it on the \emph{right}.
After that, let the burst deposit the head onto \( C(x) \) again, letting it finish
its simulation and continue on the \emph{left}, for a very long time.

Much later, the head returns to \( C(x) \), and would still not visit \( C(x+\Q_{1}) \) by
design.
But when it is at the right edge of \( C(x) \),
a burst \( \mathbf{b}_{2} \) of level 1 (which can happen essentially in every work period of level 1)
moves the head over to the left edge of \( C(x+\Q_{1}) \).
Here it will be captured, simulating the cell \( x \) of \( M_{2} \) from a cell and machine state 
represented by colony \( C(x+\Q_{1}) \) as set up by 
the big burst \( \mathbf{b}_{1} \) earlier.

From the point of view of the simulated machine \( M_{2} \) (which does not see bursts
of level 1), the head came close to the unclean area created by burst \( \mathbf{b}_{1} \), and then 
the state changed: the head started moving right, in a new state.
\end{example}


\subsection{A shortcut solution}

A fault-tolerant one-dimensional cellular automaton is constructed
in~\cite{GacsSorg01}.
If our Turing machine could just simulate such an automaton, it would become
fault-tolerant.
This can indeed almost be done provided that the size of the computation is known in advance.
The cellular automaton can be made finite, and we could define
a ``kind of'' Turing machine with a \emph{circular tape} simulating it.
But this solution requires input size-dependent hardware.

It seems difficult to define a fault-tolerant sweeping 
behavior on a regular Turing machine needed to 
simulate cellular automaton, without recreating
an entire hierarchical construction---as we are doing here.


\section{Notation}\label{sec:notation}

Most notational conventions given here are common; some other ones will
also be useful.

\begin{description}

\item [Natural numbers and integers] 
By \( \bbZ \) we denote the set of integers.
\begin{align*}
   \bbZ_{>0}&=\setOf{x}{x\in \bbZ,\;  x>0}, \\
   \bbZ_{\ge 0}&=\bbN=\setOf{x}{x\in \bbZ,\;  x\ge 0}.
\end{align*}

\item [Intervals]
We use the standard notation for intervals:
\begin{align*}
   \clint{a}{b}&=\setOf{x}{a\le x \le b},\quad \lint{a}{b}=\setOf{x}{a\le x < b}, \\
   \rint{a}{b}&=\setOf{x}{a< x \le b}, \quad  \opint{a}{b}=\setOf{x}{a< x < b}.
\end{align*}
We will also write \( \lint{a}{b} \) in place of \( \lint{a}{b}\cap \bbZ \), 
whenever this leads to no confusion.
Instead of \( \lint{x+a}{x+b} \), sometimes we will write 
\begin{align*}x + \lint{a}{b}.\end{align*}

\item [Ordered pairs]
Ordered pairs are also denoted by \( \pair{a}{b} \),
but it will be clear from the context whether we are
referring to an ordered pair or open interval.

\item [Comparing the order of a number and an interval]
For a given number \( x \) and interval \( I \), we
write
\begin{align*} x \ge I \end{align*}
if for every \( y\in I \),  \( x \ge y \).

\item [Distance]
The distance between two real numbers \( x \) and \( y \) is defined
in a usual way:
\begin{align*}
    d(x,y)= \abs{x-y}.
\end{align*}

The \df{distance of a point \( x \) from interval \( I \)}  is
\begin{align*}
    d(x,I)= \min_{y\in I}d(x,y).
\end{align*}

\item [Ball, neighborhood, ring, stripe]
A \df{ball of radius \( r>0 \), centered at \( x \)} is
\begin{align*}
    B(x,r)= \setOf{y}{d(x,y)\le r}.
\end{align*}
An \df{\( r \)-neighborhood of interval } \( I \) is
\begin{align*}
    \setOf{x}{d(x,I)\le r}.
\end{align*}
An \df{\( r \)-ring} around interval \( I \) is
\begin{align*}
    \setOf{x}{d(x,I)\le r \txt{ and } x \notin I}.
\end{align*}
An \df{\( r \)-stripe to the right of interval \( I \)} is
\begin{align*}
    \setOf{x}{d(x,I)\le r \txt{ and } x \notin I \txt{ and } x>I}.
\end{align*}

\item[Logarithms] Unless specified differently,
the base of logarithms throughout this work is 2.

\end{description}


\section{Specifying a Turing machine}\label{sec:specifying}

Let us introduce the tools allowing to describe the reliable Turing machine.

\subsection{Universal Turing machine}\label{sec:UTM}

We will describe our construction in terms of
universal Turing machines,
operating on binary strings as inputs and outputs.
We define universal Turing machines in a way that allows
for rather general ``programs''.

 \begin{definition}[Standard pairing]
For a (possibly empty) binary string \( x=x(1)\dotsm x(n) \) let us introduce the map
 \[
   \ang{x} = 0^{\abs{x}}1 x,
 \]
Now we encode pairs, triples, and so on, of binary strings as follows:
 \begin{align*}
        \ang{s,t} &=\ang{s}t,
\\ \ang{s,t,u} &= \ang{\ang{s,t},u},
 \end{align*}
and so on.

From now on, we will assume that our alphabets \( \Sigma \), \( \Gamma \)
are of the form \( \Sigma=\{0,1\}^{s} \), \( \Gamma=\{0,1\}^{g} \), that is 
our tape symbols and machine states are viewed as binary strings of a certain length.
Also, if we write \( \ang{i,u} \) where \( i \) is some number, it is understood
that the number \( i \) is represented in a standard way by a binary string.
\end{definition}

\begin{definition}[Computation result, universal machine]
 Assume that a Turing machine \( M \) starting on binary \( x \),
 at some time \( t \)
 arrives at the first time at some final state.
 Then we look at the longest (possibly empty)
 binary string to be found starting at position
 0 on the tape, and call it the \df{computation result} \( M(x) \).
We will write
 \begin{align*}
   M(x,y)=M(\ang{x,y}),\quad M(x,y,z)=M(\ang{x,y,z}),
 \end{align*}
and so on.

A Turing machine \( U \) is called \df{universal} 
among Turing machines with
binary inputs and outputs, if for every Turing machine \( M \),
there is a binary string \( p_{M} \) such that for all \( x \) we have
\( U(p_{M},x)=M(x) \).
(This equality also means that the computation denoted on the left-hand side 
reaches a final state if and only if the computation on the right-hand side does.)
\end{definition}

Let us introduce a special kind of universal Turing machines, to be
used in expressing the transition functions of other Turing machines.
These are just the Turing machines for which the so-called \( s_{mn} \) theorem
of recursion theory holds with \( s(x,y)=\ang{x,y} \).

\begin{definition}[Flexible universal Turing machine]\label{def:univ-TM}
A universal Turing machine will be called \df{flexible} if 
whenever \( p \) has the form \( p=\ang{p',p''} \) then
\begin{align*}
 U(p,x)= U(p',\ang{p'',x}).
 \end{align*}
Even if \( x \) has the form \(x =\ang{x',x''} \), this definition chooses
\( U(p',\ang{p'',x}) \) over \( U(\ang{p,x'},x'') \), that is starts with 
parsing the first argument
(this process converges, since \( x \) is  shorter than \( \ang{x,y} \)).
\end{definition}

It is easy to see that there are flexible universal Turing machines.
On input \( \ang{p,x} \),
a flexible machine first checks whether its ``program'' \( p \) 
has the form \( p=\ang{p',p''} \).
If yes, then it applies \( p' \) to the pair \( \ang{p'',x} \).
(Otherwise it just applies \( p \) to \( x \).)

\begin{definition}[Transition program]
  Consider an arbitrary Turing machine \( M \) with state set \( \Gamma \), alphabet
\( \Sigma \), and transition function \( \tau \).
A binary string \( \pi \) will be called a \df{transition program} of \( M \) if
whenever \( \tau(a,q)=(a',q',j) \) we have
 \begin{align*}
 U(\pi,a,q)=\ang{a',q',j}.
 \end{align*}
We will also require that the computation induced by the program makes
\( O(\abs{p}+\abs{a}+\abs{q}) \) left-right turns, over a length tape \( O(\abs{p}+\abs{a}+\abs{q}) \).
\end{definition}

The transition program just provides a way to compute
the (local) transition function of \( M \) by the universal machine,
it does not organize the rest of the simulation.

\begin{remark}
 In the construction of universal Turing machines provided by the textbooks
(though not in the original one given by Turing), the program is generally a string
encoding a table for the transition
function \( \tau \) of the simulated machine \( M \).
Other types of program are imaginable: some simple transition functions can
have much simpler programs.
However, our fixed machine is good enough (similarly to the optimal machine
for Kolmogorov complexity).
If some machine \( U' \) simulates \( M \) via a
very simple program \( q \), then
 \begin{align*}
     M(x)=U'(q,x) = U(p_{U'},\ang{q,x}) = U(\ang{p_{U'},q},x),
 \end{align*}
so \( U \) simulates this computation via the program \( \ang{p_{U'},q} \).
\end{remark}

\subsection{Rule language}\label{sec:language}

In what follows we will describe the generalized Turing machines \( M_{k} \) for all \( k \).
They are all similar, differing only in the parameter \( k \); the most important activity
of \( M_{k} \) is to simulate \( M_{k+1} \).
The description will be uniform, except for the parameter \( k \).
We will denote therefore \( M_{k} \) simply by \( M \), and \( M_{k+1} \)  by \( M^{*} \).
Similarly we will denote the block size \( \Q_{k} \) of the block code of the 
simulation simply by \( \Q \).

Instead of writing a huge table describing the transition function \( \tau_{k}=\tau \),
we present the transition function as a set of \df{rules}.
It will be then possible to write one \emph{interpreter} program that carries
out these rules; that program can be written for some fixed flexible 
universal machine \( \Un \).

Each rule consists of some (nested) conditional statements,
similar to the ones seen in an ordinary program:
 ``\textbf{if} \textit{condition} \textbf{then} \textit{instruction}
\textbf{else} \textit{instruction}'', 
where the condition
is testing values of some fields of the state and the observed cell, and
the instruction can either be elementary, or itself a conditional statement. 
The elementary instructions are an \df{assignment} of a value to a field
of the state or cell symbol, or a command to move the head.
Rules can call other rules, but these calls will never form a cycle.
Calling other rules is just a shorthand for nested conditions.

Even though rules are written like procedures of a program,
they describe a single transition.
When several consecutive statements are given, then they
%(almost always)
change different fields of the state or
cell symbol, so they can be executed simultaneously.
% Otherwise and in general, even if a field is updated in
% some previous statement, in all following statements that use
% this field, its old value is considered.

Assignment of value \( x \) to a field \( y \) of the state or cell symbol will
be denoted by \( y \gets x \).
We will also use some conventions introduced by the C language:
namely,
\( x\gets x+1 \) and \( x\gets x-1 \) are abbreviated to \( \increment{x} \) and
\( \decrement{x} \) respectively.

Rules can also have parameters, like \( \ruSwing(a,b,u,v) \).
Since each rule is called only a constant number of times in the whole program,
the parametrized rule can be simply seen as a shorthand.

Mostly we will describe
the rules using plain English, but it should always be clear that they
are translatable into such rules.


% \section{Fields}\label{sec:fields}

\begin{sloppypar}
For the machine \( M \) we are constructing, each state will 
be a tuple \( q=(q_{1},q_{2},\dots,q_{k}) \),
where the individual elements of the tuple will be called \df{fields}, and will
have symbolic names.
For example, we will have fields \( \Addr \) and \( \Drift \),
and may write \( q_{1} \) as \( q.\Addr \) or just \( \Addr \), 
\( q_{2} \) as \( q.\Drift \) or \( \Drift \), and so on.
\end{sloppypar}

Similarly for tape symbols.
In order to distinguish fields of tape symbols from fields of the state,
we will always start the name of a field of the tape symbols by the letter \( c \).
We have seen already one example of this, the field \( \cDir \) of tape symbols
in the definition of a generalized Turing machine.

In what follows we describe some of the most important fields we will use;
others will be introduced later.

A properly formatted configuration of \( M \) splits the tape into blocks of \( \Q \)
consecutive cells called \df{colonies}.
One colony of the tape of the simulating
machine represents one cell of the simulated machine.
The colony that corresponds to the cell that the
simulated machine is scanning is called the \df{base colony}
(a precise definition will be based on the actual history of the work of \( M \)).
Once the the direction of the simulated head movement, called the
\df{drift}, is known, the union of the base colony with the target colony in
the direction of the drift (with a possible ``bridge'' betweend them)
is called the \df{workspace} (this notion will need to be defined more carefully later).

There will be a field of the state called the \df{mode}:
 \begin{align*}
   \Mode\in\{ \Normal,\Healing, \Rebuilding \}.
 \end{align*}
In the \df{normal} mode, the machine is engaged 
in the regular business of simulation.
The \df{healing} mode tries to correct some local fault due to a couple of neighboring
bursts, while the \df{rebuilding} mode attempts to restore the colony structure
on the scale of a couple of colonies.

The content of each cell of the tape of \( M \) also has several fields.
Some of these have names identical to fields of the state.
In describing the transition rule of \( M \) we will write, for example,
\( q.\Addr \) simply as \( \Addr \), and for the corresponding field of the
observed cell symbol \( a \) we will write \( a.\cInfo \), or just \( \cInfo \).
The array of values of the same field of the cells will be called a \df{track}.
Thus, we will talk about the \( \cHold \) track of the tape, corresponding to the
\( \cHold \) field of cells.

% The basic fields of the state and of cells are listed in Section~\ref{sec:fields-list}
% with some hints of
% their function (this does not replace our later definition of the transition function).
Each field of a cell has also a possible value
\( \emptyset \) whose approximate meaning is ``undefined''.

Some fields and parameters are important enough to introduce them right away.
The 
\begin{align*}
   \cInfo,\cState
 \end{align*}
track of a colony of \( M \)
contain the strings that encode the content of the simulated cell of \( M^{*} \) and
its simulated state respectively.
\begin{align*}
 \cProg
 \end{align*}
track stores the program of \( M^{*} \), in an appropriate form 
to be interpreted by the simulation.
The field 
 \begin{align*}
  \cAddr
 \end{align*}
of the cell shows the position of the cell in its colony:
it takes values in \( \lint{-\Q}{2\Q} \), since the addresses in a bridge (see later)
will be continuations of those in the colony (which run from \( 0 \) to \( \Q-1 \)).
There is a corresponding \( \Addr \) field of the state.

The direction in \( \{-1,1\} \) in which the simulated head moves will be denoted by
 \begin{align*}
   \Drift.
 \end{align*}
There is a corresponding field \( \cDrift \).
The number of the last sweep of the work period will depend on the drift \( d \), 
and will be denoted by 
\begin{align}\label{eq:Last}
   \Last(d).
 \end{align}
The colony along with the adjacent cells that continue its addresses will be called
an \df{extended colony}.
A colony can only be extended in the direction of the drift.
The
 \begin{align*}
 \Sweep
 \end{align*}
field counts the sweeps that the head makes during the work period.
There is a corresponding \( \cSweep \) field in the cell.
In calculating parameters, we will make use of  
\begin{align}\label{eq:V}
   \V=\max(\Last(-1),\Last(1)).
 \end{align}
Cells will be designated as belonging to a number of possible \df{kinds}, signaled by the
field 
\begin{align*}
     \cKind
 \end{align*}
with values
       \begin{align*}
          \Member, \Target, \Vacant, \Stem.
       \end{align*}
Here is a description of the role of these cell kinds.
Normally, cells will have the kind \( \Member \).
During the simulation, however, the elements of the colony that is to become
the next base colony, will be made to have the kind \( \Target \).
If the neighbor colony 
is not adjacent, then the base colony will be extended to it by 
a \df{bridge} of up to \( \Q-1 \) adjacent cells.
Bridge cells are member cells with addresses outside \( \lint{0}{\Q} \).
Though they have the kind \( \Member \), they will frequently be treated
differently from the other member cells.

The stem kind is sometimes convenient when some cells need to be created temporarily
that do not participate in any known colony structure.
We will also try to keep all areas between colonies filled with (not necessarily adjacent)
stem cells.
For example the computation may find that a colony does not properly encode
a tape cell by the required error-correcting code.
Then we want to ``kill'' the whole colony.
This will happen by turning the kind of each of its cell to \( \Stem \).
% The reason is that even the ``vacuum'' taking the place of the colony needs some
% structure allowing the head to traverse it later in a reliable way.

% It is useful to group some of the fields especially important for the simulation structure into
% a ``super'' field,
%     \begin{align}\label{eq:Core}
%        \Core=(\Addr, \Sweep, \Drift, \Kind), 
%         \cCore=(\cAddr, \cSweep, \cDrift, \cKind). %, \cVisSw).
%     \end{align}
%(where the field \( \cVisSw \) will also be explained later).

%     \item \( \Af \) field of a cell will be set to true if
%     this cell belongs to a colony that ``represents'' a cell
%     that wants to become ``vacant''.
%     (We will explain  this notion later.)

  % The fields used in healing mode are all collected as subfields of the field
  %   \( \Heal \) of the state.
  %   They will be introduced in the definition of the healing rule.

During healing, some special fields of the state and cell are used, they will be subfields of 
the field
 \begin{align}\label{eq:Heal}
   \Heal.
 \end{align} 
In particular, there will be a \( \Heal.\Sweep \) field.

Healing only changes the cells that need to be changed.
But during rebuilding, the tape will also be used.
we will work with subfields of the field \( \Rebuild \), 
and a cell will be called \df{marked for rebuilding} if \( \Rebuild.\cSweep\ne 0 \).


\section{Exploiting structure in the noise}\label{sec:noise}

\subsection{Sparsity}\label{sec:sparsity}
Let us introduce a technique connecting the combinatorial and probabilistic
noise models.

\begin{definition}[Centered rectangles, isolation]
Let \( \vek{r}=\pair{r_{1}}{r_{2}} \), \( r_{1}, r_{2}\ge 0 \),
be a two-dimensional nonnegative vector.
An \df{rectangle} of radius \( \vek{r} \) \df{centered} at \( \vek{x} \) is
\begin{align}\label{eq:ball1}
  B(\vek{x},\vek{r}) = \setOf{\vek{y}}{\abs{y_{i} - x_{i}} \le r_{i}, i=1,2}.
\end{align}  
Let \( E\subseteq \bbZ^{2} \) be a two-dimensional set.
A point \( \vek{x} \) of \( E \) is \df{\( \pair{\vek{r}}{\vek{r}^{*}} \)-isolated} if
\begin{align*}
  E \cap B(\vek{x},\vek{r}^{*})\subseteq B(\vek{x}, \vek{r}).
 \end{align*}
Set \( E \) is \df{\( (\vek{r}, \vek{r}^{*}) \)-sparse} 
if \( D(E, \vek{r}, \vek{r}^{*})=\emptyset \), that is 
it consists of \( (\vek{r}, \vek{r^{*}}) \)-isolated points.
Let
\begin{align}
  D(E,\vek{r}, \vek{r}^{*}) =
     \setOf{\vek{x}\in E}{\vek{x} \txt{ is not } (\vek{r}, \vek{r}^{*})\txt{-isolated
  from } E}.
\end{align}
\end{definition}

\begin{definition}[Sparsity]\label{def:sparsity}
Let
\begin{align}\label{eq:beta}
 \beta\ge 9, \gamma\gg\beta
 \end{align}
be constants (we will choose \( \gamma \) sufficiently large as the proof requires), and let 
\( 0<\B_{1}<\B_{2}<\dotsm \), \( \Tu_{1}<\Tu_{2}<\dotsm \), 
% \( 1\le\gamma_{1}\le\gamma_{2}\le\dotsm \) 
be sequences of positive integers to be fixed later.

For a two-dimensional set \( E \), let \( E^{(1)} = E \).
For \( k>1 \) we define recursively:
\begin{align}\label{eq:noise^k}
    E^{(k+1)} = D(E^{(k)}, \beta\pair{\B_{k}}{\Tu_{k}}, \gamma\pair{\B_{k+1}}{\Tu_{k+1}}).
\end{align}
Set \( E^{(k)} \) is called the \df{\( k \)-th residue} of \( E \).
It is \( k \)-\df{sparse} if \( E^{(k+1)}=\emptyset \).
It is simply \df{sparse} if \( \bigcap_{k}E^{(k)}=\emptyset \).

When \( E=E^{(k)} \) and \( k \) is known
then we will denote \( E^{(k+1)} \) simply by \( E^{*} \).
\end{definition}

The following lemma connects the above defined sparsity notions to the requirement
of small fault probability.
It is formulated somewhat redundantly, for easier application.

\begin{lemma}[Sparsity]\label{lem:sparsiness}
Let \( \Q_{k} = \B_{k+1}/\B_{k} \), \( \U_{k} = \Tu_{k+1}/\Tu_{k} \), and
\begin{align}\label{eq:assumption}
  \lim_{k\rightarrow\infty}\frac{\log(\U_{k} \Q_{k})}{1.5^k}=0.
%  \lim_{k\rightarrow\infty}\frac{\log(\gamma_{k}\U_{k} \Q_{k})}{1.5^k}=0.
\end{align}
For sufficiently small \( \varepsilon \), for every \( k\ge 1 \) the following holds.
Let \( E\subseteq \bbZ\times \bbZ_{\ge 0} \)
be a random set with the property that each pair \( \pair{p}{t} \) belongs to \( E \)
independently from the other ones with probability \( \le \varepsilon \).

Then for each point \( \vek{x} \)  and each \( k \),
 \begin{align*}
   \Pbof{B(\vek{x},(\B_{k}, \Tu_{k}))\cap E^{(k)}\neq\emptyset} <2\varepsilon \cdot 2^{-1.5^{k}}.
 \end{align*}
As a consequence, the set \( E \) is sparse with probability 1.
\end{lemma}

\begin{proof}
Let \( k=1 \).
Rectangle \( B(\vek{x},(\B_{1}, \Tu_{1})) \) is a single point, hence
the probability of our event is \( <\varepsilon \).
Let us prove the inequality by induction, for \( k+1 \).

Note that our 
event depends at most on the rectangle \( B(\vek{x},3(\B_{k}, \Tu_{k})) \).
Let
\begin{align*}
   p_{k}=2\varepsilon\cdot 2^{-1.5^{k}}.
\end{align*}
Suppose \( \vek{y} \in E^{(k)}\cap B(\vek{x},\gamma(\B_{k+1}, \Tu_{k+1})  ) \).
Then, according to the definition of \( E^{(k)} \),  there is a point
\begin{align}\label{eq:sparse-as}
 \vek{z} \in
 B(\vek{y},\gamma(\B_{k+1},\Tu_{k+1}))\cap E^{(k)}\setminus B(\vek{y},\beta(\B_{k}, \Tu_{k})).
 \end{align}
Consider a standard partition of the (two-dimensional) space-time into
rectangles \( K_{p}=\vek{c}_{p}+\lint{-\B_{k}}{\B_{k}}\times \lint{-\Tu_{k}}{\Tu_{k}} \)
with centers \( \vek{c}_{1},\vek{c}_{2},\dots \).
The rectangles \( K_{i},K_{j} \) containing \( \vek{y} \) and \( \vek{z} \)
respectively intersect \( B(\vek{x}, 2\gamma(\B_{k+1}, \Tu_{k+1})) \).
The triple-size rectangles 
\( K'_{i}=c_{i} + \lint{-3\B_{k}}{3\B_{k}}\times \lint{-3\Tu_{k}}{3\Tu_{k}} \) and
\( K'_{j} \) are disjoint, since \eqref{eq:beta} and~\eqref{eq:sparse-as} imply
 \( \abs{y_{1} - z_{1}}>\beta\B_{k} \) and \( \abs{y_{2} - z_{2}}>\beta\Tu_{k} \).

The set \( E^{(k)} \) must intersect two rectangles \( K_{i} \),
\( K_{j} \) of size \( 2(\B_{k}, \Tu_{k}) \) separated by at least \( 4(\B_{k}, \Tu_{k}) \),
of the big rectangle \( B(\vek{x},2\gamma(\B_{k+1}, \Tu_{k+1})) \).

By the inductive hypothesis, the event \( \cF_{i} \) that
\( K_{i} \) intersects \( E_{k} \) has probability bound \( p_{k} \).
It is independent of the event \( \cF_{j} \), since these events depend
only on the triple size disjoint rectangles \( K'_{i} \) and \( K'_{j} \).

The probability that both of these events hold is at most \( p_{k}^{2} \).
The number of possible rectangles
\( K_{p} \) intersecting \( B(\vek{x},2\gamma(\B_{k+1}, \Tu_{k+1})) \) is
at most
\( C_{k}:=((2\gamma^{2}\U_{k} \Q_{k})+2)^{2} \), so the number of possible pairs of rectangles
is at most \( C_{k}^{2}/2 \), bounding the probability of our event by
 \begin{align*}
   C_{k}^{2}p_{k}^{2}/2
    &=
      2 C_{k}^{2}\varepsilon^{2} 2^{-1.5^{k+1}}\cdot 2^{-0.5\cdot 1.5^{k}}
   \\ &=2\varepsilon 2^{-1.5^{k+1}} \cdot \varepsilon
        C_{k}^{2}2^{-0.5\cdot 1.5{k}}.
 \end{align*}
%Since \( \lim_{k}\frac{\log{(\gamma_{k}\U_{k} \Q_{k})}}{1.5^k}=0 \),
Since \( \lim_{k}\frac{\log{(\U_{k} \Q_{k})}}{1.5^k}=0 \),
the last factor is \( \le 1 \) for sufficiently small  \( \varepsilon \).
\end{proof}



\subsection{Error-correcting code}\label{sec:coding}

Let us add error-correcting features to block codes introduced in
Definition~\ref{def:codes}.

\begin{sloppypar}
\begin{definition}[Error-correcting code]\label{def:err-code}
A block code is \( (\beta,t) \)-\df{burst-error-correcting},
if for all \( x\in\Sigma_{2} \), \( y\in\Sigma_{1}^{\Q} \) we
have \( \psi^{*}(y)=x \) whenever \( y \) differs from
\( \psi_{*}(x) \) in at most \( t \) intervals of size \( \le\beta \).

For such a code, we will call a word \( y\in\Sigma_{1}^{\Q} \) is \( r \)-\df{compliant}
if it differs from a codeword of the code by at most \( r \) intervals of size \( \le\beta \).
\end{definition}
  \end{sloppypar}

\begin{example}[Repetition code]\label{xmp:tripling}
  Suppose that \( \Q\ge 3\beta \) is divisible by 3,
  \( \Sigma_{2}=\Sigma_{1}^{\Q/3} \), \( \psi_{*}(x)=xxx \).
  Let \( \psi^{*}(y) \) be obtained as follows.
  If \( y=y(1)\dots y(\Q) \), then \( x=\psi^{*}(y) \) is defined as follows:
    \( x(i)=\maj(y(i),y(i+\Q/3),y+2\Q/3) \).
    For all \( \beta\le \Q/3 \), this is a
    \( (\beta,1) \)-burst-error-correcting code.

    If we repeat 5 times instead of 3, we get a \( (\beta,2) \)-burst-error-correcting
    code.
    Let us note that there are much more efficient such codes than just repetition.
 \end{example}

Consider a Turing machine 
\( (\Gamma, \Sigma,\tau, q_{\start},F) \) (actually a generalized one to be defined later)
%    M = (\Gamma, \Sigma, \tau, \NonAdj, \cDir, q_{\start},F, \B, \Tu)
simulating some Turing machine \( (\Gamma^{*}, \Sigma^{*},\tau^{*}, q^{*}_{\start},F^{*}) \).
% \begin{align*}
% M^{*} = (\Gamma^{*}, \Sigma^{*}, \tau^{*}, \NonAdj^{*}, \cDir^{*}, q^{*}_{\start},F^{*}, \B^{*}, \Tus).
% \end{align*}
We will assume that \( \Gamma^{*}\cup\set{\emptyset} \),
and the alphabet \( \Sigma^{*} \) are subsets of the set of  binary strings
\( \{0,1\}^{l} \) for some \( l<\Q \) (we can always ignore some states or tape
symbols, if we want).

\begin{definition}[Interior]\label{def:interior}
Consider an interval \( I \) of neighbor cells.
A cell belongs to the \df{interior} of \( I \) if there are at least \( \PadLen \) neighbors between it 
and the complement of \( I \).
In particular, we will talk about the interior of a colony and the interior of a workspace.
\end{definition}

We will store the coded information in the interior of the colony, since it is more exposed 
to errors near the boundaries.
So let \( (\upsilon_{*}, \upsilon^{*}) \) be a \( (\beta,2) \)-burst-error-correcting block code
\begin{align*}
  \upsilon_{*}: \{0,1\}^{l} \cup \set{\emptyset}
   \to\{0,1\}^{(\Q-2\cdot\PadLen)\B}.
\end{align*}
We could use, for example, the repetition code of Example~\ref{xmp:tripling}.
Other codes are also appropriate, but we require that they have some fixed
programs \( p_{\encode} \), \( p_{\decode} \)
on the universal machine \( \Un \), in the following sense:
 \begin{align*}
   \upsilon_{*}(x)=\Un(p_{\encode},x),\quad
   \upsilon^{*}(y)=\Un(p_{\decode}, y).
 \end{align*}

Also, these programs must work in quadratic time and linear space on a one-tape
Turing machine (as the repetition code certainly does).

Let us now define the block code \( (\psi_{*}, \psi^{*}) \) used in the
definition of the configuration code \( (\varphi_{*}, \varphi^{*}) \) as 
outlined in Section~\ref{sec:hier-codes}.
We define
\begin{equation}\label{eq:psi}
   \psi_{*}(a)  = 0^{\PadLen}\upsilon_{*}(a)0^{\PadLen}.
\end{equation}
It will be easy to compute the configuration code from \( \psi_{*} \),
once we know what fields there are which ones need initialization.

The decoded value \( \psi^{*}(x) \) is obtained by first removing \( \PadLen \)
symbols from both ends of \( x \) to get \( x' \), and then computing \(
\upsilon^{*}(x') \).


\section{The model}\label{sec:model}

Recall the definition of sparsity in Section~\ref{sec:sparsity}: there will be 
a sequence \( 0<\B_{1}<\B_{2}<\dotsm \) of ``scales'' in space and a sequence
\( \Tu_{1}<\Tu_{2}<\dotsm \) of scales in time, and a constant \( \beta \).
We will define a sequence of simulations \( M_{1}\to M_{2}\to\dotsm \) where
each \( M_{k} \) is a machine simulating one on a higher level.
For simplicity, we will use the notation \( M=M_{k} \), \( M^{*}=M_{k+1} \),
and similarly for the other parameters, for example \( \B,\Tu, \Q, \U \).
As already indicated in Section~\ref{sec:hier}, these machines will generalize
ordinary Turing machines, with a number of new features.

\subsection{Generalized Turing machine}

Standard Turing machines do not have
operations like ``creation'' or ``killing'' of cells, nor
do they allow for cells to be non-adjacent.
We introduce here a \df{generalized Turing machine}.
It depends on an integer \( \B \ge 1 \) that denotes the cell body size,
and an upper bound \( \Tu \) on the transition time, as well as a \df{ pass number} \( \pi \),
whose meaning will be explained in the definition of configurations.
These parameters are convenient since they provide the illusion that the different Turing
machines in the hierarchy of simulations all operate on the same linear space.
Even if the notions of cells, alphabet
and state are different for each machine of the hierarchy, 
at least the notion of a \emph{location on the tape} is the same.


\begin{definition}[Generalized Turing machine]\label{def:gen-TM}
    A \df{generalized Turing machine} \( M \) is defined by a tuple
        \begin{align}\label{eq:gen-TM}
             (\Gamma, \Sigma, \tau, \NonAdj, \cDir, q_{\start},F, \B, \Tu, \pi),
       \end{align}
    where \( \Gamma \) and \( \Sigma \) are finite sets
    called the \df{set of states} and the \df{alphabet} respectively,
        \begin{align*}
             \tau: \Sigma\times \Gamma
             \to \Sigma\times \Gamma\times\{-1,1\},
        \end{align*}
    is the \df{transition function}.
\( \NonAdj \) is a function of the state (can be called a ``field'' if a state is viewed 
as a piece of data, a record with several fields).
\( \NonAdj\colon\Gamma\to\{\true,\false\} \) will show 
whether the last move was from a non-adjacent cell.
The function \( \cDir\colon\Sigma\to\{-1,1\} \) of the cell content
needs to always point towards the head, so 
the transition function \( \tau \) is required to have the property that
if \( (a',q',j)=\tau(a,q) \) then \( a'.\cDir=j \).

The role of starting state \( q_{\start} \) and final states in \( F \) is as before.
The integer \( \B\ge 1 \) is called the \df{cell body size},
and the real number \( \Tu \) is a bound on the transition time.
The \df{pass number} \( \pi \) will play a role in the definition of trajectories.

Among the elements of the tape alphabet \( \Sigma \), 
we distinguish the elements \( 0,1,\Bad,\Vacant \).
The role of the symbols \( \Bad \) and \( \Vacant \) will be clarified below.
\end{definition}


\begin{definition}[Configuration]\label{def:config}
     Consider a generalized Turing machine~\eqref{eq:gen-TM}.
    A \df{configuration} is a tuple
        \begin{align*}
             (q,A,\h,\hc,),
        \end{align*}
    where \( q\in\Gamma \), 
\( A:\bbZ\to\Sigma \), \( \h,\hc\in\bbZ \).
The array \( A \) is the tape configuration.
As in~\eqref{eq:config-1} before, \( \h \) is the head position, but it may differ from the \df{current cell}
\( \hc \), so now if \(  \xi= (q,A,\h,\hc,) \) then we write
        \begin{align}\label{eq:config-2}
             \xi.\state=q, \quad \xi.\tape=A, \quad \xi.\pos=\h,\quad \xi.\curcell=\hc. 
        \end{align}
A point \( p \) is \df{clean} if  \( A(p)\ne\Bad \).
A set of points is \df{clean} if it consists of clean points.

We say that there is a \df{cell} at a position \( p\in\bbZ \) if the interval
\( p+\lint{0}{\B} \) is clean and \( A(p)\ne \Vacant \).
In this case, we call the interval \( p+\lint{0}{\B} \) the \df{body} of this cell.
Cells must be at distance \( \ge\B \) from each other, that is their
bodies must not intersect.
They are called \df{adjacent} if the distance is exactly \( \B \).
A configuration will always have the following property:
\begin{enumerate}[label=\upshape{(C\arabic*)}, ref=C\arabic*]
\item\label{i:config.sharp-ends}
 If a maximal clean interval has an endpoint to the right (left) then it ends in a cell body in that direction.
\end{enumerate}
For all cells \( p \), the value \( A(p).\cDir \) is required to point towards 
the head position \( \h \), that is 
 \begin{align*}
   A(p).\cDir=\sign(\h-p).
 \end{align*}
Whenever the interval \( \h+\lint{3\B}{3\B} \) is clean there must be a
cell at some position \( \hc \) within this interval called the \df{current cell},
with a body within \( 2\B \) from \( \h \).

The array \( A \) is \( \Vacant \) everywhere but in finitely many positions.
Let
    \begin{align*}
         \Configs_{M}
    \end{align*}
    denote the set of all possible configurations
    of a Turing machine \( M \).
\end{definition}

All the above definitions can clearly be localized to define a configuration
\df{over a space interval} \( I \), where it is always understood that \( \h\in I \), that is 
\( I \) contains the head.

\begin{definition}[Local configuration, replacement]
\label{def:local-config}
  A \df{local configuration on} a (finite or infinite)
  interval \( I \) is given by values assigned to the cells
  of \( I \), along with the following information: whether
  the head is to the left of, to the right of or inside
  \( I \), and if it is inside, on which cell, and what is
  the state.

  If \( I' \) is a subinterval of \( I \), then a local configuration
  \( \xi \) on \( I \) clearly gives rise to a local configuration
  \( \xi(I') \) on \( I' \) as well, called its
  \df{subconfiguration}: If the head of \( \xi \) was in \( I \)
  and it was for example to the left of \( I' \), then now
  \( \xi(I') \) just says that it is to the left, without
  specifying position and state.

  Let \( \xi \) be a configuration and \( \zeta(I) \) a local
  configuration that contains the head if and only if
  \( \xi(I) \) contains the head.
  Then the configuration \( \xi\vert\zeta(I) \) is obtained by
  replacing \( \xi \) with \( \zeta \) over the interval \( I \),
  further if \( \xi \) contains the head then also replacing
  \( \xi.\pos \) with \( \zeta.\pos \) and \( \xi.\state \) with
  \( \zeta.\state \).
\end{definition}

It is natural to name a sequence of configurations that is conceivable as a computation
(faulty or not) of a Turing machine as ``history''.
The histories that obey the transition function then could be called ``trajectories''.
In what follows we will 
stretch this notion to encompass also some limited violations of the
transition function.

In connection with any underlying Turing machine with a given starting configuration, we will
denote by
\begin{align}\label{eq:noise-first}
   \Noise\subseteq \bbZ\times \bbZ_{\ge 0}
\end{align}
the set of space-time points \( \pair{p}{t} \), such that
a fault occurs at time \( t \) when the head is at position \( p \).

\begin{definition}[History]\label{def:history}
  \begin{sloppypar}
    Let us be given a generalized Turing machine~\eqref{eq:gen-TM}.
    Consider a sequence \( \eta = (\eta(0), \eta(1), \dots) \) of configurations with
    \( \eta(t) = \) \( (q(t), A(\cdot, t), \h(t), \hc(t)) \), along with a noise set \( \Noise \).
    The \df{switching times} of this sequence 
are the times \( t \) when one of the following changes:
the state, the content or position of the current cell.
In other words, if the triple \( (q(t),A(\hc(t),t), \hc(t)) \) changes.
The interval between two consecutive switching times is the \df{dwell period}.
The pair
      \end{sloppypar}
    \begin{align*}
       (\eta,\Noise)
    \end{align*}
    will be called a \df{history} of machine \( M \) if the following conditions hold.
        \begin{itemize}
            \item We have \( \abs{\h(t) - \h(t')} \le \abs{t' - t} \).

            \item In two consecutive configurations, content \( A(p,t) \) of the positions \( p \)
              not in \( \h(t) + \lint{-\B}{\B} \), remains the same: for example
                  \( A(n,t+1) = A(n,t) \) for all \( n \notin \h(t) + \lint{-\B}{\B} \).
            \item At each noise-free switching time the head is on the new current cell, that is
\( \hc(t)=\h(t) \).
In particular, when at a switching time a current cell becomes
\( \Vacant \), the head must already be on another (current) cell.

            \item The length of any noise-free 
dwell period in which the head is staying on clean positions is at most \( \Tu \).

        \end{itemize}
    Let
        \begin{align*}
            \Histories_{M}
        \end{align*}
    denote the set of all possible histories of \( M \).

We say that a cell \df{dies} in a history if it becomes \( \Vacant \).

It is clear that all the above definition can be \df{localized} to define a history
\df{over a space-time rectangle} \( I\times J \), 
where it is always understood that \( \h\in I \) for all times \( t\in J \),
that is \( I \) contains the head throughout the time interval considered.

\end{definition}

The transition function \( \tau \) of a generalized Turing machine
imposes constraints on histories: those
histories obeying the constraints will be called trajectories.

\begin{definition}
Suppose that at times \( t' \) before a switching time \( t \) but after 
any previous switch,
the machine is in a state \( q=q(t') \), with current cell \( x=\hc(t') \), 
with cell content \( a=A(\hc(t'),\hc(t')) \).
We say that the new state, the new content of cell \( x \) (including when it dies),
and the direction of the new position \( y \) from \( x \)
are \df{dictated by the transition function} if 
\begin{align*}
 (q(t),A(\hc(t),\hc(t)),\sign(\hc(t)-\hc(t'))) =  \tau(a,q),   
 \end{align*}
further \( q(t).\NonAdj=\true \) if and only if \( \hc(t)-\hc(t')\notin\{-B,0,B\} \).
In other words, if \( (q',a',j)=\tau(q,a) \) then the new state is \( q' \),
the new content of \( x \) is \( a' \), the direction of \( y \) from \( x \) is \( j \),
and \( q'.\NonAdj=\true \) only if \( y \) is not equal or adjacent to \( x \).
\end{definition}

The following definitions will use the constants
\begin{align}\label{eq:cns.traj}
   \CAtt =6, \CSpill =5, \CDwell.
 \end{align}


 \begin{definition}\label{def:clean-hole}
   If in a configuration the head is in a clean interval of size \( \ge 1.5 \CAtt\B \),
at a distance \( \ge\B \) from the complement, then we will say that the head is \df{inside a clean hole}.
 \end{definition}

\begin{definition}[Trajectory]\label{def:traj}
\begin{sloppypar}
   A history  \( (\eta, \Noise) \) of a generalized Turing 
machine~\eqref{eq:gen-TM} with \(\eta(t) =\)
\( (q(t), A(t), \h(t), \hc(t)) \)
is called a \df{trajectory} of \( M \) if the following conditions hold, in any 
noise-free time interval \( J \).
  \end{sloppypar}
\begin{description}

\item[Transition Function]\label{i:def.traj.transition}
Suppose that there is a switch, and the current cell \( x \)
is inside the clean area, by a distance of at least \( 2.5\B \) in the direction of the
new cell \( y \), and by at least \( 0.5\B \) in the other direction.
Then the new state, the cell content of \( x \) (including when it dies), and
the direction of \( y \) from \( x \) are dictated by the transition function.
If \( y \) did not exist before then it is adjacent to \( x \).
Nothing else changes on the tape.

Further the length of the dwell period is bounded by \( \Tu \).

\item[Spill Bound]\label{i:spill-bound}
A clean space interval may shrink by at most \( \CSpill \B \) on either side.

\item[Dwell Cleaning] \label{i:def.traj.dwell-cleaning}
If the head spends time \( \CDwell\Tu \) \emph{cumulatively} during \( J \) in any interval 
\( I \) of size \( 1.5\CAtt B \)  (counting the times spent in \( I \) and 
not the times spent outside \( I \)) then at some time during this, it will be 
inside a clean hole intersecting \( I \).

\begin{sloppypar}
\item[Attack cleaning] \label{i:def.traj.attack-cleaning}
Suppose that the current cell \( x \), is the right endcell of a maximal clean interval of size
\( \ge (\CAtt+1)\B \).
Suppose further that the transition function directs the head right.
Then by the time the head comes back to \( x-\CAtt \B \), the right end of the clean interval containing it
advances to the right by at least \( \B \).
 \end{sloppypar}

A similar property is required when ``left'' and ``right'' are interchanged.

\item[Pass Cleaning]\label{i:def.traj.pass-cleaning}
If the head passes  \( \pi \) times  an interval \( \lint{a}{b} \) then
the subinterval \( \lint{a+\CSpill B}{b-\CSpill B} \) becomes clean.

\end{description}

\end{definition}

The Spill Bound property makes the following notation convenient:
\begin{align}\label{eq:beta'}
   \beta'=\beta+2\CSpill.
 \end{align}


The above definition can also clearly be localized to some space-time
rectangle just as the definition of history was.

The Attack Cleaning property says essentially that if the head moves out on the right end
of a clean interval then next time it comes in, it must extend the right end of the interval 
by at least \( \B \) (while temporarily possibly withdrawing it by \( \CSpill\B \)).


\subsection{Simulation}

Until this moment, we used the term ``simulation'' informally, to denote
a correspondence between configurations of
two machines which remains preserved during the computation.
In the formal definition, this correspondence will essentially be a code
\( \varphi=(\varphi_{*},\varphi^{*}) \).
The \emph{decoding} part of the code is the more important.
We want to say that machine \( M_{1} \) simulates machine \( M_{2} \) via
simulation \( \varphi \) if whenever \( (\eta, \Noise) \) is a trajectory of \( M_{1} \) 
then \( (\eta^{*},\Noise^{*}) \),
defined by \( \eta^{*}(\cdot,t)=\varphi^{*}(\eta(\cdot,t)) \), is a
trajectory of \( M_{1} \).
Here, \( \Noise^{*} \) is computed by an appropriate mapping.

We will make, however, two refinements.
First, we may weaken the condition by requiring this only for
those \( \eta \) for which the initial configuration
 \( \eta(\cdot,0) \) has been obtained by encoding, that is it has the form 
\( \eta(\cdot,0)=\varphi_{*}(\xi) \).
The encoding function gets a role this way in the definition, after all.

But there is a more complex refinement.
When a colony is in transition between encoding one simulated value to encoding another one,
there may be times when the value represented by it before the transition
is already not decodable from it, and the value after the transition is not yet decodable from it.
So we will define simulation decoding as a mapping \( \Phi^{*} \)
between \emph{histories}, not just configurations.
This allows a certain amount of ``looking back'':
the map \( \Phi^{*} \) can depend on the configurations at the beginning of the ``work period''.

It is the mapping \( \Phi^{*} \) that will also define \( \Noise^{*} \).
A history was defined above in Definition~\ref{def:history} 
as a pair \( (\eta,\Noise) \), so we will have
\( \Phi^{*}(\eta,\Noise)=(\eta^{*},\Noise^{*}) \).
The meaning of \( \Noise^{*} \) will be, just as in Definition~\ref{def:sparsity} of sparsity:
it will be obtained from \( \Noise \) by deleting those small isolated parts that the 
error-correcting simulation can deal with.

\begin{definition}[Simulation] \label{def:simulation-central}
Let \( M_{1},M_{2} \) be two generalized Turing machines, and let
\begin{align*}
    \varphi_{*}:\Configs_{M_{2}} \to \Configs_{M_{1}}
\end{align*}
be a mapping from configurations of \( M_{2} \)
to those of \( M_{1} \), such that it maps
starting configurations into starting configurations.
We will call such a map a \df{configuration encoding}.
Let
\begin{align*}
   \Phi^{*}:\Histories_{M_{1}} \to \Histories_{M_{2}}
\end{align*}
be a mapping.
The pair \( (\varphi_{*}, \Phi^{*})  \)
is called a \df{simulation} (of \(  M_{2}  \) by \(  M_{1}  \)) if for every
trajectory \(  (\eta, \Noise)  \) with initial
configuration \(  \eta(\cdot,0)=\varphi_{*}(\xi)  \),
the history \(  (\eta^{*},\Noise^{*})=\Phi^{*}(\eta,\Noise)  \) is
a trajectory of machine \(  M_{2}  \).

We say that \( M_{1} \) \df{simulates} \( M_{2} \) if there is a simulation
\( (\varphi_{*},\Phi^{*}) \) of \( M_{2} \) by \( M_{1} \).
\end{definition}

\subsection{Hierarchical codes}\label{sec:hier-codes}

Recall the notion of a code in Definition~\ref{def:codes}.

\begin{definition}[Code on configurations]\label{def:configuration-code}
\begin{sloppypar}
 Consider two generalized Turing machines \( M_{1},M_{2} \) with the corresponding
state spaces, alphabets and transition functions, and an integer \( \Q\ge 1 \).
We require
\begin{align}\label{eq:B_{2}-B_{1}-\Q}
  \B_{2} = \Q \B_{1}.
\end{align}
Assume that a block code
\(
   \psi_{*}:\Sigma_{2}\times(\Gamma_{2}\cup\{\emptyset\})\to\Sigma_{1}^{\Q}
\)
is given, with an appropriate decoding function, \( \psi^{*} \).
With \( \pair{a}{q}\in\Sigma_{2}\times(\Gamma_{2}\cup\{\emptyset\}) \),
symbol \( a \) is interpreted the content of some tape square.
The value \( q \) is the state of \( M_{2} \) provided the head is observing this square,
and \( \emptyset \) if it is not.
For all \( a,q \), if \( \psi_{*}(a,q) = (b_{1},\dots,b_{\Q}) \) then we require
\( b_{i}.\cDir=a.\cDir \) for each \( i \).
\end{sloppypar}

This block code gives rise to a \df{code on configurations}, that is a pair of functions
    \begin{align*}
        \varphi_{*} :\Configs_{M_{2}} \to \Configs_{M_{1}},
        \quad
        \varphi^{*}:\Configs_{M_{1}} \to \Configs_{M_{2}}
    \end{align*}
    that encodes some (initial) configurations \( \xi \) of \( M_{2} \) into configurations of \( M_{1} \).
Let \( \xi \) be a configuration of \( M_{2} \) with \( \xi.\curcell=\xi.\pos \), \( \xi.\state = q_{\start} \).
We set \( \varphi_{*}(\xi).\pos = \varphi_{*}.\curcell= \xi.\pos \), \(\varphi_{*}.\state=q^{*}_{\start}  \),
the starting state of \( M_{1} \), and
\begin{align*}
 \varphi_{*}(\xi).\tape(i\B_{2}, \dots, (i+1)\B_{2} - \B_{1}) = \psi_{*}(\xi.\tape(i), s)
 \end{align*}
where \( s=\xi.\state \) if \( i = \xi.\pos \), and \( \emptyset \) otherwise.
A configuration \( \xi \) is called a \df{code configuration} if 
it has the form \( \xi=\varphi_{*}(\zeta) \).
 \end{definition}


\begin{definition}[Hierarchical code]\label{def:hierarchical-code}
For \( k\ge 1 \), let \( \Sigma_{k} \) be an alphabet, \( \Gamma_{k} \) be
a set of states of a generalized Turing machine \( M_{k} \).
Let \( \Q_{k}>0 \) be an integer colony size, let \( \varphi_{k} \)
be a code on configurations defined by a block code
  \begin{align*}
       \psi_{k}: \Sigma_{k+1}\times(\Gamma_{k+1}\cup\{\emptyset\})
       \rightarrow \Sigma_{k}^{\Q_{k}}
  \end{align*}
as in Definition~\ref{def:configuration-code}.
The sequence of triples \( (\Sigma_{k},\Gamma_{k},  \varphi_{k}) \), (\( k\ge 1) \),  is
called a \df{hierarchical code}.
For the given hierarchical code, the configuration \( \xi^{1} \) of \( M_{1} \)
is called a \df{hierarchical code configuration} if a sequence
of configurations \( \xi^{2},\xi^{3},\dots \) of \( M_{2},M_{3},\dots \) exists with
\begin{align*}
 \xi^{k}=\varphi_{*k}(\xi^{k+1})
 \end{align*} 
for all \( k \).
(Of course, then whole sequence is determined by \( \xi^{1} \).)

Let \( M_{1} \), \( M_{2} \), \( \dots\) be a sequence of generalized Turing machines,
let \( \varphi_{1} \), \( \varphi_{2} \) , \(\dots \) be a hierarchical code for this sequence,
let \( \xi^{1} \) be a hierarchical code configuration for it, where \( \xi^{k} \) is an
initial configuration of \( M_{k} \) for each \( k \).
Let further be a sequence of mappings \( \Phi^{*}_{1} \), \( \Phi^{*}_{2} \), \( \dots \) be
given such that for each \( k \), the pair \( (\varphi_{k*},\Phi_{k}^{*}) \),
is a simulation of \( M_{k+1} \) by \( M_{k} \).
Such an object is called a \df{tower}.
\end{definition}

The main task of the work will be the definition of a tower, since the simulation
property is highly nontrivial.


\section{Simulation structure}

In what follows we will describe the program of the reliable Turing machine
(more precisely, a simulation of each \( M_{k+1} \) by \( M_{k} \) as defined above).
Most of the time, we will just refer to \( M_{k} \) as \( M \) and to \( M_{k+1} \) as
\( M^{*} \).
Cells will be grouped into colonies, where  \( \Q=\B^{*}/\B \) is the colony size.
The behavior of the head on
each colony simulates the head of \( M^{*} \) on the corresponding cell
of \( M^{*} \).
The process takes a number of steps, constituting a \df{work period}.

\begin{definition}\label{def:Tu}
The parameter \( \U=\Tu^{*}/\Tu \) is defined to be
twice the maximum number of steps that any simulation work period can take.
\end{definition}

Machine \( M \) will perform the simulation even if the noise
in which it operates is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tu^{*}}) \)-sparse.
By the above definitions, 
sparsity means that noise comes in \df{bursts} that are confined to
rectangles of size \( \beta\pair{\B}{\Tu} \) (affecting at most \( \beta \) consecutive tape cells), 
and are separated from each other in time in such a way that there is at most one burst
in any \( \gamma \) neighboring work periods.
A design goal for the program the following:

\begin{goal}[Local correction]\label{goal:locality}
Correct a burst within space and time comparable to its size, and much smaller than the 
size of a colony work period.
\end{goal}

There are some difficulties faced by our desire for a structured presentation:
In order to analyze the error-correcting performance even of one a part of the  program,
we may need to see the whole, since the noise can bring the machine into 
some state corresponding to an arbitrary other part.

We mentioned \df{modes} in Section~\ref{sec:language}.
Ordinary simulation proceeds in the normal mode. 
To see whether the basic structure
supporting this process is broken somewhere, each step will check 
whether the present state is \df{coordinated}
with the currently observed cell symbol (see Definition~\ref{def:coordinated}).
If not then the state jumps into the \df{healing} mode.
We will also say that \df{alarm} will be called.
On the other hand, the state enters into \df{rebuilding} mode on some indications that
healing fails.
The crudest outline of the main rule of machine \( M \) is given in
Rule~\ref{alg:main1}; the \( \Comp \) and \( \Transfer \) rules will be
outlined below.
(Rebuilding may be triggered inside the \( \rHeal \) rule.)

\begin{algorule}\caption{\textbf{Main rule}\label{alg:main1}}

  \If{the mode is normal}{
    \lIf{\algNot \( \Coordinated \)}{\( \rHeal \)}
    \lElseIf{\( 1 \le \Sweep < \TransferSw(1) \)}{ \( \Comp \) }
    \lElseIf{\( \TransferSw(1) \le \Sweep < \Last \)}{ \( \Transfer \)}
   \lElseIf{\( \Last \le \Sweep  \)}{move the head to the new base.}
  }
\end{algorule}


\subsection{Head movement}\label{sec:sweep}

The global structure of a work period is this:
\begin{description}

\item[Computation phase] 
The new simulated state and direction (called the drift) is computed.
Then the ``meaningfulness'' of the result is checked.
During this phase the head sweeps, roughly, back-and-forth between
the ends of the base colony.

\item[Transfer phase]
The head moves into the neighbor colony in the simulated head direction,
and transfers the simulated state to there.
If the drift is, say, to the right, then the head sweeps, roughly, between the 
left end of the source colony and the right end of the target colony.
There may be an area between the  two colonies to bridge over.
\end{description}

The timing is controlled by a field \( \Sweep \) of the state counting the sweeps,
and a corresponding field \( \cSweep \)  of the cell state.
We can read off the direction of the sweep \( s \) using the formula
     \begin{align}\label{eq:sweep-dir}
       \dir(s)=(-1)^{s + 1}.
     \end{align}
Some issues complicate the head movement, even in the absence of faults.

\subsubsection{Zigging}\label{sec:zigging}

A burst could turn the head back in the middle of its sweep.
To detect such an event promptly (in accordance with Goal~\ref{goal:locality}),
the sweeping will be complicated by a \df{zigzag} movement.
On its backward zig, the head can check whether the structure of 
last few cells is correct.
On its forward zig, it can also check whether it has not entered into forbidden territory/
Zigging will use certain parameters \( \F,\Z \) that we set to 
 \begin{align*}
     \F    &=7,
\\   \Z   &= 14\F\beta, %?
\\  \tZig &= 1,
\quad  \tHeal =2,
\quad  \tRebuild =3.
 \end{align*}
The choice of the parameter \( \F \) will be motivated by the discussion
of feathering in Section~\ref{sec:feathering}.
In its movement in direction \( \delta\in\{-1,1\}  \) the head, every forward sweeping
move shifts what is called the \df{front}.
These moves also change the \( \cSweep \) field.
Now it will perform a forward-backward zigzag of size approximately \( 2\Z \), without changing anything
on the tape.
First, it makes \( >\Z \) more steps forward, looking for the first place ahead where 
it is allowed to turn back (see Section~\ref{sec:feathering}).
As we will see this will be within \( \F \) steps.
Then it will move back to the front and further \( >\Z \) steps backwards from the front, to
the first position where it sill be allowed to turn forward.
The fields
\begin{align*}
\ZigDir\in\{-1,0,1\}, \ZigDepth\in\lint{-\Z-\F}{\Z+\F}
 \end{align*}
control the process.
At the front, we have \( \ZigDepth=0 \) and \( \ZigDir=0 \).
At the start of zigging, we set \( \ZigDir\gets -1 \), and starts \df{descending} into a (forward) zig,
while increasing \( \ZigDepth \).
When the front reaches the extreme edge of zigging at an appropriate turning point (see below),
we set \( \ZigDir\gets 1 \),
the head turns towards the front and starts \df{ascending}, while decreasing \( |\ZigDepth| \).
% There is no corresponding field \( \cZigDepth \) on the tape: though it would seem provide
% some check against some faults, it would create more of its own problems.
Once the front is reached, a similar backward zigging swing is also performed (with \( \ZigDepth \) decreasing below 0).

% Normally the head should reach the front exactly when \( \ZigDepth \) reaches 0.
% However, if it reaches the front earlier (for whatever cause), then we just set \( \ZigDepth\gets 0 \).
% If it does not reach the front by this time (for whatever cause), then the head will just keep marching forward, 
% keeping \( \ZigDepth=0 \) until the front is reached.
% \\Pnote{Do we still have an example now that zigging begins only well inside?}

\subsubsection{Feathering}\label{sec:feathering}

A somewhat arcane
threat is responsible for another complication in head movement.

\begin{example}\label{xmp:feather-need}
Let \( C(x) \) denote the colony with starting point \( x \).
Consider the following scenario.

\begin{enumerate}
\item\label{i:leave-dirt}
After the last turn at the end of a right sweep on the right end of
colony \( C(x) \), at the bottom of the zig, say in
position \( y = x+(\Q+\Z)\B \) if the zig goes outside the colony, a burst creates some dirt.
Then the simulation dictates the head to leave \( C(x) \) on the left.

\item\label{i:later-burst} 
Much later, on the first sweep of a return to \( C(x) \), this dirt traps the head, and
lets it start a right sweep to the right of \( y \).  
When the head returns on a zig to \( y \), 
a new burst corrects everything on the left of \( y \), including \( y \), but leaves
the start of the new (false) right sweep on the right of \( y \)---even though
the simulation does not dictate any move to the colony \( C(x+\Q\B) \).

\item\label{i:repeat} Much later, the pair of events \ref{i:leave-dirt}-\ref{i:later-burst}
happens again and again, allowing the started false sweep on the right of \( y \) to continue.
After \( \Z \) repetitions of this, the zig does not return already.

\end{enumerate}
This way, a number of cleverly placed distant bursts can trigger an effect on the
next level.
(A probability model more sophisticated than sparsity might deal with this problem, but we did not find it.)
\end{example}

The sequence of Example~\ref{xmp:feather-need} would not occur if
our machines had the following property:

\begin{definition}[Feathering]\label{def:feathering}
A Turing machine execution is said to have the \df{feathering} property if the following holds.
If the head turned back at a position \( x \) at some time, 
the next time the head arrives at position \( x \) it cannot turn back from it.
\end{definition}

(The name ``feathering'' refers to the picture of the path of the head in a space-time diagram.)
The property can be enforced by the following simple
device: let the cell state have a field called
\begin{align*}
   \cCanTurn\in\{ \false,\true \}.
 \end{align*}
We will allow the head to turn \emph{only} in a cell with \( \cCanTurn=\true \).
Whenever a head turns at a cell, it sets this value to \( \false \); if  it passes the cell, the value 
is reset to \( \true \).
For example, if the head is moving right and decides to turn left then
it can do that at the first cell with \( \cCanTurn=\true \).
The following example suggests that any computation can be reorganized to accomodate feathering,
without too much extra cost.

\begin{example}[Feathering]\label{xmp:feathering}
Suppose that, arriving from the left at position 1, the head decides to turn left again.
In repeated instances, it can then turn back at the following sequence of positions:
\begin{align*}
 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, 5, 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, 6, \dots
 \end{align*}
\end{example}

So the feathering constraint does not divert the head too much.
If in the original execution the head turned back \( t \) consecutive
times to the left from position 0, then now it will 
turn back from somewhere in a zone of size \( O(\log t) \) to the right of 0 in 
each of these times.
We also see here that the exact turning point at the end of each sweep
could be computed from the sweep number.

The simulated Turing machine will also have the feathering property,
therefore it will not happen that the simulation turns back repeatedly 
from the same colony without passing it in the meantime.
It may still happen that at a point where it decided to turn, 
the head encounters a long run of cells with \( \cCanTurn=\false \).
But this indicates the absence of a clean interval comparable with the size of this 
run---and such cases will have to be dealt with in the context of error correction.

Let us argue, informally (without replacing any later, formal argument), that one does 
not have to consider more than three islands in any area of size \( \Q\B \). 
We assume that the simulated program obeys the
feathering condition: so if, say, the head comes from the left neighbor colony
then at the end of the work period can turn back to the left only if 
the previous time it visited the colony, it has passed it over from right to left.

\begin{example}[Three islands]\label{xmp:3-islands}
  Suppose that the head has arrived at \( C \) from the left,
performs a work period and then passes to the right.
In this case, if no new noise burst occurs then we expect that
all islands found in \( C \) will be eliminated.
On the other hand, a new island \( I_{1} \) can be deposited (in the last pass).
We can assume that there is no island on the left of  \( I_{1} \) 
within distance \( \Q\B \), since the noise burst
causing it would have been too close to the noise burst causing \( I_{1} \).

Consider the next time (possibly much later), when the head arrives (from the right).
If it later continues to the left, then the situation is similar to the above.
Island \( I_{1} \) will be eliminated, but a new one may be deposited.
But what if the head turns back left at the end of the work period?
If \( I_{1} \) is close to the left end of \( C \), then due to the feathering
construction, the head may never reach it to eliminate it; moreover,
it may add a new island \( I_{2} \) on the right of \( I_{1} \).
We can also assume, similarly to the above, 
that there is no island on the right of  \( I_{2} \) within distance \( \Q\B \).

When the head returns a third time (possibly much later), 
from the right, it will have to leave on the left.
The islands \( I_{1},I_{2} \) will be eliminated as they are passed over
but a possible new island
\( I_{3} \), created by a new burst (before, after or during the elimination), may remain.
We can also assume, similarly to the above, 
that there is no island on the right of  \( I_{3} \) within distance \( \Q\B \).
\end{example}

Though the \( \cCanTurn \) field assures feathering, we must make sure by organization
that it is not preventing any needed turns.
For this sake, some further regulations will be introduced.
We will make use of a parameter \( \E \) introduced in Definition~\ref{def:substantial}
in connection with the healing procedure.
It is a multiple of \( \F \).
\begin{description}
  \item[Zigging in normal mode]
Above, we made sure that the points for turning from left to right during simulation
introduced at the bottom of a zig when sweeping right have \( \Addr\equiv \tZig\pmod{\F} \).
This way, during left sweep, the head will always find a place within any interval of
\( \F \) cells with \( \cCanTurn=\true \); indeed, the zigging turns during the right sweep
happened only in one cell in any interval of size \( \F \).

  \item[Sweeping in normal mode]
Recall from~\eqref{eq:V} that the colony work period consists of at most \( \V \) sweeps,
and according to Definition~\ref{def:interior}, an area of
\begin{align}\label{eq:PadLenDef}
 \PadLen = \E\cei{\log\V}+\Z % Change eq:PadLen?
 \end{align} 
cells on both ends of the colony are left free of simulation information---they can be used for
feathering.
Suppose that the head is to be turned back at ``the right end''.
Then at a distance \( \PadLen \) from the right end, 
it starts looking for a place to turn;
it will actually turn back at the first position coming after this
with \( \Addr\equiv \tEnd \pmod{\E} \) and \( \cCanTurn=\true \).
This will still allow to find a turning point with \( \Addr<\Q \), 
as remarked after Example~\ref{xmp:feathering}.

Note that when the head changes direction at the end of a sweep,
it has \( \Addr\equiv \tEnd\pmod{\E} \); in other words, the
possible turning points at the colony ends are separated from each other by the
larger distance \( \E \), a multiple of \( \F \). 
The rationale for this will become clear only when analyzing the boundary of a large
clean and a large dirty area.

\item[End turn of healing or rebuilding]
The healing and the rebuilding procedures need a turning point
at the end of their ranges.
In case the cell in question has an address (is not a stem cell) then
it will need to have \( \Addr\equiv \pm\tHeal\pmod{\F} \) 
for healing turns and \( \equiv \pm\tRebuild \) for rebuilding turns.

\item[Zigging while rebuilding]
Rebuilding mode will have its own internal addresses: as with healing (see later),
to the left of the rebuilding front, the addresses are counted from the left end, and to the right
of the head from the right end.
When sweeping right then the bottom of the left zig will again be at a 
point with left address \( \equiv\tZig\pmod{\F} \).

% \item[Sweeping while rebuilding]
% Suppose that the head is to be turned back at ``the right end'' of a rebuilding range.
% Then at a distance \( \PadLen \), it starts looking for a place to turn;
% it will actually turn back at the first position coming after this
% with right address \( \equiv \tEnd \pmod{\E} \) and \( \cCanTurn=\true \).

\end{description}

We introduce a convenient notation.
\begin{notation}\label{not:plus}
Suppose that within a procedure we want the head to move \( n \) cells, and turn.
It may not be able to turn then, but only at a point with \( \cCanTurn=\true \) and
also having  for example \( \cAddr\equiv\pm\tHeal \) during healing.
For simplicity, we will say that the head moves \( n^{+} \) cells and turns.
Thus \( n^{+} \) is greater than \( n \), but by not much: only an amount \( O(\log\beta) \) for healing,
and \( O(\log \Q) \) for simulation and rebuilding.
\end{notation}

\subsection{Computation phase}\label{sec:computation-phase}

As shown in Rule~\ref{alg:main1} describing a top-down view of the simulation,
the first phase computes new values for the state of the 
simulated machine \( M^{*} \)
represented on track \( \cState \), the direction of the move of the head of  \( M^{*} \)
(represented in the \( \cDrift \) field of each cell of the colony of \( M \)), and
the simulated cell state of \( M^{*} \) represented on the track \( \cInfo \).
During this rule, the head sweeps the base colony.

Recall Definition~\ref{def:err-code}.
The rule \( \Comp \) will rely on a certain fixed \( (\beta,3) \) burst-error-correcting
code, moreover
it expects that each of the words found on the \( \cState \) and \( \cInfo \) tracks
is 2-compliant.  % Probably \( (\beta,2) \) would be sufficient.

The rule \( \rul{ComplianceCheck} \) checks whether a word is \( 2 \)-compliant.

The rule \( \Comp \) essentially repeats 3 times % why was here 5?
the following \df{stages}: decoding, applying the transition, encoding.
Then it calls \( \rul{ComplianceCheck} \); if the latter fails
it will mark the colony for rebuilding.

In more detail:
\begin{enumerate}
     % \item   In the first sweep of the work period, set
     %  \( \cAddr \gets \cAddr\bmod{\Q} \) and \( \cKind \gets \Member \).

      \item For every \( j=1,\dots,3 \), if \( \Addr \in \set{0, \dots, \Q-1} \) do 
        % Why was here 5 times instead of 3?

       \begin{enumerate}

          \item Calling by \( g \) the  string found on the \( \cState \) track of
            the interior of the base colony,
            decode it into string \( \tilde g=\upsilon^{*}(g) \)
            (this should be the current state of the simulated machine), and
            store it on some auxiliary track in the base colony.
            Do this by simulating the universal machine on the \( \cProg \) track:
            \( \tilde g = \Un(p_{\decode}, g) \).

            Proceed similarly with the string \( a \) found on the \( \cInfo \)
            track of the base colony to get \( \tilde a = \upsilon^{*}(a) \)
            (this should be the observed tape symbol of the simulated machine).
            % What is this out-commented part saying?
           % in the address interval \( J \), 
          
            % Perform all this computation in a way using no information from outside
            % the address interval \( J \), nor on any 
            % part of the state brought back into this interval other than the address,
            % sweep and zigging fields.

          \item \label{i:comp.trans}
           Compute the value \( (a',g',d)=\tau^{*}(\tilde a, \tilde g) \).
           Since the program of the transition function \( \tau^{*} \) is not written explicitly anywhere, 
           this ``self-simulation'' step needs some elaboration, see Section~\ref{sec:self-simulation}.

            \item\label{i:comp.write}
              Write the encoded new state \( \upsilon_{*}(g') \) onto the
              \( \cHold[j].\State \) track of the interior of the base colony.
              Similarly, write the encoded new observed cell
              content \( \upsilon_{*}(a') \) onto the \( \cHold[j].\Info \) track.
              Write \( d \) into the \( \cHold[j].\Drift \) field of \emph{each cell} of
              the base colony.

              Special action needs to be taken in case 
              the new state \( g' \) is a vacant one, that is 
              \( g'.\Kind^{*}=\Vacant^{*} \).
              In this case, write \( 1 \) onto the \( \cHold[j].\Doomed \) track (else 0).

        \end{enumerate}

       \item
      % \item Repeat the following twice (hoping that at least
      %   one repetition will be burst-free):  % Why twice?
            Sweeping through the base colony,
            at each cell compute the majority of \( \cHold[j].\Info \), \( j=1,\dots,3 \),
            and write it into the field \( \cInfo \).
            Proceed similarly, and simultaneously, with \( \cState \) and \( \Drift \).

       \item       For \( j=1,\dots,3 \), call \( \rul{ComplianceCheck} \) on the \( \cState \)
and \( \cInfo \) tracks, and
write the resulting bit into the \( \fld{Compliant}_{j} \) track.

Then pass through the colony and turn each cell in which the majority 
of \( \fld{Compliant}_{j} \), \( j=1,\dots,3 \) is false, into a stem cell
(thus destroying the colony if the result was false everywhere).

  \end{enumerate}

It can be arranged---and we assume so---that the total number of sweeps of this
phase, and thus the starting sweep number of the next phase,
depends only on \( \Q \).

\subsection{Forced self-simulation}\label{sec:self-simulation}

Step~\ref{i:comp.trans} of Section~\ref{sec:computation-phase} needs elaboration.

\subsubsection{New primitives}

We will make use of a special track
\begin{align*}
   \cWork
 \end{align*}
of the cells and the special field
\begin{align*}
   \Index
 \end{align*}
of the state of machine \( M \) that can store a certain address of a colony.

Recall from Section~\ref{sec:language} that the program
of our machine is a list of nested
``\textbf{if} \emph{condition} \textbf{then} \emph{instruction}
\textbf{else} \emph{instruction}''
statements.
As such, it can be represented as a binary string 
 \begin{align*}
   R.
 \end{align*}
If one writes out all details of the construction of the present paper, this string \( R \)
becomes completely explicit, an absolute constant.
But in the reasoning below, we treat it as a parameter.

Let us provide a couple of \df{extra primitives} to the rules.
First, they have access to the parameter \( k \) of machine \( M=M_{k} \), 
to define the transition function
 \begin{align*}
            \tau_{R,k}(a,q).
 \end{align*}
The other, more important, new primitive is a special instruction
 \begin{align*}
   \WriteProgramBit
 \end{align*}
in the rules.
When called, this instruction makes the assignment \( \cWork\gets R(\Index) \).
This is the key to self-simulation: \emph{the program has
access to its own bits}.
If \( \Index=i \) then it writes \( R(i) \) onto the current position of the \( \cWork \) track.


\subsubsection{Simulating the rules}

By convention, in our fixed flexible universal machine \( \Un \),
program \( p \) and input \( x \) produce an output \( \Un(p,x) \).
Since the structure of all rules is very simple, they can be read and
interpreted by \( \Un \) in reasonable time:

\begin{theorem}
There is a constant string called \( \Interpr \) with the property that for
all positive integers \( k \), string \( R \) that is a
sequence of rules, and bit strings \( a\in\Sigma_{k} \), \( q\in \Gamma_{k} \):
 \begin{align*}
  \Un(\Interpr,R,0^{k},a,q)=\tau_{R,k}(a,q).
 \end{align*}
The computation on \( \Un \) takes time \( O(\abs{R}\cdot (\abs{a}+\abs{q})) \).
\end{theorem}

The proof parses and implements the rules in the string \( R \); each of these rules
checks and writes a constant number of fields.

Implementing the \( \WriteProgramBit \) instruction is straightforward:
Machine \( \Un \) determines the number \( i \)
represented by the simulated \( \Index \) field, 
looks up \( R(i) \) in \( R \), and writes it into the simulated \( \cWork \) field.

Note that there is no circularity in these definitions:
  \begin{itemize}
  \item 
The instruction \( \WriteProgramBit \) is written \emph{literally}
in \( R \) in the appropriate place, as ``\(\WriteProgramBit \)''.
The string \( R \) is \emph{not part} of the rules (that is of itself).  
  \item On the other hand, the computation in
\( \Un(\Interpr,R,0^{k},a, q) \) 
has \emph{explicit} access to the string \( R \) as one of the inputs.
  \end{itemize}

Let us show the computation step invoking the ``self-simulation'' in detail.
In the earlier outline, step~\ref{i:comp.trans} of Section~\ref{sec:computation-phase},
said to compute \( \tau^{*}(\tilde a, \tilde g) \)
(for the present discussion, we will just consider computing 
\( \tau^{*}(a,q)=\tau_{k+1}(a,q) \)), where \( \tau=\tau_{k} \),
and it is assumed that \( a \) and \( q \) are available on two appropriate
auxiliary tracks.
We give more detail now of how to implement this step:

\begin{enumerate}
\item Onto the \( \cWork \) track, write the string \( R \).
To do this, for \( \Index \) running from 1 to \( \abs{R} \), 
execute the instruction \( \WriteProgramBit \) and move right.
Now, on the \( \cWork  \) track, replace it with \( \ang{\Interpr,0^{k+1},R,a,q} \).
Here, string \( \Interpr \) is a constant, so it is just hardwired.
String \( R \) already has been made available.
String \( 0^{k+1} \) can be written since the parameter \( k \) is available.
Strings \( a,q \) are available on the tracks where they were stored.
\begin{sloppypar}
 \item Simulate the universal automaton \( \Un \) on track \( \cWork \):
   it computes \( \tau_{R,k+1}(a,q)=\Un(\Interpr,R,0^{k+1}, a,q) \)
as needed.  
\end{sloppypar}
\end{enumerate}

This achieves the forced self-simulation.
Note what we achieved:

\begin{itemize}
  \begin{sloppypar}
\item On level 1, the transition function \( \tau_{R,1}(a,q) \) is defined completely
when the rule string \( R \) is given.
It has the forced simulation property by definition, and
string \( R \) is \emph{``hard-wired''} into it in the following way.
If \( (a',q',d)=\tau_{R,1}(a,q) \), then
\begin{align*}
  a'.\cWork=R(q.\Index)
\end{align*}
whenever \( q.\Index \) represents a number between 1 and \( \abs{R} \),
and the values \( q.\Sweep \), \( q.\Addr \) satisfy the conditions
under which the instruction \( \WriteProgramBit \) is 
called in the rules (written in \( R \)).
      \end{sloppypar}

      \begin{sloppypar}
\item The forced simulation property of the \emph{simulated}
transition function \( \tau_{R,k+1}(\cdot,\cdot) \) is 
achieved by the above defined computation 
step---which \emph{relies on} the forced simulation property of \( \tau_{R,k}(\cdot,\cdot) \).
              \end{sloppypar}
\end{itemize}

\begin{remark}
This construction resembles the proof of Kleene's fixed-point theorem.
\end{remark}



\subsection{Transfer phase}\label{sec:TransferPhase}

In the transfer phase, simulated state information will be transferred to the
neighbor colony in the direction of the simulated head movement: this is
called the direction of the transfer, or the \df{drift}.
During this phase, the range of the head
includes the base colony and the neighbor colony
determined by the drift, including a possible bridge between them.

\begin{sloppypar}
The sweep number in which we start transferring in direction \( \delta \) is called
\( \TransferSw(\delta) \), the \df{transfer sweep}.
We have \( \TransferSw(-1) =\TransferSw(1)+1 \).  
\end{sloppypar}

\subsubsection{General structure of the phase}\label{sec:TransferPhase.general-struc}

We will make use of some extra rules that we will
specify in more detail later, but whose role is spelled out here.

The phase consists of the following actions.
\begin{enumerate}
\item
  Spread the value \( \delta \) found in the cells of the \( \cDrift \) track
  (they should all be the same)
  onto the neighbor colony in direction \( \delta \).

There are some details to handle in case the neighbor colony is not adjacent:
see Section~\ref{sec:adjacency}.

\item\label{i:transfer-state} For \( i=1,2,3 \):
        \begin{quote}
          Copy the content of \( \cState \) track of the base colony
            to the \( \cHold[i].\State \) track of the neighbor colony.
        \end{quote}

\item Repeat the following twice:
  \begin{quote}
 Assign the field majority: \( \cState\gets \maj(\cHold[1 \dots  3].\State) \)
in all cells of the neighbor colony.    
  \end{quote}
  
\item If \( \Drift = 1 \), then move right to the left end cell of the neighbor colony
(else you are already there).

        \begin{sloppypar}
          \item In the last sweep (possibly identical with the move step above), 
            in the base colony, if the majority of \( \cHold[j].\Doomed \), \( j=1,\dots,3 \), 
            is 1 then turn the scanned cell into a stem cell: 
            in other words, carry out the destruction.
          \end{sloppypar}


\end{enumerate}

\subsubsection{Transfer to a non-adjacent colony}\label{sec:adjacency}

Let us address the situation when the neighbor colony is not adjacent.

\begin{definition}[Adjacency of cells]\label{def:adjacent}
  Cells \( a \) and \( b \) are \df{adjacent} if \( \abs{a-b}=\B \).
  Otherwise, if \( \B < \abs{a- b} < 2\B \), then
  \( a \) and \( b \) are two \df{non-adjacent neighbor cells}.
For the sake of the present discussion, a \df{colony} is a sequence of \( \Q \) adjacent
cells whose \( \cAddr \) value runs from \( 0 \) to \( \Q-1 \).
It may be extended by a bridge of up to \( \Q-1 \) adjacent cells in the direction of the drift.

If the bodies of two cells are not adjacent, but are at a distance \( <\B \) then the space
between them is called a \df{small gap}.
We also call a small gap such a space between the bodies of two colonies.
On the other hand, if the distance of the bodies of two colonies is \( >\B \) 
but \( <\Q\B \) then the space between them is called a \df{large gap}.
\end{definition}

In the transfer phase, in order to know in a robust local way where the head is,
the \( \cKind \) field of the cells visited will be set as follows.
The base colony has cells of kind \( \Member \) to begin with.
The kind of the cells of the neighbor colony, the target of the transfer, will be set
as \( \Target \) for the duration of the transfer.
However, in the first transfer sweep, if there was a gap between the base and
the target, then cells between them will be created or adapted to form
a bridge that extends the base colony, also extending its addresses.
It is added to the workspace.
A bridge can override an opposite old bridge (``old'' meaning 
that its \( \Sweep \) is maximal) or move 
into an empty area, or kill opposite bridge cells or stem cells while it extends.
If while forming a bridge, another colony is encountered before
the bridge grows to length \( \Q\B \)), then this new colony's cells will get the 
kind \( \Target \), and all will be added to the workspace.
(There can be a gap of size \( <\B \) between the bridge and the neighbor colony.)
Otherwise the bridge itself becomes this neighbor colony, and its cell kinds are
turned to \( \Target \) on the return sweep.

Recall that the \( \NonAdj \) field of the state determines
if the current cell is not adjacent to the cell  where the head came from.
After the transfer stage, we update the \( \NonAdj^{*} \) field encoded in the
\( \cState \) track of the target colony: it becomes 1 if either there is a nonempty bridge,
or there is a gap (found with the help of the \( \NonAdj \) field) between the base colony
and the target colony.
This is done in part~\ref{i:transfer-state} 
of Section~\ref{sec:TransferPhase.general-struc}
again three times, storing candidate values into \( \cHold[j].\NonAdj \)
and repeated with everything else.


\section{Health}            \label{sec:health}

The main part of the simulation uses an error-correcting
code to protect information stored in \( \cInfo \) and \( \cState \) fields.
However, faults can ruin the simulation structure and disrupt the simulation itself.
The error-correcting capabilities of the code 
used to store the information on the \( \cInfo \) and
\( \cState \) tracks, will preserve the content of these tracks as long as the coding-decoding
process implemented in the simulation is carried out.
The structural integrity of a configuration is maintained with the help of a small number
of fields.
Below we outline the necessary relations among them 
allowing the identification and correction of local damage.

A configuration with local structural integrity will be called \df{healthy}.
\begin{sloppypar}
\begin{remark}
In all discussions of the health of a configuration \( \xi=(q,A,\h,\hc,) \), we can ignore
the current cell position \( \hc=\xi.\curcell \), since at all switching times 
it agrees with \( \h=\xi.\pos \).
\end{remark}
  \end{sloppypar}

No cell in a healthy configuration should have marks of a rebuild procedure.
Larger bursts may also introduce new, non-local anomalies: these can only be recognized
once the local anomalies have been corrected.
Cells of a healthy configuration are grouped into gapless colonies, and 
a few transitional segments described below.
The big picture is this:
\begin{itemize}
\item There is a base colony, possibly extended by a bridge in the direction of
  the drift.
Possibly, in the direction of the drift, the neighbor colony of the base is a 
\df{target}, its cells are marked as such.
\item Non-base colonies are called \df{outer colonies}.
If an outer colony is not adjacent to its neighbor colony closer to the base,
then it is extended by a bridge that covers this gap.

A partial exception is the colony \( C \) closest to the base colony in the direction of the
drift.
If there is a gap between it and the base colony then initially it is covered by 
a bridge extending \( C \), but in the first transfer sweep this bridge will be
overridden by the bridge extending the base colony.
\end{itemize}
\Pnote{Pictures!}

\begin{definition}[Outer cells]\label{def:outer-cells}
    Recall the definition of the sweep value
    \(  \Last(\delta)  \) from~\eqref{eq:Last}.
    For \( \delta \in \{ -1,1 \} \), if a cell is stem or
\( \cDrift = \delta \),  \( \cSweep = \Last(\delta) \)
    then it will be called a \df{right outer cell} if
    \( \delta = -1 \), \( -\Q< \cAddr < \Q \),  and a \df{left outer cell} 
    if \( \delta = 1 \), \( 0\le\cAddr<2\Q-1 \).
\end{definition}

According to this definition, a stem cell is both a left and a right outer cell.

\begin{definition}[Segments]\label{def:segments}
The following possible sequences of neighbor cells will be called a \df{(homogenous) segment}:
\begin{description}
\item[Desert] A sequence of neighboring (not necessarily adjacent) stem cells.

\item[Workspace segment]  A sequence of adjacent neighbor 
cells that could form part of the workspace and does not include the front.
It consists either of only target cells or of cells of an extended base colony.

\item[Outer segment]
A sequence of outer cells that could belong to the same extended colony.
\end{description}

The \df{left end} of a segment is the left edge of its first cell, and its \df{right end} is 
the right edge of its last cell.

A \df{colony} has addresses grow from \( 0 \) to \( \Q-1 \), 
possibly continued by a bridge of size \( <\Q \) on one side,
and a right end segment.
It is a \df{target} if it consists of target cells.
A target is never extended by a bridge.

A \df{boundary} of a homogenous segment is called \df{rigid} if its address is the end 
address of a colony in the same direction.

A \df{boundary pair} 
is a right boundary followed by a left boundary at distance \( <\B \).
It is a \df{hole} if the distance is positive.
It is \df{rigid} if at least one of its elements is.
\end{definition}

In a healthy configuration, cells fall into certain categories.
Outer cells are member cells in colonies other than the ones that 
are currently being manipulated.

Recall the definition of the \df{transfer sweep}
\( \TransferSw(\delta) \) in Section~\ref{sec:TransferPhase}, if \( \delta \ne 0 \).
(There is no transfer sweep if \( \delta = 0 \).)

\begin{definition}[Healthy configuration]\label{def:healthy}
The health of a configuration \( \xi \) of a generalized Turing machine 
\( M \) will be defined over a certain interval \( A \).
It depends on the state, 
on \( \xi_{\tape}(A) \), further on whether \( \xi_{\pos} \) is in \( A \) and
if it is, where.
But we will mention the interval \( A \) explicitly only where it is necessary.
In particular, some of the structures described below may fall partly or fully
outside \( A \).
We require that mode be normal, and the following conditions hold, with \( \delta=\Drift \).

\begin{description}

\item[Normality]
  No cell in \( A \) is marked for rebuilding (recall the notion of marking from
Section~\ref{sec:language}):
that is, for every \( x \in A \), \( \Rebuild.\cSweep(x) = 0 \).

\item[Segments]
  The \df{base} is defined by counting back from \( \Addr \).
This is simple as long as we are within one colony.
However, when we are passing from a target cell to a member cell, then addresses on
the tape will undergo a \df{jump}: we set \( \Addr \) to \( \cAddr \) before
continuing the count-back.

  All cells can be grouped into full extended colonies,
  with possibly some stem cells between these.
  In more detail:
  \begin{itemize}
  \item An \df{extended base colony} consisting of member cells and bridge cells.
  \item \df{Extended outer colonies}, consisting of outer member cells.
  \item A possible target, defined by the value of \( \cSweep \) in its cells.
  \item Desert filling out the gaps between the above parts.
  \end{itemize}

  To define the non-base segments, we consider several cases.
  \begin{itemize}
  \item 
    If \( \delta=0 \) or \( \Sweep < \TransferSw(\delta) \), 
    then there are no bridge or target cells.

  \item If \( \TransferSw(\delta) +1 < \Sweep <  \) (the last two sweeps for \( \delta \)),
    then there is a target colony in the direction \( \delta \).
    If its distance from the base colony is \( \ge\B \) then the base
    colony is extended by a bridge filling the gap.
    
  \item If \( \Sweep = \TransferSw(\delta) \) then the above 
    described situation is in the making, as
    a bridge is being built up in direction \( \delta \), or after that,
    a target is being built in direction \( \delta \), converting member cells into target cells.
          
  \item If \( \Sweep = \TransferSw(\delta) +1 \) and there is still not a complete target colony,
    then the whole bridge is being converted into a target, 
    as the head is traveling in direction \( -\delta \).
    
  \item In the last sweep, the target cells are being converted into member cells.
  \end{itemize}
  These are the only possible segments to be seen in a healthy area.
        
    \item [The front] 
      The farthest position \( \front(\xi) \) to which the head has 
      advanced before starting a new zig is called the \df{front}:
      it can be computed from the fields \( \xi.\pos \) and \( \ZigDepth \) of the state, 
      but can also be reconstructed from the tape, namely from the \( \cSweep \) track.
      It is always inside the extended base colony or the target.

     \item[Workspace]

       \begin{sloppypar}
          The \df{workspace} is an interval of non-outer cells, such that:         
        \end{sloppypar}
        
        \begin{itemize}
        
        \item For \( \Sweep < \TransferSw(\delta) \), it is equal to the base colony.

        \item In case of \( \Sweep = \TransferSw(\delta) \), it is the smallest interval including
              the base colony and the cell neighboring to \( \front(\xi) \) on the side of the base colony.

        \item If \( \TransferSw(\delta) < \Sweep < \Last(\delta) \),
              then it is equal to the union of the extended base colony and the target.

        \item When \( \Sweep = \Last(\delta) \), it is the smallest interval including the target
              (future  base) colony and \( \front(\xi) \).
        \end{itemize}

   \end{description}

A tape configuration is called \df{healthy} on an interval \( A \)
when there is a head position (possibly outside \( A \) ) and a 
state that turns it into a healthy configuration.
 \end{definition}

 \begin{definition}
Let the tuple of fields
\begin{align*}
   \Core =(\Addr, \Sweep, \Drift, \Kind)
 \end{align*}
is called the \df{core}.   
 \end{definition}

Note that health only depends on the fields in \( \Core \) and
the zigging field \( \Z \) in the state, further the  \( \cCore \) track
and the lack of marks on the tape.
Note also that in a healthy configuration every cell's \( \cCore \) field
determines the direction in which the front is found, from the point of
view of the cell.

A violation of the health requirements can sometimes be noted immediately:

\begin{definition}[Coordination] \label{def:coordinated}
   The state of the machine is \df{coordinated} with the current cell
   if it is possible for them to be together in a healthy configuration.
\end{definition}

Recall that in Rule~\ref{alg:main1}, in normal mode,
if lack of coordination is discovered then the healing procedure is called.
The following lemmas show 
how local consistency checking will help achieve longer-range consistency.

\begin{lemma}\label{lem:coordination1}
  In a healthy configuration, each \( \Core \) 
  value along with \( \Z \) determines uniquely the 
  \( \cCore \) value of the cell it is coordinated with, with the following exceptions.
  \begin{itemize}
  \item During the first transferring sweep, while
    creating a bridge between the base colony and the target colony, 
    the front can be a stem cell or the first cell of an outer colony.
  \item Every jump backward from the target colony can end up on the last cell of a 
    bridge (whose address is not recorded in the state) or the last cell of the base colony.
  \end{itemize}
  \end{lemma}
  \begin{proof}
  To compute the values in question, calculate \( \Z \) steps backwards from the front,
  referring to the properties listed above.
\Pnote{Elaborate!}    
  \end{proof}

\begin{lemma}[Health extension]\label{lem:health-extension}
  Let \( \xi \) be a \emph{tape} configuration that is healthy on intervals \( A_{1}, A_{2} \) 
where \( A_{1}\cap A_{2} \) contains a whole cell body of \( \xi \).
Then \( \xi \) is also healthy on \( A_{1}\cup A_{2} \).
\end{lemma}
\begin{proof}
  The statement follows easily from the definitions.
\Pnote{Elaborate!}    
\end{proof}

In a healthy configuration, the possibilities of finding non-adjacent neighbor
cells are limited.

\begin{lemma}\label{lem:two-domains}
  An interval of size \( <\Q \) over which the configuration \( \xi \) is healthy
contains at most two maximal sequences of adjacent non-stem neighbor cells.
\end{lemma}
\begin{proof}
Indeed, by definition a healthy configuration consists of full extended colonies, with 
possibly stem cells between them.
An interval of size \( <\Q \) contains sequences of adjacent cells 
from at most two such extended colonies.
\end{proof}

% Let us classify the boundary pairs possible in a healthy configuration.
% Rigid pairs:

% \begin{enumerate}[label=\upshape{(r\arabic*)}, ref=r\arabic*]
% \item\label{i:rigid.outer-workspace}
%   Between an outer extended colony or a desert, and a colony closer to the base.
% \item\label{i:rigid.bridge-target} Between the extended base colony 
% and the target or an outer colony.
% \end{enumerate}

% Non-rigid pairs are at the front:
% \begin{enumerate}[label=(nr\arabic*), ref=nr\arabic*]

%   \item\label{i:nr.bridge-bridge} End of a new bridge in direction \( \delta \)
% not within distance \( <\B \) of a rigid boundary of a colony of drift \( -\delta \) or its own target.
% On the other side is either desert or the end of an old bridge.

%   \item\label{i:nr.aligned} Between aligned segments:
%     \begin{enumerate}[label=(\arabic*), ref=nr\arabic{enumi}.\arabic*]
%      \item\label{i:nr.aligned.differ-1} Between sweep values differing by 1,
%      \item\label{i:nr.aligned.bridge-target} between a new bridge and the target it is being converted into,
%      \item\label{i:nr.aligned.target-member} 
% between a target in direction \( \delta \) and the remaining segment of member cells, of drift \( -\delta \),
%      \item\label{i:nr.aligned.member-target}  between a target and the member cells replacing it in the last sweep,
%      \item\label{i:nr.aligned.internal-end} 
% between an internal segment and an end segment (see Definition~\ref{def:segments}).
%     \end{enumerate}

% \end{enumerate}

% The following is worth noting.

% \begin{lemma}
% In a healthy configuration, any interval of size \( <\Q\B \) contains at most 3 boundary pairs,
% only one of which can be  a hole.
% \end{lemma}
% \begin{proof}
% Any interval of size \( <\Q\B \) contains at most one rigid left boundary and
% one rigid right boundary.
% Any nonrigid boundary coincides with the front.
% A hole can exist only at the end of a bridge.
% If there are two bridges, then the hole can only be between their ends.
% \end{proof}

\begin{lemma}\label{lem:infer-between}
In a healthy configuration, 
a cell's content shows whether it is an outer cell, colony cell, bridge cell or target cell.
It also shows whether the cell is to the left or to the right of the front.

The \( \cCore \) track of a homogenous segment can be satisfactorily reconstructed
from its endcells.
The reconstruction unique, with one exception: in case the segment is not internal: the
\( \cSweep \) values are not uniquely defined, only the parity and the direction of its 
change between the ends.
\end{lemma}
\begin{proof}
The information mentioned in the first sentence is explicit in some fields.
About the front: if the cell is outer then \( \cDrift \) shows its direction from the front.
In case it is not outer, then the parity of \( \cSweep \) shows it: odd values are on the left, even 
ones on the right. 

The uniqueness of reconstruction is a direct consequence of the definitions.
\end{proof}

\section{Healing and rebuilding}\label{sec:healing}

\subsection{Annotation, admissibility, stitching}\label{sec:stitching}

In this section we show how to correct configurations of machine \( M \)
that are ``almost'' healthy.

\begin{definition}[Annotation]\label{def:annotation}
  An \df{annotated configuration} is a tuple
  \begin{align*}
    (\xi,\chi, \cI),
  \end{align*}
  with the following meaning.

  \( \xi \) is a configuration.

  \( \cI \) is a set of disjoint intervals called \df{islands}.
Their complement is clean.

  \( \chi \) is a healthy configuration differing from \( \xi \) only in the islands.

The \df{base colony} and the \df{workspace} of \( (\xi,\chi, \cI) \) are those of \( \chi \).
The head is \df{free} when it is not in any island,
and the state is in normal mode, coordinated with the observed cell.
  \end{definition}

\begin{definition}[Admissibility]\label{def:admissible}
An annotation is \df{admissible} on an interval \( K \) if the following holds on any subinterval 
of \( J\subseteq K \) of size \( \Q\B \):
\begin{enumerate}[label=\upshape{\alph*)},ref=\thelemma.\upshape{\alph*},series=admissible]
\item The dirt of \( J \) is covered by 3 intervals, each of size \( \le \beta'\B \).
\item There are at most 3 islands in \( J \), each of size at most \( \cns{island}\beta\B \).
\end{enumerate}
A configuration is \df{admissible} on \( K \) if it can be annotated in a way admissible on \( K \).
\end{definition}

We will show that a configuration admissible over an interval of a certain size can be locally corrected;
moreover, in case the configuration is clean then this correction
can be carried out by the machine \( M \) itself.
For the time being, we will just talk about an admissible configuration, without specifying the interval
\( K \).

We will deal with the cleaning process later, but it will imply that,
in the absence of new noise, islands will not grow much, and the ones near the head
will be eliminated.

\begin{definition}[Substantial segments]\label{def:substantial}
Let \( \xi(A) \) be a tape configuration over an interval \( A \).
A homogenous segment of size at least \( 7\cns{island}\beta\B \)
will be called \df{substantial}.
The area between two neighboring maximal
substantial segments or between an end of \( A \) and the closest substantial segment in \( A \)
will be called \df{ambiguous}.
It is \df{terminal} if it contains an end of \( A \).
  Let
 \begin{align*}
     \Delta &= 39\cns{island}\beta, % ,\quad \Delta'=\Delta+3\cns{island}\beta. %?
\\  \E  &\ge 6\Delta. %?
 \end{align*}
We assume that \( \E \) is an integer multiple of
the parameter \( \F \) introduced for feathering in Section~\ref{sec:zigging}.
\end{definition}

\begin{lemma}\label{lem:ambiguous}
Consider an admissible configuration.
In a substantial segment, each half contains at least one cell outside the islands.

If an interval of size \( \le  \Q\B \) of a tape configuration \( \xi \) differs from a  healthy tape 
configuration \( \chi \) in at most three islands, then 
the size of each ambiguous area is at most \( \Delta\B \).
\end{lemma}
\begin{proof}
The first statement is immediate from the definition of substantial segments 
There are at most 3 boundary pairs in \( \chi \) at a total size of \( 3\B \), and
3 islands of size \( \le\cns{island}\beta\B \).
There are at most 5 non-substantial segments of sizes \( < 7\cns{island}\beta\B \)
between these: this adds up to
\begin{align*}
 < (3+3\cdot\cns{island}\beta+ 5\cdot 7\cdot\cns{island}\beta)\B<39\cdot\cns{island}\beta\B=\Delta\B.
 \end{align*}
\end{proof}

The following lemma forms the basis of the cleaning algorithm.

\begin{lemma}[Stitching]\label{lem:stitching}
In an admissible configuration, inside a clean interval,
let \( U,W \) be two substantial segments separated by an ambiguous area \( V \).
It is possible to change the cell content \( U,V,W \) using only information in \( U,W \) in such a 
way that the configuration over \( U\cup V\cup W \) becomes healthy.
Moreover, it is possible for a cellular automaton to do so gradually, keeping admissibility, and
changing the cell content in \( U \) or \( W \) or enlarging \( U \) or \( W \) gradually at the expense of \( V \).
\end{lemma}
We do not worry about the feathering property now, since the cellular automaton in question may choose
where to make its turns.
\begin{Proof}
We distinguish several cases, based on the kind of segments involved.
At any step, if we find that \( U\cup V\cup W \) is healthy then we stop.
\begin{step+}{step:stitching.mergeable}
Assume that \( U,W \) both belong to the same extended colony: either the extended base colony, or 
an outer extended colony, or the target.
\end{step+}
\begin{prooof}
If both belong to the interior of the area indicated, then the merging is simple.
In this case, by Lemma~\ref{lem:infer-between}, the content of the \( \cCore \) track 
of \( V \) in the healthy configuration is completely determined by that of \( U,W \),
So the intervals \( U \) and \( W \) can be gradually extended towards each other in any order, 
overtaking \( V \).

Suppose that they do not belong to the interior and are, for example towards the right from it.
In this case, the \( \cSweep \) value may be decreasing in both \( U \) and \( W \) to the right.
But since \( U,W \) themselves may overlap with islands, it may happen that the \( \cSweep \) value
on the right end of \( U \) is smaller than the \( \cSweep \) value on the left end of \( W \).
In this case, before extending \( U,W \) towards each other, the \( \cSweep \) values in both
may need to be changed.

By Lemma~\ref{lem:ambiguous}, the left and right halfs of \( U \) contains a cell \( u_{1},u_{3} \) 
each, not belonging to any island, and similarly with \( w_{1},w_{3} \) in \( W \).
Let \( u_{2} \) be leftmost cell of the right half of \( U \) and \( w_{2} \) the rightmost cell of the left 
half of \( W \).
Then
\begin{align*}
 \cSweep(u_{1})\ge \cSweep(u_{2})\ge \cSweep(u_{3})\ge \cSweep(w_{1})\ge \cSweep(w_{2})\ge \cSweep(w_{3}).   
 \end{align*}
So the transformation will change \( \cSweep \) to \( \cSweep(u_{2}) \) everywhere in the right half of \( U \),
and to \( \cSweep(w_{2}) \) everywhere in the left half of \( W \).
After this, it will extend \( U \) towards \( W \), overtaking \( V \) and keeping \( \cSweep \) constant.

In both of the above cases, if \( U  \) and \( W \) belonged to different sides of the front, then the
new front will be the new boundary between \( U \) and \( W \).
\end{prooof} % step:stitching.mergeable

\begin{step+}{step:stitching.not-front}
Suppose now that \( U,W \) do not belong to the same extended colony, but they are on 
the same side of the front: without loss of generality, to the left of it.
\end{step+}
\begin{prooof}
In this case, only one of \( U \) can be outer: suppose that it is.
Now \( W \) is either in a target or in the base colony.
The base colony cannot have an extension to the left, because given that the front
is to the right, the same sweep that created the extension would have created already a target, 
separating \( U \) from \( W \) too much.
So in both cases, \( W \) is close to the left end of its colony which is in \( V \).
It must be extended to this left end.
Following this, \( U \) must be extended until its reaches \( W \).
The \( \cSweep \) values can extended without change, there is no need to coordinate them
between \( U \) and \( W \).

Suppose that \( U \) is a target, and then \( W \) belongs to the extended base colony.
Since both \( U \) and \( W \) are in the interior, the \( \cSweep \) values must be equal, so they
can be extended without change.
Then \( U \) must be extended until its endcell, and then \( W \) must be extended to meet \( U \).
Since both \( U \) and \( W \) are in the interior, the \( \cSweep \) values must be equal, so they
can be extended without change.
\end{prooof} % step:stitching.not-front

\begin{step+}{step:stitching.front}
Suppose that \( U \) and \( W \) belong to different extended colonies, with the front between them.
\end{step+}
\begin{prooof}
The \( \cSweep \) values can extended without change, there is no need to coordinate them
between \( U \) and \( W \).

Suppose that \( U \) contains no bridge cells: in this case extend it to the right until it hits
a colony endcell.
If you overlap \( W \), decrease \( W \) accordingly.
Due to admissibility, only bridge cells of \( W \) can be overwritten in this way.
Similarly, if \( W \) contains no bridge cells then extend it to the left, until it hits a colony endcell.
Again, only bridge cells of \( U \) can be overwritten in this way.

Only one of \( U \) and \( W \) can be in an outer extended colony, suppose that \( U \) is.
If \( W \) is a target then we already extended it to its limit.
Now extend the bridge of \( U \) to meet it.
If \( W \) is in an extended base colony then extend its bridge until either meets \( U \) or
reaches full length.
In the latter case, extend the bridge of \( U \) to meet \( W \).

If \( U \) is in a target then it is already at its right end: we extend the bridge of \( W \) to meet it.
\end{prooof} % step:stitching.front
\end{Proof}


\subsection{The healing procedure}\label{sec:healing-proc}

Structure repair will be split into two procedures.
The first one, called \df{healing}, performs
only local repairs of the structure: for a given (locally) admissible configuration,
it will attempt to compute a satisfying (locally) healthy configuration.
If it fails---having encountered a configuration that is not admissible, or
a new burst---then the \df{rebuilding} procedure is called, which is designed
to repair a larger interval.
On a higher level of simulation, 
this corresponds to the implementation of the ``cleaning'' trajectory properties.
The healing procedure runs in \( O(\beta) \) 
steps, whereas rebuilding needs \( O(\Q^{2}) \) steps. \Pnote{maybe even \( \Q^{3} \)?}

The description of the procedures looks as if we assumed that there is no noise or dirt.
The rules described here, however (as will be proved later), will clean an area locally under the 
appropriate conditions, and will also work under appropriately moderate noise.

The healing procedure does not even protect itself against any possible noise during it.
The only protection is that any one call of the healing procedure will change only 
a small part of the tape, essentially one cell: so a noise burst will have limited impact even if it
happens during healing.

One possible outcome of the healing procedure is \df{failure}. 
In this case the plan is to mark a ``germ'' of \( 4\beta \) cells and call rebuilding.
The healing procedure carries  out only one step of this plan, then it calls itself again.

Recall the parameters \( \Delta,\E \) introduced in Definition~\ref{def:substantial}.
Suppose that \( \rHeal \) is called at some position \( \z \).
Then it sets
\begin{align*}
\Mode\gets\Healing,\ \Heal.\Sweep \gets 1,\ \Heal.\Addr = 0.
 \end{align*} 
The healing procedure starts by surveying an interval \( \R \) of at least \( 2\E \) cells 
around its starting point \( z \).
(It will go a little farther than \( \E \) cells, in search of an allowed turning point.
If such a point is not found within \( 3\beta+\F \) steps, healing fails.)
Whenever the head steps on a stem cell or creates a new cell, 
\( \cDrift \) is set to point to \( \z \) (to make sure that the head does not get
lost in the desert).

Suppose that in the survey a pair of substantial segments separated by an ambiguous area is found,
that is at least \( \Delta \) removed from the complement of \( R \). 
Choose the ambigous area closest to the center (of healing).
Then the first needed ``stitching'' operation (a single step) as defined in Lemma~\ref{lem:stitching}
is determined, and is performed.
If the ambigous area still remains, go to its leftmost cell.
and restart healing.
In case that we started from a clean admissible configuration, this operation creates a new admissible
configuration that is one step closer to being healthy.

If the required stitching operation is not found then, 
if the surveyed area is found inadmissible, healing fails.
Otherwise, if the head is at the front, healing is declared completed.
Otherwise, the healing center is moved one step closer to the front, and healing is restarted.

The healing operation defined this way has the following property.

\begin{lemma}\label{lem:combined-heals}
  Assume that the head moves in a noise-free and clean space-time rectangle \( \lint{a}{b}\times \lint{u}{v} \),
with \( b-a>3\E\B \), \( v-u>2\E\Tu \),
touching every cell of \( \lint{a}{b} \) at least once, and never in rebuilding mode.
Then at time \( v \), the area \( \lint{a+1.5\E\B}{b-1.5\E\B} \) is healthy.
\end{lemma}
\begin{proof}
In healing, the head will not move away from an ambiguous area before eliminating it, creating a healthy interval
\( I \) containing the two substantial intervals that originally bordered it.
If the head leaves this interval in normal mode and the zigging does not start new healing then the
healthy interval covered by zigging can be added to \( I \) to form an even larger healthy interval,
and this continues until new healing starts.
One of the substantial intervals of the new healing will be inside \( I \), and when it is completed,\( I \) will be
extended further.
The head may later return into the healthy interval \( I \), but it will then just continue the simulation without
affecting health while stying inside.  
\end{proof}

\subsection{Rebuilding}\label{sec:rebuilding}

If healing fails, it calls the rebuilding procedure.
This indicates that the colony structure is ruined in an interval of size larger than 
what can be handled by local healing.
Just as with healing, we will be speaking here only about a situation with no 
mention of dirt---but the final analysis will take dirt into account.

Since rebuilding makes changes on an interval of the length of several colonies,
it is important not only that it is invoked when it is needed, but that it is not
invoked otherwise!
A failed healing ends on a ``germ'' of cells marked for rebuilding,
with the state in rebuild mode.
Rebuilding starts by extending the germ to the left, in a zigging way,
expecting rebuild-marked cells on the backward zig.
Since the zig is larger than a burst, if rebuilding started from just a burst then this
will trigger healing, thus leaving the rebuilding mode.

Here is an outline, for rebuilding that started from a cell \( z \) in the middle
of the germ.
Recall the stitching operation from Section~\ref{sec:stitching}.
\begin{description}
 \item[Mark] Starting from the germ, extend a rebuilding area  over \( 3\Q \) cells to the left 
and \( 3\Q \) cells to the right from \( z \).
Mark the area using the addresses \( \Rebuild.\cAddr_{j} \) for \( j=\pm 1 \), where addresses are counted from both 
ends of the rebuilding area, and the track \( \Rebuild.\cSweep \).
Use zigging to make sure that false rebuilding started by a burst will be recognized (since we started from a germ,
the zigging must always see more than \( \beta \) marked cells already present).
Also in what follows, every step that changes the configuration must be accompanied by zigging, to check that
the rebuilding is indeed going on.

 \item[Survey and Create]
The details of this stage will be outlined below.
It looks for existing colonies (possibly needing minor repair) in the rebuilding area, and 
possibly creates some.
As a result, we will have one colony called \( C_{\Left} \) 
on the left of \( z \), one called \( C_{\Right} \) on the right of \( z \),
and possibly some colonies between them.
Make all these colonies represent stem cells.
Declare \( C_{\Left} \) the base colony, direct all the others
with drifts and bridges towards it.
(The creation of a bridge may result also in the creation of a new 
colony if the bridge becomes \( \Q \) cells long.)
Survey and possibly change additional \( 0.5\Q \) cells on the left of \( C_{\Left} \) and \( 0.5\Q \) cells
on the right of \( C_{\Right} \), making the whole interval healthy.
This interval will be called the \df{output interval} of rebuilding.

\item[Mop] Remove the rebuild marks, shrinking the rebuilding area onto the left end of \( C_{\Left} \).
\end{description}

\subsubsection*{Details of the Survey and Create stage}

\begin{enumerate}[label=s\arabic*., ref=s\arabic*]
\item\label{i:survey.left}
In the marked area for substantial segments,  search on left of \( z \) for a colony \( C \),
or a set of cells that looks stitchable by up to 3 stitches into a colony.
The first substantial segment should be at least \( 3\beta \) cells to the left of \( z \),
to make sure that \( C \) is \df{manifestly} to the left of \( z \).
If the search and the stitching attempt are successful, mark \( C \) as \( C_{\Left} \).

Repeat this search for a colony manifestly to the right of \( z \): if found, call it \( C_{\Right} \).

\item Suppose that only \( C_{\Left} \) is found, then create \( C_{\Right} \)
The other case is symmetrical.

\item Suppose that neither \( C_{\Left} \) nor \( C_{\Right} \) have been found.
Then search the whole area, starting from the left, for a colony.
If no such colony is found then create \( C_{\Left} \) and \( C_{\Right} \).

\item\label{i:survey.both} Suppose that both \( C_{\Left} \) and \( C_{\Right} \) 
have been determined (found or created).
Then search between them, from the left, for (stitcheable) colonies, one-by-one.

\item Suppose that only a colony has been found that 
is not manifestly to the left or right of \( z \); then either the left end is manifestly to the left or
the right end is manifestly to the right of \( z \).

Suppose that the left end it is manifestly to the left of \( z \), then call the colony \( C_{1} \).
Then create \( C_{\Left} \) on the left of  \( C_{1} \).
Now search for another one on its right (it is not manifestly on the right).
If not found, create \( C_{\Right} \), manifestly on the right of \( z \).
If found call it \( C_{2} \), and create \( C_{\Right} \) on its right.
The other case is symmetrical.

\end{enumerate}

The steps above requiring the creation of colonies and bridges are destructive
(the stitching ones are not).
Therefore before a creation step, the whole survey preceding it is performed twice, 
marking the result in all rebuilding cells.
The required actions (erasing cells in order to build new ones in their place)
are then only performed if the two survey results are identical---otherwise alarm is called.

Rebuilding also obeys feathering: marked turning points allowed for 
rebuilding must have \( \Rebuild.\cAddr\equiv\pm \tRebuild\pmod{\F} \).

How to defend against the effects of a burst during the rebuild procedure?
There is only limited defense.
The major decisions on creation are made twice, with the two results compared.
Otherwise if the usual zigging (with simple consistency check on the rebuild addresses
and sweeps) fails, healing is called.
This will most likely start another rebuilding, which now will operate without a burst.
Traces (say, marked cells) from the first, interrupted rebuilding might remain, and trigger a new
heal-rebuild cycle if found at the mopping stage.

The following lemma is an immediate consequence of the definition of rebuilding.

\begin{lemma}\label{lem:rebuild-health}
Suppose that a rebuilding procedure runs noiselessly from start to finish in a clean interval \( I \),
starting on the boundary of a subinterval \( J\subset I \) that is super-healthy at the beginning.
If \( K \) denotes the output interval of rebuilding, then \( J\cup K \) will be healthy.
The part spanned from the leftmost to the rightmost colony will be super-healthy.
\end{lemma}

\section{Super annotation and scale-up}

It is convenient to introduce some additional structures when discussing
the effects of moderate noise and their repair.

\subsection{Super annotation}\label{sec:annotation}

As indicated in Section~\ref{sec:model}, when dealing with the behavior
of machine \( M \) over some space-time rectangle, we will assume that the noise
over this rectangle is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tu^{*}}) \)-sparse.
With Definition~\ref{def:Tu} of \( \Tu^{*} \) this means 
in simpler terms that at most one noise \df{burst} affecting an
area of size at most \( \beta\B \) can occur in any \( \gamma \) consecutive work periods.
In the present section, histories will always be assumed to have this property.

Some histories lend themselves to be viewed as a healthy development
that is disturbed only in some well-understood ways.
Added to such histories the information pointing out these disturbances will be called an annotation.
The proof of the error-correcting behavior of machine \( M \) (essentially the proof of the Transition Function
property of trajectories of Definition~\ref{def:traj} for the simulated machine \( M^{*} \))
will take the form of showing the possibility of annotation under sparse of noise.
An annotation, as per Defnition~\ref{def:annotation}, marks some ways in which the health 
of a tape configuration has been be affected.
Now extend annotation in order to deal with damage not only to health but also to information.

 \begin{definition}[Super healthy]\label{def:super-healthy} 
A configuration is
\df{super healthy} if in addition to the requirements of health, in each colony,
whenever the head is not in the last sweep, the \( \cInfo \) and \( \cState \)
tracks contain valid codewords as defined in Section~\ref{sec:coding}.
A configuration \( \xi \) defined on an interval \( I \) is \df{(super) healthy}
on \( I \) if it can be extended to a (super) healthy configuration.
\end{definition}


\begin{definition}[Super annotation]\label{def:super-annotation}
  A \df{super annotated configuration} is a tuple
  \begin{align*}
    (\xi,\chi, \cI, \cS),
  \end{align*}
  with the following meaning.

\( (\xi,\chi,\cI) \) is an annotated configuration.

\( \cS \) is a set of disjoint intervals called \df{stains}.
All islands are contained in stains.

We can change \( \chi \) into a super healthy configuration by changing it only in the stains.
  \end{definition}

Recall Definition~\ref{def:admissible} of admissibility.
  \begin{definition}[Super admissibility]\label{def:super-admissible}
A super annotation is \df{super admissible} on an interval \( K \) if it is admissible on \( K \),
further consider any interval \( J\subseteq K\) of size \( \le\Q\B \).
\begin{enumerate}[label=\upshape{\alph*)},ref=\thelemma.\upshape{\alph*},resume=admissible]
\item At most 3 stains intersect \( J \).
\item If \( J \) contains \( k \) stains, surrounded by some healthy area
(we already know \( k\le 3 \)), while the head is at a distance \( >2\B \) within the clean area,
then the total size of these stains is at most \( k\cdot\cns{stain}\B \), where
\begin{align}\label{eq:stain}
   \cns{stain} = (\F+2)\beta.
 \end{align}
\item\label{i:super-annotated.two-stains} If a colony in \( J \) outside the workspace intersects two stains then the 
simulated cell state decoded from it has \( \cCanTurn = \false \).
\end{enumerate}
\end{definition}

Let us motivate requirement~\ref{i:super-annotated.two-stains}.  
One stain can arise and remain somewhere for example if a burst occurs in the
last sweep of a work period at the bottom of a zig.
A second stain can remain at the end of a work period in which the simulated
machine makes a turn---this sets \( \cCanTurn\gets\false \) in the simulated
cell.
Requirement~\ref{i:super-annotated.two-stains} says that in an annotated
configuration, this is the only way for two stains to remain outside the
workspace.

A configuration may allow several possible super annotations;
however, the valid codewords (referred to in the definition
of super health) that can be recovered from it do not depend on the choice of the
annotation.


The following definitions help extend the notion of annotation to histories.

\begin{definition}[Distress and relief, safety]\label{def:distress}
Consider a sequence of annotated configurations over a certain time interval.
If the head is free (see Definition~\ref{def:annotation}), then
the time (and the configuration) will be called \df{distress-free}.
A time that is not distress-free and is preceded by
a distress-free time will be called a \df{distress event}.
This can be of two kinds: the head steps onto an island, or a burst occurs
(creating an island and leaving the head in it).

% The direction of the last \( \Z \) non-turning moves of the front before the distress event
% will be called the \df{pre-distress sweeping direction}.

Consider a time interval \( K \) starting with a distress event and
ending with a distress-free configuration.
Let \( J \) be the interval of tape where the head passed during \( K \), then 
we will call \( J\times K \) a \df{relief event}, if the following holds.
% the free head making \( \Z \) non-turning moves.
% Their direction is called the \df{post-distress sweeping direction}.
% If it coincides with the pre-distress sweeping direction then we will say that
% \df{no turn} occurred.

\begin{alphenumIn}
\item Any new islands occurring in \( J \times K \) are due to some new burst.
\item The island that started the distress event disappears by the end of \( K \).
% \item Provided no turn occurred, the only possible island intersecting \( J \) 
% at the end of \( K \) belongs to some island caused by a burst during \( K \).
% (In other words, the \emph{old} islands will be eliminated in \( K \).
% This is always the case if the distress occurs in the interior of a colony.)
\end{alphenumIn}
\end{definition}

In a relief event, it is possible to leave behind some islands other than the one initiating the distress
if the sweeping direction changes during it.
Consider the situation in Example~\ref{xmp:3-islands}.
In the work period where the head deposits island \( I_{2} \) near the left colony
end, it may repeatedly dip into \( I_{2} \) at the descending end of a zig at a
right turn.
During this dip it can expand the islands
\( I_{1}, I_{2} \) and then emerge on the right, with the healing unfinished.
 
We will consider the annotation of histories over a limited space-time region, but will
not point this out repeatedly.

\begin{definition}[Annotated history]\label{def:annotated-hist}
An \df{annotated history} of a generalized Turing machine
    \begin{align*}
        M=(\Gamma,\Sigma,\tau,q_{\start},F,B,\Tu{})
     \end{align*}
is a sequence of \emph{super} annotated configurations such that
the sequence of underlying configurations is a trajectory, and 
every distress event is followed by a relief event \( J\times K \) with 
\( \abs{J}=O(\beta\B) \) and \( \abs{K}= O(\beta^{2}\Tu) \). %?
\end{definition}

In what follows we will show that for any trajectory \( (\eta, \Noise) \) of a
generalized Turing machine \( M \) on any space-time rectangle on which the
noise is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tu^{*}}) \)-sparse, if at the
beginning the configuration was super healthy then the history can be annotated.
In the rest of the section we always rely on the assumption of  this 
sparsity property of the noise.

The main part of the proof is about obtaining relief  after a distress event.
Unlike in~\cite{burstyTuring13}, now islands 
may have their cell structure damaged: may contain dirt.
However, since \( \eta \) is a trajectory, as we will see the islands will be cleaned out.
So, relief will be made up of two stages: cleaning, and correcting the structure.
This division is only possible for an observer: the machine has no
``dirt-detector'', we just rely on the cleanness-extending properties of a
trajectory introduced in Definition~\ref{def:traj}.


\subsection{The simulation codes}\label{sec:sim-codes}

Let us now define formally the codes \( \varphi_{*k},\Phi_{k}^{*} \) that are needed
for the simulation of history \( (\eta^{k+1},\Noise^{(k+1)}) \) by history \( (\eta^{k},\Noise^{(k)}) \).
Omitting the index \( k \) we will write \( \varphi_{*},\Phi^{*} \).
To compute the configuration encoding \( \varphi_{*} \) we proceed first as
done in Section~\ref{sec:hier-codes}, using the code \( \psi_{*} \) there,
and then add some initializations:
In cells of the base colony and its left neighbor  colony,
the sweep and drift fields \( \cSweep \) and \( \cDrift \) are set 
to \( \Last(1)-1 \),  \( 1 \), and \( \Last(1) \),  \( 1 \) respectively.
In the right neighbor colony, these values are \( \Last(-1) \) and \( -1 \) respectively.
In all other cells, these values are empty.
The \( \cAddr \) fields of each colony are filled properly:
the \( \cAddr \) of the \( j \) cell of a colony
is \( j \bmod \B^{*} \). \Pnote{Picture?}

The value \( \Noise^{(k+1)} \) is obtained by a residue operation
just as in Definition~\ref{def:sparsity} of sparsity.
It remains to define \( \eta^{*}=\eta^{(k+1)} \) when \( \eta=\eta^{k} \).
Parts of the history that are locally super-annotated will be called clean.
In the clean part,
if no colony has its starting point at \( x \) at time \( t \), set \( \eta^{*}(x,t)=\Vacant \).
Otherwise \( \eta^{*}(x,t) \) will be decoded from
the \( \cInfo \) track of this colony, at the beginning of its work period 
containing time \( t \).
More precisely:

\begin{definition}[Scale-up]\label{def:scaleup}
Let \( (\eta,\Noise) \) be a history of \( M \), where \( \Noise=\Noise^{(k)} \)
as in Definition~\ref{def:sparsity}.
We define \( (\eta^{*},\Noise^{*})=\Phi^{*}(\eta,\Noise) \) as follows.
Let \( \Noise^{*}=\Noise^{(k+1)} \).
Consider position \( x \), and let \( I=\lint{x-2.5\Q\B}{x+2.5\Q\B} \),
\( J=\rint{t-\Tus}{t} \).
If the history \( (\eta,\Noise) \) cannot be super-annotated on \( I\times J \)
then \( \eta^{*}(x,t)=\Bad^{*} \); assume now that it is, and let
\( \chi(\cdot,u) \) be some super healthy configuration satisfying \( \eta \) over 
\( I \) at time \( u \).
If in \( I \) there is not a full colony of \( \chi \) starting on left of \( x \) and also a full colony 
ending on the right of \( x \) then \( \eta^{*}(x,t)=\Bad^{*} \). 
Else if \( x \) is not the start of a colony then let \( \eta^{*}(x,t)=\Vacant \); assume now that it is.
Then let \( t'\in J \) be the starting time in \( \chi \) of the work period of \( C \)
containing \( t \), and let \( \eta^{*}(x,t) \) be the value decoded from \( \eta(C,t') \).
In more detail, as said at the end of Section~\ref{sec:coding}, we apply the decoding
\( \psi^{*} \) to the interior of the colony it to obtain \( \eta(x,t) \).
\end{definition}

Note that this definition has the property~\eqref{i:config.sharp-ends} of cleanness required
in Definition~\ref{def:config}.

\section{Isolated bursts}\label{sec:1-level-noise}

Here, we will prove that the healing procedure indeed deals with isolated bursts.
Our goal is to show that the healing procedure provides relief, as required in an
admissible annotated trajectory.

Bursts can create dirt.
For its elimination we will rely on the Dwell Cleaning, Spill Bound and the Attack Cleaning
properties of a trajectory, see Definition~\ref{def:traj}.

Let us first see how the head can escape dirt.

\begin{lemma}[Local escape]
Let \( G \) be an interval of size \( n\B \) where \( n<\Z \).
Then in the absence of noise, the head will either escape \( G \) within time \( O(n\Tu) \),
or at some point during this time, it will be inside a clean hole, as per Definition~\ref{def:clean-hole}.
\end{lemma}
\begin{proof}
  Let \( c = (\CAtt+2\CSpill) \), as used in the Dwell Cleaning property of Definition~\ref{def:traj}.
Let us cover \( G \) by consecutive intervals of size \( c\B \) called \df{blocks}, let \( m \) be the
number of these blocks.
Assume that the head does not escape \( G \) within time \( m\cdot\CDwell\Tu \).
Then there is a block \( K \) in which it spends cumulative time \( \CDwell\Tu \),
and the Dwell Cleaning property of trajectories implies that at some point during this time, it
will be inside a clean hole in \( K \).
\end{proof}
 
In a clean configuration, whenever healing started with an alarm, the procedure
will be brought to its conclusion as long as no new burst occurred.
Now, however, the trajectory properties do not allow any conclusion about the
state whenever the head emerges from possible dirt.
This complicates the reasoning, and may require several restarts of the healing procedure.
By design, the healing procedure can change the \( \cCore \) track only in one cell,
even if the head emerged from dirt, with wrong information.
The following lemma limits even this kind of damage:
it says that the head with the wrong information may increase some existing island, but will
not create any new one.

\begin{lemma}
In the absence of noise, no new island will arise.
\end{lemma}
\begin{proof}
The islands are defined only by the \( \cCore \) track.
In normal mode, this track changes only at the front.
If this is not the real front, then we are already in or next to an island.

The healing procedure's change of \( \cCore \) is part of a stitching operation.
Looking at the different cases of the proof of Lemma~\ref{lem:stitching}, we see that 
inside a healthy area, healing can only change the \( \cCore \) track in two ways.

The first way is case~\ref{step:stitching.mergeable}, when the \( \cSweep \) values were changed
in a segment not belonging to the interior.
Such an operation can be applied to any healthy configuration without affecting health.

The second case is when the front is moved left or right.
This does not affect health either.
\end{proof}

The following lemma is central to the analysis of the behavior of the machine under the condition that 
bursts are isolated.

\begin{lemma}[Healing]\label{lem:healing}
In the absence of noise in \( M^{*} \), the history can be super-annotated.
Also, the decoded history \( (\eta^{*},\Noise^{*}) \) satisfies the Transition Function property 
of trajectories (Definition~\ref{def:traj}).
\end{lemma}
\begin{Proof}
The proof of super-annotation is by induction on time, extending the super-annotation into the future.
The extension is straightforward as long as no distress event is encountered.
The Transition Function property is observed, as no obstacle arises to the simulation.
Stains do not cause problems: the computation stage of the simulation cycle eliminates them
using the error-correcting code.

Consider now the occurrence of a distress event.
This can be due to either a burst or the encounter with an island.
In the latter case, before the relief a burst can still hit.
In the analysis below, then we will just cut our losses and restart, knowing that 
burst cannot hit again before relief.

At the time of the distress event, let us draw an interval \( I \) of size \( \Q\B \) centered 
around the head.
The head will not leave it before relief, so we will consider the changes of 
the configuration inside it.
Let \( S_{1},\dots,S_{m} \) be the list of substantial segments of \( I \), 
and \( A_{1},\dots,A_{m-1} \) the ambiguous segments between them.
Note that \( m=O(1) \).
Let \( R \) be the set of those \( i \) for which \( S_{i}\cup A_{i}\cup S_{i+1} \) is clean.
For \( i\in R \) let \( n_{i} \) be the number of 
stitching steps by the algorithm of Lemma~\ref{lem:stitching} needed to stitch them 
together.
Let \( N=\sum_{i\in R} n_{_{i}} \).
Whenever the head enters dirt, we will mark the edge where it entered as \df{attacked}.

We introduce a few variables for the proof.
\begin{itemize}
\item\( D= \) be the total size of unclean intervals, divided by \( \B \).
\item\( E= \) the number of un-attacked boundaries where the head can exit without entering.
  This number never increases.
\item \( F= \) the number of un-attacked boundaries where the head cannot exit without entering.
\item \( P= \) be the number of steps that the front moves forward (including the ones after possibly
changing the meaning of forward at a regular turn).
\end{itemize}

The following kinds of event may occur:
\begin{enumerate}[label=\upshape{(h\arabic*)}, ref=h\arabic*]

\item\label{i:heal.N-decr} With a stitching done, \( N \) decreases by 1 while possibly 
moving the front backward.

\item\label{i:heal.enter-carpet} The head enters dirt, decreasing \( F \).

\item\label{i:heal.leave-carpet.E}
The head leaves dirt on a non-attacked edge, decreasing \( E \).

\item\label{i:heal.leave-carpet.F}
The head leaves dirt on an attacked edge, decreases the dirt by at least \( \B \) and
increases \( F \) by 1.

\item\label{i:heal.hole}
The head creates a clean hole, as per Definition~\ref{def:clean-hole}.
This may increase \( E \) by 2, but also decreases the dirt by \( \Omega(\B) \).

\item\label{i:heal.clean}
A set \( A_{i} \) becomes clean: then \( i \) gets added to \( R \); 
this can happen only \( m \) times.

\item\label{i:heal.N-incr.healing}
\( N \) increases in the healing mode by \( 1 \).
This can only happen after the head entered the clean area in healing mode (with the wrong information).
At the exit, either \( E \) or \( D \) had to decrease.

\item\label{i:heal.approach-front} 
The head approaches the front: if it does not reach there then it gets closer by  
 \( \Omega(\beta)\B \).

\item\label{i:heal.front-move} The front moves forward in normal mode
(possibly after making a regular turn, and thus changing the forward direction).
This may also increase \( N \) by 1 (and is the only way to do so), 
say by decreasing a segment \( S_{i} \).
But it is followed by zigging.
The zigging may hit dirt, leading to case~\eqref{i:heal.enter-carpet},
or find something wrong---and trigger new healing, which leads to some of the other events.
Otherwise the island around the front will be deleted, and the head becomes distress-free.
\end{enumerate}

The above possibilities suggest a potential function
\begin{align*}
   N + c_{D}D + c_{E}E + c_{F}F + c_{m}m - c_{P}P
 \end{align*}
where \( c_{D},\dots \) are appropriate positive constants.
Let us look at what each possibility does to the potential.

Cases~(\ref{i:heal.N-decr}-\ref{i:heal.leave-carpet.E}) decrease the potential if \( c_{P}<1 \).
Cases~(\ref{i:heal.leave-carpet.F}-\ref{i:heal.hole}) decrease the potential if \( c_{D}>c_{F} \).
There are only \( O(1) \) cases of type~(\ref{i:heal.clean}), increasing \( N \) by a total of \( O(\beta) \).

In case~(\ref{i:heal.N-incr.healing}), if \( c_{D} \) and \( c_{E} \) are large enough,
the increase in \( N \) can be charged to a decrease in \( D \) or \( E \).

Case~(\ref{i:heal.approach-front}) will not change the potential but there are at most \( O(1) \)
consecutive such cases.
 
Consider case~(\ref{i:heal.front-move}).
If the zigging hits new dirt and \( c_{F}>1 \) then the potential decreases.
If it triggers new healing then this will lead to one of the other cases,
compensating for the increase of \( N \) if the constants (other than \( c_{P} \)) are large.

These considerations show that relief indeed follows distress in time \( O(\beta^{2}\Tu) \).
At that point a normal-mode step moving the front has been made, followed by zigging.
Therefore the island causing the distress must have been eliminated, allowing the 
simulation to continue.
The stain caused by the island remains, but as discussed after Definition~\ref{def:super-admissible},
the simulation guarantees that the
size and number of stains remains bounded as required by that definition.
The simulation also continues to deliver the Transition Function property of trajectories.
\end{Proof}

\section{Cleaning}\label{sec:cleaning}

\subsection{Escape}\label{sec:escape}

Let us prepare the proof of the scale-up of the trajectory properties.

The following lemma is similar to Lemma~\ref{lem:combined-heals}, but deals with longer intervals.

\begin{lemma}\label{lem:rebuild-pass}
Let \( I=\lint{a}{b} \) be a clean interval of size \( > 13 \Q\B \).
If the head passes it without a burst then a subinterval of size \( |I|-10\Q\B \)
becomes clean for \( M^{*} \).
\end{lemma}
\begin{proof}
Assume, without loss of generality, that the head passes \( I \) from left to right.
In the absence of noise, and inside a clean area,
the healing procedure can only fail if it starts a rebuilding process.
And a rebuilding process never fails.

If the head passes \( I \) in normal mode, then
the simulation makes \( I \) clean for \( M^{*} \).
It may encounter a colony that is only healthy, not super healthy;
however, then the simulation will turn it into a healthy one.
If healing is invoked but no rebuilding then we can follow the proof of Lemma~\ref{lem:combined-heals}:
the result of each successful healing is consistent with the the continuation of a simulation.

If a rebuilding is invoked whose whole range falls within \( I \) then it succeeds.
Then simulation continues, with possible healings thrown in, until new rebulding starts.
Lemma~\ref{lem:rebuild-health} shows that rebuilding will only extend the super-healthy area.
\end{proof}

The following lemma shows that a clean interval cannot hold the head too long.

\begin{lemma}[Clean escape]\label{lem:clean-escape}
Suppose that \( \G \) is a clean interval of size \( n\B \).
Then within \( t=O(n+\beta |n-2\Z|^{+}) \) steps, in the absence of noise, one of the following
cases happens to the head:
\begin{Alphenum}
  \item\label{i:clean-escape.leave}  it leaves \( G \);
  \item\label{i:clean-escape.rebuild} it is in a complete clean rebuilding area (continuing the work of rebuilding);
  \item\label{i:clean-escape.healthy} it is in 
the interior of a colony, which is at least \( 0.5\Q\B \) inside a healthy area. 
  \item\label{i:clean-escape.aging} it is in the healthy interior of 
a---possibly partial---colony (which might need cells outside \( G \) to complete it),
whose sweep number increases by \( \Omega(t/\beta\Q) \).
\end{Alphenum}
\end{lemma}
When the head leaves \( \G \) we will say that it \df{escapes}.
The bound \( O(n+|n-2/Z|^{+}) \) is \( O(n) \) if \( n<2\Z \), but is \( O(\beta n) \) otherwise.
Case~\eqref{i:clean-escape.aging} encompasses case~\eqref{i:clean-escape.healthy}.
The latter is included just for convenient later reference.
\begin{Proof}
\begin{step+}{step:clean-escape.rebuild}
Suppose that at some time while the head is inside \( \G \), the rebuild procedure is started,
from a base of rebuild-marked cells large enough not to result in alarm (renewed call for healing)
after the first zigging.
Then case~\eqref{i:clean-escape.leave} or~\eqref{i:clean-escape.rebuild} 
happens within \( O(\beta n) \) steps.
\end{step+}
\begin{pproof}
The rebuilding procedure is outlined in Section~\ref{sec:rebuilding}.
From a sufficiently large base, it 
marks a whole rebuilding area (if it stays inside \( G \)), resulting
in alternative~\eqref{i:clean-escape.rebuild}.
This happens in \( O(\beta n) \) steps, with zigs of size \( O(\beta) \).
Of course, if \( n<Z \) then the head escapes in \( n \) steps.
\end{pproof} % step:clean-escape.rebuild

\begin{step+}{step:clean-escape.normal}
Assume that at the beginning, the mode is normal.
Then within \( O(\beta n) \) steps either a healing call happens, or we have 
case~\eqref{i:clean-escape.leave}, \eqref{i:clean-escape.healthy} or~\eqref{i:clean-escape.aging}.
\end{step+}
\begin{pproof}
The head begins or continues a sweeping motion that, in case it does not escape
and sees no incoordination to call for healing,
passes (with zigging) over the healthy interior of a colony, possibly several times.
This leads to case~\eqref{i:clean-escape.aging}.
\end{pproof} % step:clean-escape.normal

\begin{step+}{step:clean-escape.alarm}
Suppose that at some time \( t \), at position \( x \),
while the head is inside \( \G \), healing will be called.
Then one of the cases of the lemma occur in \( O(\beta n) \) steps.
\end{step+}
\begin{pproof}
If healing fails then it creates a base for rebuilding and calls the rebuilding
procedure, leading to the case of part~\ref{step:clean-escape.rebuild} above.
\emph{In the discussion below, we will not mention again the possibility that the head
leaves \( G \) or that a healing ends unsuccessfully: the result in these cases is considered
known.}

If healing is repeated, every invocation of it will bring some progress, described in the proof of 
Lemma~\ref{lem:healing}.
Eventually, it will succeed, and the front will continue performing the regular sweeping motion of 
the simulation.
New inconsistencies may start new healing; if this succeeds then after it only the same old simulation
can continue.
(Lemmas~\ref{lem:health-extension} and~\ref{lem:combined-heals} imply that 
the overlapping successful healing areas can be combined.)
This leads to case~\eqref{i:clean-escape.aging}.
\end{pproof} % step:clean-escape.alarm

\begin{step+}{step:clean-escape.finish}
Let us complete the proof.
\end{step+}
\begin{prooof}
Consider all the possibilities for the state at the beginning.
We will again leave it unmentioned that the head can always escape (case~\eqref{i:clean-escape.leave}).
The case when we start from normal mode is discussed in part~\ref{step:clean-escape.normal}.
If the head is in healing mode then within one call of the healing procedure, we end up in
one of the other cases.
Suppose now that the head is in rebuilding mode.
If alarm is not called within \( O(\beta n) \) steps then the head either builds sweeps
a whole rebuilding area, taking us to case~\eqref{i:clean-escape.rebuild},
or starts or continues erasing the rebuild marks, ending eventually in either
alarm or normal mode.
\end{prooof} % step:clean-escape.finish
\end{Proof}

We will apply Lemma~\ref{lem:clean-escape} together with
the basic trajectory properties in Definition~\ref{def:traj} (in particular 
the definition of the constant \( \CAtt  \) there).

\begin{lemma}[Escape]\label{lem:escape}
Suppose the head is in some interval \( \G \) in some configuration, at some time.
In the absence of noise,
then within time \( O(\beta n^{2}\Tu) \), one of the cases~\eqref{i:clean-escape.leave}-\eqref{i:clean-escape.healthy} 
of Lemma~\ref{lem:clean-escape} occurs.
  \end{lemma}
In difference to Lemma~\ref{lem:clean-escape}, we do not assume 
that the interval \( \G \) is clean.
On the other hand, note that the upper bound increased from \( O(\beta n\Tu) \) to \( O(\beta n^{2}\Tu) \).
The escape can indeed take longer in our proof.
Imagine that \( n/2 \) cells in the middle of \( G \) are clean representing the middle part of a colony, 
and the rest is not.
The head may perform regular simulation over these \( n/2 \) cells.
As it leaves the clean area on either side, it returns almost 
immediately, in a state allowing it to sweep again.
The only effect is an increase of the clean area by \( \B \), so it may take \( \Omega(n) \) sweeps to
escape.
\begin{Proof}
Lemma~\ref{lem:clean-escape} asserts an upper bound \( O(\beta n) \) on the 
number of steps needed to either reach cases~\eqref{i:clean-escape.leave}-\eqref{i:clean-escape.healthy} or
increase the sweep number in case~\eqref{i:clean-escape.leave} of that lemma, in a clean 
interval intersecting \( G \).
Let \( \CCleanEsc \) be a constant such that this is \( \le \CCleanEsc\beta n \) steps.
Recall clean holes of Definition~\ref{def:clean-hole}, which we will just call \df{holes} here.
We will show that while the head does not escape \( G \), progress will be made every \( O(n) \) steps:
holes keep arising and growing in \( G \).
The head may spend considerable time inside a hole, but we will limit this time by
showing that during it also a certain kind of progress will be made.

% \begin{step+}{step:escape.rebuild}
% Assume that the head is inside a clean hole, in rebuilding mode, over an iterval marked for rebuilding,
% having the size necessary for a rebuilding process.
% Then within \( O(\Q\beta) \) steps either the head leaves the hole or case~\eqref{i:clean-escape.rebuild} of
% Lemma~\ref{lem:clean-escape} occurs.
% \end{step+}
% \begin{pproof}
% This follows directly from the definition of the rebuild procedure.
% \end{pproof} % step:escape.rebuild

\begin{step+}{step:escape.attacked}
Assume that a right attacking event of the Attack Cleaning property
of a trajectory takes place at point \( x \) (thus the interval
\( \lint{x-\CAtt\B}{x+\B} \) is clean).
After this event, until the head returns to the left of \( x-\CAtt \B \),
we will call the point \( x-\CSpill \B \) an \df{attacked boundary}.
If a hole does not contain the head
then its edge closer to the head is an attacked boundary (except the first time the head passed there).
By the Spill property
before the return of the head the right edge of the clean interval may shrink to
\( x-\CSpill \B \), but this is temporary: after the return, it extends to at least \( x+2\B \).
\end{step+}

\begin{step+}{step:escape.alternatives}
Let \( \f=\max(\CCleanEsc\beta, \CDwell) \).
During the time interval considered, in any time subinterval \( I \) of size \( \f n\Tu \),
one of the following kinds of progress will be made:
\begin{enumerate}[label=\upshape{(p\arabic*)}, ref=p\arabic*]
\item\label{i:clean-escape} One of the cases ~(\ref{i:clean-escape.leave}-\ref{i:clean-escape.healthy}) 
of Lemma~\ref{lem:clean-escape} occurs.
\item\label{i:enter-hole} The head enters a clean hole (possibly resulting in the merge of two clean holes).
\item\label{i:leave-hole} The head leaves a clean hole.
\item\label{i:new-hole} A new clean hole appears.
\item\label{i:aging} The sweep number 
increases in some clean area intersecting \( G \), as 
in case~\eqref{i:clean-escape.aging} of Lemma~\ref{lem:clean-escape}.
\end{enumerate}
\end{step+}
\begin{pproof}
Suppose that the head is in a hole of size \( k\B \): let us apply 
Lemma~\ref{lem:clean-escape}.
Within time \( \CCleanEsc k\beta\Tu \) either case~\eqref{i:clean-escape.leave} of that lemma occurs,
leading to our case~\eqref{i:leave-hole},
or cases~(\ref{i:clean-escape.rebuild},\ref{i:clean-escape.healthy}) in which case we are done, or
case~\eqref{i:clean-escape.aging}, which is our case~\eqref{i:aging}.

Assume that \( \G \) is partitioned into \df{blocks}: subintervals of size \( 1.5\CAtt\B \).
Suppose that the head is in an interval \( L \) between holes, which it does not leave for 
the time interval \( I \) considered.
Let \( L \) intersect with \( k \) blocks.
The head spends on average a cumulative time \( \ge\f n\Tu/k\ge\f\Tu \) over these blocks, so
on one of them it spends at least this much time.
By the trajectory property called Dwell Cleaning, if \( \f\ge\CDwell \) then at some
time during \( I \), the head will find itself in a clean hole: in other words,
either case~\eqref{i:enter-hole} or case~\eqref{i:new-hole} occurs.
\end{pproof} % step:escape.alternatives

\begin{step+}{step:escape.aging}
The total number of events of type~\eqref{i:aging} is \( O(n) \).
\end{step+}
\begin{pproof}
Each such event is associated with some colony \( C \) contained in some hole \( H \).
We claim that each colony \( C \) is involved in only \( O(\Q) \) of these events,
and the interiors of all these colonies are disjoint, therefore their number is \( O(n/\Q) \).
This limits the number of these events to \( O(n) \).

Without loss of generality, assume that \( C \) is on the right edge of \( H \).
The total number of these events  is \( O(\Q) \) (even if during different intervals
after the head leaving and entering \( H \)).
Indeed, in \( O(\Q) \) sweeps, a whole colony simulation cycle is completed.
At its end, it is followed by transfer, either to the right or to the left.
If it is to the right then there is no returning of the head without
moving the edge by at least \( \Q\B \), and thus paying for all the \( O(\Q) \) sweeps.
Transfering left can occur at most twice, because of the feathering property of the simulated 
machine.

The interiors of two such colonies \( C_{1},C_{2} \) are necessarily disjoint.
Indeed, suppose that an event of type~\eqref{i:aging} occurred first in colony \( C_{1} \).
If \( C_{2} \) intersects \( C_{1} \) (without being equal to it) then a 
total restructuring of \( C_{1} \) must have happened,
but this must have involved the rebuilding procedure, moving many times over an interval much wider than 
\( C_{1}\cup C_{2} \), widening the hole---therefore \( C_{2} \) could not appear on the edge of a hole.
\end{pproof} % step:escape.aging

\begin{step+}{step:escape.finish}
Let us finish the proof.
\end{step+}
\begin{prooof}
Each clean hole can be entered at most once on a non-attacked boundary.
When it is entered on an attacked boundary, the hole increases at least by \( \B \).
It follows that except for the steps when the hole is entered the first time, 
or when a hole is created that intersects the boundary of \( \G \), the possibilities
(\ref{i:leave-hole}-\ref{i:new-hole}) increase the clean part of \( \G \) by \( \B \).
So there are only \( O(n) \) events of type (\ref{i:enter-hole}-\ref{i:new-hole}).
By part~\ref{step:escape.aging} there are also at most \( O(n) \) events of type~\eqref{i:aging}.
\end{prooof} % step:finish
\end{Proof}

\subsection{Attack cleaning}

Eventually, we will prove the Attack Cleaning property of trajectories (Definition~\ref{def:traj}) for the 
decoded history \( (\eta^{*},\Noise^{*}) \).
However, first we prove a weaker version, requiring the space-time rectangle of interest to be free
not only of \( \Noise^{*} \) but also of \( \Noise \)---that is burst-free.

\begin{lemma}\label{lem:clean-attack}
Consider a trajectory \( \eta \) in a noise-free space-time rectangle.
Here, the decoded history \( \eta^{*} \) satisfies the Attack Cleaning property.
\end{lemma}
\begin{proof}
The property says the following for the present case.
For current cell \( x \), suppose that the interval
\( \lint{x-\CAtt\Q\B}{x+\Q\B} \) is clean for \( M^{*} \).
Suppose further that the transition function, applied to \( \eta^{*}(x,t) \), directs the head right.
Then by the time the head comes back to \( x-\CAtt\Q\B \), the right end of the interval clean in \( M^{*} \)
containing \( x \) advances to the right by at least \( \Q\B \).

The computation phase of the simulation on the colony of \( x \) is completed without the disturbing effect
of noise: even the zigging does not go beyond the boundary.
Then the transfer phase begins which enters the unclean area to the right of \( x+\Q\B \).

We argue that there are only two ways for the head to get back to \( x-\CAtt\Q\B \).
\begin{enumerate}[label=(a\arabic*),ref=a\arabic*]
\item\label{i: clean-attack.normal} The transfer into a new colony with starting point
\( y\ge x+\Q\B \) succeeds despite the uncleanness, and the clean interval extends over it, before
the head moves left to \( x-\CAtt\Q\B \) in the course of the regular simulation.
Some inconsistencies may be discovered along the way, but they are corrected by healing.

\item\label{i: clean-attack.rebuild} The inconsistencies encountered along the way trigger some
rebuilding processes.
Eventually, a complete, clean rebuilding area is created, the rebuilding succeeds, leaving a clean
colony also to the right of \( x+\Q\B \).
\end{enumerate}

The Spill Bound property guarantees that the area to the left of \( z=x+(\Q-\CSpill)\B \) remains clean,
therefore the only way for the head to move left of \( z \) is by the rules.
Suppose that rebuilding is not initiated (and maintained): then moving left can only happen by the normal
course of simulation: the transfer stage of the simulation must be carried out, and this requires 
at least as many attacks to the right as the number of sweeps in the transfer stage.
Every attack (followed by return) extends the clean interval further, until the whole target colony becomes clean,
and the transfer completed.
This is the case~\eqref{i: clean-attack.normal}.

Recall the definition of the rebuilding procedure in Section~\ref{sec:rebuilding}:
the rebuilding area extends \( 2.5\Q \) cells to the left and right from its initiating cell.
This may become as large as \( 5\Q\B \) to the left and right.
If rebuilding is initiated, its starting position
is necessarily to the right of \( x+(\Q-2)\B \): since inconsistency with
the colony of \( x \) would be discovered already in the last cell of the colony of \( x \)..
It then may extend to the left to at most \( x+\Q\B-5\Q\B=x-4\Q\B \) (this is overcounting,
since the cells of the colony of \( x \) are all adjacent).
Its many sweeps will result in attacks that clean an area to the right of the starting point.
The procedure may be restarted several times, but those restartings will also be initiated
to the right of \( x+(\Q-2)\B \).
Therefore the rebuilding area does not advance beyond \( x-4\Q\B \):
if the head moves to the left of this, then the rebuilding must have succeeded.
The rebuilding also must find or create a colony manifestly to the right of the restarting site: this will be
to the right of \( x+\Q\B \), moving this way the boundary of the area clean in \( M^{*} \) 
by at least \( \Q\B \).
\end{proof}

Note that in the process described in the above proof,
it is possible that the rebuilding finds a competing colony \( C \)
starting at some \( y \in x+\Q\B + \lint{-\beta\B}{0}\) which
slightly (by the size of an island) overlaps from the right with the colony of \( x \).
The rebuilding may decide to keep \( C \) and to overwrite the rest of the colony of \( x \) as a bridge.
This does not affect the result.

\subsection{Extended cleaning}\label{sec:extended-cleaning}

Let us draw some consequences of the Pass Cleaning property of trajectories (Definition~\ref{def:traj}).
We will extend this property to longer intervals, allowing also  some bursts.
For the following lemma, let us use the following notation for convenience:
\begin{align*}
   \beta'=\beta+2\CSpill.
 \end{align*}
Also, for any interval \( I=\lint{a}{b} \) let \( I'=\lint{a+\CSpill\B}{b-\CSpill\B} \).

\begin{lemma}\label{lem:dirty-passes}
Assume that the head passes \( n> 2\pi \) times over and interval \( I \) of size \( |I|\ge 2\pi B \) 
in a burst-free way during some time interval \( J \).
There could be some other times in \( J \) when the head is subject to a burst inside \( I \).
Assume that the total number of these bursts is \( < n/4\beta' \).
(We don't count the times when the head enters and leaves \( I \) without
passing over and without any burst.)
Then there is some time during \( J \) when \( I' \) becomes clean.
\end{lemma}
\begin{proof}
We introduce a \df{virtual} time counting.
We only count the times when the head passes \( I \).
Let this virtual time interval be denoted by \( K \): we can assume it starts at 0.
Let \( K_{1}=\rint{0}{\pi} \) and \( K_{2}=\rint{\pi}{n} \), so \( K=K_{1}\cup K_{2} \).

For each burst (of size \( \le\beta\B \)), let us extend it left and right by \( \CSpill B \) 
to a size \( \le  \beta' B \).
Let \( D_{1} \) be the union of all those intervals coming from bursts occurring before virtual time \( \pi \).
By the end of time interval \( K_{1} \) the set \( I\setminus D_{1} \) 
becomes clean, due to the Pass Cleaning property.
In what follows we are looking at how \( D_{1} \) shrinks during \( K_{2} \)---while some new bursts may
delay the shrinking.

Consider intervals \( \lint{a}{b}, U \) such that \( \lint{a}{b}\subset U' \),
and \( U\setminus \lint{a}{b} \) is clean at virtual time \( u>1 \).
If no burst occurs at the virtual times \( u,u+1 \) (that is during two burst-free passes) then 
the Attack Cleaning property implies that 
by the virtual time \( u+2 \), already the interval \( U'\setminus \lint{a+\B}{v-\B} \) will also be clean.
So let us mount a triangle \( T\subset I\times K \) over \( \lint{a}{b} \)
which at time \( u+2j \) covers the interval
 \begin{align*}
 \lint{a+j\B}{v-j\B},
 \end{align*}
and thus its tip is at virtual time \( v=u+(b-a)/\B \).
We will call \( v-u \) the \df{height} of \( T \), denoted by \( |T| \).
If no burst occurs until virtual time \( v \), then \( U' \) becomes clean
by virtual time \( v \).
In the special case when \( \lint{a}{b} \) reaches, say, to the right end of the interval \( I \),
we create a triangle (with the same slopes) 
twice as large, whose tip is at the right end of \( I \) as well.

\begin{sloppypar}
Let us mount a triangle of the above type over each interval of the set \( D_{1} \)
at virtual time \( \pi \), creating a set \( \cT_{0} \) of disjoint triangles.
While no burst occurs, the Attack Cleaning property confines dirt to these triangles.
On the other hand, every burst may create a dirt interval 
\( \lint{x}{x+\beta\B} \), at some virtual time \( u \).
Let us mount a triangle then over \( \lint{x-\CSpill B}{x+(\beta+\CSpill)\B}\times\{u\} \),
and add all these triangles to the set \( \cT_{0} \) to get a set of triangles \( \cT \).
These are not necessarily disjoint anymore.
  \end{sloppypar}

If two virtual triangles \( T_{1},T_{2} \) intersect,
and \( T \) is the smallest virtual triangle containing both,
then it is easy to see that \( |T|\le|T_{1}|+|T_{2}| \).
Denote \( T=T_{1}+T_{2} \).

Let us now perform the following operation that will create a disjoint set
of triangles.
We start with \( \cT \) and if we find two intersecting triangles 
in it, then we replace them with their sum.
Repeat until the remaining set \( \cT' \) consists of disjoint triangles.
By the above remark \( \sum_{T\in \cT}|T| = \sum_{T\in \cT'}|T| \).
By the Attack Cleaning property, the complement of these triangles
is clean.
Let us ignore the triangles on the boundary for a moment.
The sum of the heights of the triangles is at most \( \beta' \) times the number of bursts,
and thus at most \( n/4 \).
Taking the boundary triangles into account can increase this by at most a factor of 2, to \( n/2 \).
Therefore in the interval \( K \) of length \( n-\pi>n/2 \) there will be virtual
times not intersecting with any element of \( \cT' \).
At the corresponding real times, the interval \( I \) is clean.
\end{proof}


\begin{lemma}\label{lem:burst-density}
Consider an interval \( K \) of size \( 3 \Q\B \) and a time interval \( J \) in which
no burst of \( M^{*} \) occurs.
If at least \( 4\beta'\pi \) bursts occur in \( K \) during \( J \) then at some time in \( J \)
the interval \( K \) becomes clean in \( M^{*} \).
\end{lemma}
\begin{proof} 
For \( d=8\beta' \), both positive and negative \( i \), and \( j=0,\dots,d-1 \), let
\begin{align*}
   K_{ij}=x+3\Q\B\lint{d i+j}{d i+j+1},\ 
K_{i}=\bigcup_{j=0}^{d-1}K_{ij}=x+3\Q\B\lint{d i}{d(i+1)},
 \end{align*}
so \( K=K_{00} \).
Consider the sweeps corresponding to the \( 4\beta'\pi \) bursts in \( K \).
If a sweep does not exit \( K \) on either side then Dwell Cleaning and Attack Cleaning
of \( M^{*} \) becomes applicable.
Assume this does not happen; then
either half of these sweeps exits \( K \) on the left, or half of them exits it on the right.
Without loss of generality assume that they pass on the right.
Then they will have to pass the whole interval \( K_{0} \) (otherwise again Dwell Cleaning
and Attack Cleaning of \( M^{*} \)
applies), and thus each interval \( K_{0,j} \) gets at least this many passes.
So, the intervals \( K_{0,j} \), \( j>0 \) gets \( 4\beta'\pi/2 \) burst-free passes (since the 
bursts occurred in \( K_{00} \)).
Also, none of the \( K_{0j} \) becomes clean for \( M^{*} \) during the first half of these passes,
since then the Attack Cleaning property would clean \( K_{00} \) as well during the remaining
passes.

Now, for \( i=1,2,\dots \) we will show that there is 
an \( i'  \) with \( |i'|\le i \) such that each \( K_{i'j} \) gets at least 
\begin{align*}
  n_{i}= 4\beta'\times 2^{i-1}\pi
\end{align*}
 burst-free overpasses  during \( J \), 
and \( K_{i'j} \) does not become clean during the first half of these.
This clearly must break down somewhere, leading to a contradiction.

We have just proved the case \( i=0 \).
Suppose that the statement was proved up to some \( i \), we will prove it for \( i+1 \).
In order for the interval \( K_{i'j} \) not to become clean for \( M^{*} \), 
by Lemmas~\ref{lem:dirty-passes}, \ref{lem:rebuild-pass}
the total number of bursts happening in it must be at least \( n_{i}/4\beta' = 2^{i-1}\pi \).
This is true of all \( j \), so the total number of bursts in \( K_{i'} \)
is at least \( d 2^{i-1}\pi \).
If a sweep does not exit \( K_{i'} \) on either side then Dwell Cleaning becomes applicable,
so either half of these sweeps exits \( K_{i'} \) on the left, or half of them exits it on the right.
If they pass on the right then let \( (i+1)'=i'+1 \), otherwise \( (i+1)'=i'-1 \).
Then they will have to pass the whole interval \( K_{(i+1)'} \) (again, otherwise Dwell Cleaning
applies), and thus each interval \( K_{(i+1)',j} \) gets at least \( d 2^{i-2}\pi \) passes.
Now the inequality~\eqref{eq:cns.traj} implies \( d > 8\beta'=2\cdot 4\beta' \), finishing the proof.
\end{proof}

\section{Further analyses}

\subsection{Trouble with the current plan}

The current plan seems not to be working. 
This plan required the trajectory properties
Transition Function, Attack Cleaning, Spill Bound, Dwell Cleaning, Pass Cleaning.
But there is a question about the conditions of these properties, for example
of Pass Cleaning.
If we require it only during a noise-free time interval, then Lemmas~\ref{lem:dirty-passes} 
and~\ref{lem:burst-density} are not applicable.
They may become applicable if we require it in a time interval in which noise is sparse.
But then it is not clear how to scale it up.
These same two lemmas are needed to scale it up, but it seems difficult to prove their
noisy version.

\subsection{Cleaning in a small number of passes}

What is the real number of passes needed to clean an interval?

\begin{example}
  One pass is not sufficient.
Consider the following local configuration on an interval \( I \).
It is covered with colonies (of level 1, representing cells of level 2).
The interval is broken up into subintervals \( I_{1},I_{2},\dots \), each consisting of \( \gamma \) colonies.
(Recall that a sparse set of bursts allows a burst in every \( \gamma \) colonies.)
These colonies \( C_{k,1},\dots,C_{k,\gamma} \) represent cells \( x_{k,1},\dots, x_{k,\gamma} \).
The leftmost cell \( x_{k,1} \) in each interval \( I_{k} \) is 
\( \Z \) steps behind the front which is on its right, also in \( I_{k} \).
Colony \( C_{k,1} \) 
pretends that the head is observing \( x_{k,1} \), and is in the last stage of moving right.
So once the head is in \( C_{k,1} \), it does not make any move back, and
the simulation of a head moving (in a zigging way) from \( x_{k,1} \) to \( x_{k,\gamma} \) proceeds.
Once the head reaches the right end of \( C_{k,\gamma} \), a burst moves it
into \( C_{k+1,1} \).

This way, the head passes \( I \) while leaving the organization at level 2.
\end{example}

The example suggests that maybe two passes are sufficient: on the way back from the right end, the
example configuration will trigger rebuilding at every level.



\section{After a large burst}

Our goal is to show that the simulation \( M\to M^{*} \)
defined in Section~\ref{sec:sim-codes} is indeed a simulation.
Section~\ref{sec:1-level-noise} shows this as long as the head operates in an
area that is clean for the simulated machine\( M^{*} \) (can be super-annotated), 
and has no noise for machine \( M^{*} \) (that is its bursts on the level of machine \( M \)
are isolated).
In other words, essentially the Transition Function property of Definition~\ref{def:traj} of
trajectories for the simulated machine \( M^{*} \) has been taken care of.

The new element is the possibility of large areas that cannot be super-annotated: they
may not even be clean, even on the level of machine \( M \).
The Spill Bound, Attack Cleaning, Dwell Cleaning and Pass Cleaning properties still must be proven.

\subsection{Spill bound}

One of the most complex analyses of this work is the proof that 
the simulated machine \( M^{*} \) also obeys the Spill Bound.
Let us outline the problem.

We are looking at the boundary \( z \) of a large area that is clean for the
machine \( M^{*} \): without loss of generality suppose that this is the right boundary.
We will be looking at it in a space-time rectangle \( \lint{a}{b}\times\lint{u}{v} \)
that is noise-free in \( M^{*} \).
The interesting case has \( a<z<b \) with \( z-a,b-z=O(\Q\B) \).
The assumption allows occasional bursts of noise of the trajectory \( \eta \) of \( M \),
but no two of these bursts must occur in a space-time rectangle of size comparable to
the size of a colony work period of \( M \).
The Spill Bound for trajectory \( \eta \) keeps the dirt of \( \eta \) within
\( O(\B) \) on the left of \( z \) while \( \eta \) is noise-free.
If we could assume \( \eta \) noise-free then the heal/rebuild procedures would also keep
the dirt of \( \eta^{*} \) from spilling over by more than \( O(\B^{*})=O(\Q\B) \).
However, nothing is assumed about the length of the time interval \( \lint{u}{v} \).
If the head would spend all the time within \( O(\Q\B) \) of \( z \) then due to 
the Dwell Cleaning property of trajectories,  the area would be cleaned out, again
preventing spilling.
But the head can slide out far to the right of \( b \) fast, since
the dirty area on the right of \( z \) is arbitrarily large.
Then it can come back much later to the left of \( z \), and by
a burst (allowed since much time has passed)
can deposit an island of dirt there.
Repeating this process would produce unlimited spillover, not only 
of the dirt of \( \eta^{*} \) but even that of \( \eta \).

This is where the Pass Cleaning property helps.
If the above process is repeated \( \pi \) times, the \( \pi \) passes
would clean out an interval on the
right of \( z \) whose size is of the order of \( \Q\B \),
while depositing only \( \pi \) islands of dirt to the left of \( z \).
Our construction will have \( \pi\ll \Q \): more precisely, in our hierarchy
of generalized Turing machines we will have \( \pi_{k}=k \), \( \Q_{k}=k^{2} \). %?
And \( \ll \Q \) bursts will still be handled by healing/rebuilding in \( \eta \).

Of course this sketch is very crude, but it should help motivate the reasoning that
follows.



\bibliographystyle{plain}
\bibliography{reli,publ}

\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
