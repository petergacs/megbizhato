\documentclass[11pt]{memoir}

\pagestyle{plain}
%\pagestyle{simple}
\setlrmarginsandblock{1in}{*}{*}
\checkandfixthelayout

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{3}
\counterwithout{section}{chapter}
\counterwithin{equation}{section}
%\numberwithin{equation}{section} % in amsmath
%\counterwithout{figure}{chapter}
\counterwithin{figure}{section}

\makeatletter
% To correct a memoir bug:
\renewcommand{\@memmain@floats}{%
  \counterwithin{figure}{section}
  \counterwithin{table}{section}}
\makeatother

\firmlists

% If you do not want the bibliography on a separate page:
\renewcommand{\bibsection}{% 
\section*{\bibname} 
\prebibhook}

\usepackage[backref,hyperindex,colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage[numbered]{bookmark} % Allows to place a bookmark, see the title. Shows section numbers.
% \usepackage[all]{hypcap} % After hyperref, to anchor floats correctly.
% \usepackage{float}
 % After hyperref:
\usepackage[algo2e,algosection,tworuled,noend,noline]{algorithm2e}
\usepackage[charter]{gacs}
%\usepackage[noBBpl]{mathpazo}
%\usepackage{amssymb}
%\usepackage[charter]{gacs}
%\usepackage[libertine]{gacs}
\usepackage{gacs-algo} % After hyperref.
% After gacs.sty

%\usepackage[pagecolor={LightCyan1}]{pagecolor}
%\usepackage[pagecolor={DarkSeaGreen1}]{pagecolor}
%\usepackage[pagecolor={Honeydew1}]{pagecolor}
%\usepackage[pagecolor={Azure1}]{pagecolor}
%\usepackage[pagecolor={Cornsilk1}]{pagecolor}
%\usepackage[pagecolor={Ivory1}]{pagecolor}

\hyphenation{com-plex-ity des-tin-at-ion co-lon-ies}

% \newcommand{\shownotes}{1}
% \ifnum\shownotes=1
% \newcommand{\authnote}[3]
% {\text{{ \textcolor{#3}{\( \langle\hspace{-0.2em}\langle \)\textsf{\footnotesize #1: #2}\( \rangle\hspace{-0.2em}\rangle \)}}}}
% \else
% \newcommand{\authnote}[2]{}
% \fi
% \newcommand{\Pnote}[1]{{\authnote{P}{#1}{cyan}}}
% \newcommand{\Inote}[1]{{\authnote{I}{#1}{blue}}}

\theoremstyle{definition} % not italicized
\newtheorem{Premark}{\color{cyan}Peter remark}
\newenvironment{premark}{\begin{Premark}\color{cyan}}{\varqed\end{Premark}}

% \newtheorem{Iremark}{\color{blue}Ilir remark}[Premark]
% \newenvironment{iremark}{\begin{Iremark}\color{blue}}{\varqed\end{Iremark}}

% \renewcommand{\Pnote}[1]{\begin{premark}#1\end{premark}}
% \renewcommand{\Inote}[1]{\begin{iremark}#1\end{iremark}}

\renewcommand{\le}{\leq}
\renewcommand{\ge}{\geq}

\renewcommand{\vek}[1]{\mathbf{#1}}
\newcommand{\fld}[1]{\ensuremath{\textit{#1\/}}}
\newcommand{\rul}[1]{\ensuremath{\texttt{#1}}}

% Using def for the possibility of switching between LaTeX and XeTeX:
\def\B{B}  
\def\U{U}

\newcommand{\va}{\vek{a}} % current cell-pair
\newcommand{\blank}{\text{\textvisiblespace}}
\newcommand{\Configs}{\mathrm{Configs}}
\renewcommand{\d}{d}
\newcommand{\E}{E} % size of healing interval
% Feathering:

\makeatletter
\if@charter\renewcommand{\f}{f}\else\newcommand{\f}{f}\fi
\makeatother
\newcommand{\F}{F}
\newcommand{\g}{g}
\def\G{G} % TM to be simulated (why a def?)
\newcommand{\h}{h} % head position
% Current pair
\newcommand{\hc}{\hat h}
\newcommand{\vhc}{\vek{\hat h}}
\newcommand{\Int}{\mathrm{Int}} % interior of intervals
\newcommand{\Noise}{\mathit{Noise}}
\newcommand{\passno}{\pi}
\newcommand{\pos}{\mathrm{pos}}
\newcommand{\curcell}{\textrm{cur-cell}}
\newcommand{\Q}{Q} % Colony size
\newcommand{\R}{R} % healing interval
\newcommand{\s}{s} % used as burst number upper bound
% switching time upper bound:
\newcommand{\Tu}{T} 
\newcommand{\Tus}{T^{*}}
\newcommand{\Z}{Z} % Zigging length
\newcommand{\z}{z} % center of healing or rebuilding

\newcommand{\tape}{\mathrm{tape}}
\newcommand{\Interpr}{\mathrm{Interpr}} % interprets the set of rules
\newcommand{\Decode}{\mathrm{Decode}}
\newcommand{\Encode}{\mathrm{Encode}}
\newcommand{\Un}{\mathrm{Univ}}
\newcommand{\increment}[1]{#1\mathord{+}\mathord{+}}
\newcommand{\decrement}[1]{#1\mathord{-}\mathord{-}}

% Indexes of program names
\newcommand{\decode}{\mathrm{decode}}
\newcommand{\encode}{\mathrm{encode}}

\newcommand{\dir}{\mathrm{dir}} % sweep direction
\newcommand{\Histories}{\mathrm{Histories}}
\newcommand{\Trajectories}{\mathrm{Trajectories}}

% Certain milestone sweep numbers.
\newcommand{\TransferSw}{\mathrm{TransferSw}}
\newcommand{\Last}{\mathrm{Last}}

\newcommand{\PadLen}{\mathit{PadLen}} % width of margin used for feathering
%\newcommand{\Interpr}{\mathit{Interpr}} % interprets the set of rules

% Left and right newly created colonies C_{\Left}, etc.
\newcommand{\Left}{\text{left}}
\newcommand{\Right}{\text{right}}

\renewcommand{\r}{\vek{r}} % radius. What was it? 
\newcommand{\x}{\vek{x}} % center of a segment 
\newcommand{\y}{\vek{y}} % center of a segment 

% Fields
\newcommand{\Addr}{\fld{Addr}}
\newcommand{\BridgeDir}{\fld{BridgeDir}}
\newcommand{\BridgeKind}{\fld{BridgeKind}}
\newcommand{\Core}{\fld{Core}}
\newcommand{\Drift}{\fld{Drift}}
\newcommand{\Doomed}{\fld{Doomed}}
\newcommand{\Heal}{\fld{Heal}} % collects fields used in healing mode
\newcommand{\Hold}{\fld{Hold}}
\newcommand{\Index}{\fld{Index}}
\newcommand{\Info}{\fld{Info}}
\newcommand{\Kind}{\fld{Kind}}
\newcommand{\Mode}{\fld{Mode}}
\newcommand{\Output}{\fld{Output}}
\newcommand{\Rebuild}{\fld{Rebuild}} % collects fields used in rebuilding mode
\newcommand{\Rider}{\fld{Rider}} % to execute the actual simulation.
\newcommand{\Sweep}{\fld{Sweep}} % the number of sweep during simulation
\newcommand{\Turned}{\fld{Turned}} % to control feathering
\newcommand{\Work}{\fld{Work}} % track for all kinds of work
\newcommand{\FCount}{\fld{FCount}} % to keep track of the feathering count 
\newcommand{\ZigAddr}{\fld{ZigAddr}}

% modes
\newcommand{\Normal}{\mathrm{Normal}}
\newcommand{\Healing}{\mathrm{Healing}}
\newcommand{\Rebuilding}{\mathrm{Rebuilding}}

\newcommand{\Bad}{\mathrm{Bad}}
\newcommand{\Vacant}{\mathrm{Vac}}

% kinds
\newcommand{\Stem}{\mathrm{Stem}}
\newcommand{\Booting}{\mathrm{Booting}}
\newcommand{\Bridge}{\mathrm{Bridge}}
\newcommand{\Inner}{\mathrm{Inner}}
\newcommand{\Member}{\mathrm{Member}}
\newcommand{\Outer}{\mathrm{Outer}}

% Rules
\newcommand{\rHeal}{\rul{Heal}}
\newcommand{\Compute}{\rul{Compute}}
\newcommand{\Transfer}{\rul{Transfer}}
\newcommand{\WriteProgramBit}{\rul{WriteProgramBit}}

% Constants
\newcommand{\cns}[1]{c_{\textrm{\upshape #1}}}
\newcommand{\CEsc}{\cns{esc}}
\newcommand{\CMarg}{\cns{marg}}
\newcommand{\CPass}{\cns{pass}}
\newcommand{\CRedund}{\cns{redund}}
\newcommand{\CSim}{\cns{sim}}
\newcommand{\CSpill}{\cns{spill}}


\begin{document}

\title{A reliable Turing machine}
% Why do I need this?  Some people get the title bookmarked even without this.
\bookmark[page=1,level=0]{A reliable Turing machine}

\author{Ilir \c{C}apuni 
\\ University of Montenegro
\\ ilir@bu.edu
\and
Peter G\'acs
\\ Boston University
\\ gacs@bu.edu
}
\maketitle

\begin{abstract}
  We consider computations of a Turing machine subjected to noise.
  In every step, the action (the new state and the new content of the observed
  cell, the direction of the head movement) can differ from that prescribed by
  the transition function with a small probability (independently of previous
  such events).  We construct a universal 1-tape Turing machine that for a low enough
  (constant) noise probability performs arbitrarily large computations.  For
  this unavoidably, the input needs to be encoded---by a simple code depending
  on its size.  The work uses a technique familiar from reliable cellular
  automata, complemented by some new ideas.
\end{abstract}

\newpage

% \tableofcontents*

\section{Introduction}

This work addresses a question from the area of ``reliable computation with unreliable components''.
A certain class of machines is chosen, (like a Boolean circuit, cellular automaton, Turing machine).
It is specified what kind of faults (local in space and time)
are allowed, and a machine of the given
kind is built that---paying some price in performance---carries out essentially the same
task as any given machine of the same kind without any faults would.

We confine attention to \emph{transient}, \emph{probabilistic} faults:
the fault occurs at a given time but no component is damaged permanently,
and faults occur independently of each other, with a bound on their probability.
This in contrast to bounding the \emph{number} of faults, allowing them to be set by an adversary.

Historically the first result of this kind is~\cite{VonNeum56}, which for each Boolean
circuit \( C \) of size \( n \) constructs a new circuit \( C' \)
of size \( O(n\log n) \) that performs the same task as \( C \) with a (constant)
high probability, even though each gate of
\( C' \) is allowed to fail with some (constant) small probability.

Cellular automata as a model have several theoretical advantages over Boolean circuits, and results concerning
reliable computation with them have interest also from a purely mathematical or 
physical point of view (non-ergodicity).
The simple construction in~\cite{GacsReif3dim88} gives, for any 1-dimensional
cellular automaton \( A \) a 3-dimensional cellular automaton \( A' \) 
that performs the same task as \( A \) with high probability, even though each cell of
\( A' \) is allowed to fail in each time step with some constant small probability.
(A drawback of this construction is the requirement of synchronization: all cells of \( A' \) must
update simultaneously.)
Reliable cellular automata in less than 3 dimensions can also be constructed (even
without the synchrony requirement), but so far only at a
steep increase in complexity (both of the construction and the proof).
The first such result was~\cite{Gacs1dim86}, relying on some ideas proposed in~\cite{Kurd78}.

Here, the reliability question will be considered for a \emph{serial}
computation model---Turing machine---as opposed to parallel ones like
Boolean circuits or cellular automata.
There is a single elementary processing unit (the \df{active} unit)
interacting with a memory of unlimited size.
The error model needs to be relaxed.
Allowing each memory component to fail in each time step with constant probability
makes, in the absence of parallelism, reliable computation seems impossible.
Indeed, while in every step some constant fraction of the memory gets corrupted,
the active unit can only correct a constant \emph{number} of them per step.

In the relaxed model considered here, faults can affect only the operation of the active unit.
More precisely at any given time the allowed operations of the machine are the usual ones:
changing its state, writing to the observed tape cell, moving the head by a step left or right (or not at all).
The transition table of the machine prescribes which action to take.
So our fault model is the following.

\begin{definition}
  Let \( \Noise \) be a random subset of some set \( U \).
  We will say that the distribution of \( \Noise \) is \( \eps \)-\df{bounded} if for every finite subset \( A \)
  we have
  \begin{align*}
   \Pbof{A\subseteq\Noise} \le \eps^{|A|}.
  \end{align*}
\end{definition}
See~\cite{Toom80} for an earlier use of this kind of restriction.

\begin{definition}\label{def:faults-eps-bounded}
  Let \( \cC = (C_{1},C_{2}\dots) \) be  a random sequence of configurations of a Turing machine \( T \)
  with a given fixed
  transition table, with the property that for each time \( t \), the \( C_{t+1} \) is obtained from \( C_{t} \) by
  one of the allowed operations.
  We say that a \df{fault} occurred at time \( t \) if the operation giving \( C_{t+1} \) from \( C_{t} \) is
  not obtained by the transition function.
  Let \( \Noise\subseteq\bbZ_{+} \) be the (random) set of faults in the sequence.
  We say that faults of the sequence \( \cC \) are \( \eps \)-\df{bounded}, if the set \( \Noise \) is.  
\end{definition}

The challenge for Turing machines
is still significant, since even if only with small probability, occasionally a group
of faults can put the head into the middle of a large segment of the tape rewritten
in an arbitrarily ``malicious'' way.
A method must be found to recover from all these situations.

Here we define a Turing machine that is reliable---under this fault model--- in
the same sense as the other models above.
The construction and proof are similar in complexity to the ones for 1-di\-men\-sion\-al cellular automata;
however, we did not find a reduction to these earlier results.
A natural idea is to let the Turing machine simulate a 1-dimensional cellular automaton, by
having the head make large sweeps, and updating the tape as the simulated cellular automaton would.
But apart from the issue of excessive delay, we did not find any simple
way to guarantee the large sweeps in the presence of faults (where ``simple'' means not building some new
hierarchy), even for some price paid in efficiency.
So here we proceed ``from scratch''.

Many ideas used here are taken from~\cite{Gacs1dim86} and~\cite{GacsSorg01},
but hopefully in a somewhat simpler and more intuitive conceptual framework.
Like~\cite{Gacs1dim86} it confines all probability reasoning to a single lemma,
deals on each level only with a numerical restriction on faults (of that level).
On the other hand, like~\cite{GacsSorg01} it defines a series of generalized objects
(generalized Turing machines here rather than generalized cellular automata),
each one simulating the next in the series.
In~\cite{GacsSorg01} a ``trajectory'' (central for defining the notion of simulation) was
a random history whose distribution satisfies certain constraints (most of which are combinatorial).
Here it is a single history satisfying only some combinatorial constraints.

The work~\cite{AsarinCollins2005} seems related by its title 
but is actually on another topic.
The work~\cite{DurandRomashShenTiling12} applies the self-simulation and
hierarchical robustness technique developed for cellular automata in an interesting, but
simpler setting.
Several attempts at the chemical or
biological implementation of universal computation have to deal with
the issue of error-correction right away.
In these cases generally the first issue is the faults occurring at the active site (the head).
See~\cite{BennettThermodynComp1982,QianSoloveichikWinfree2011}.

Our result can use any standard definition of 1-tape Turing machines whose tape alphabet
contains some fixed ``input-output alphabet'' \( \Sigma \); we will
introduce one formally in Section~\ref{sec:TM}.
We will generally view a tape symbol as a tuple consisting of several \df{fields}.
The notation
\begin{align*}
   a.\Output, a.\Info
 \end{align*}
 shows \( \Output \) and \( \Info \) as fields of tape cell state \( a \).
Combining the same field of all tape cells, we can talk about a \df{track}
(say the \( \Output \) track and \( \Info \) track).
For ease of spelling out a result, we consider only computations whose outcome
is a single symbol, written into the \( \Output \) field of tape position 1.
It normally holds a special value---say \( * \) ---meaning \df{undefined}.

 Block codes (as defined in Section~\ref{sec:codes} below) are specified by a pair \( \pair{\psi_{*}}{\psi^{*}} \)
of encoding and decoding functions.
In the theorem below, the input of the computation, of some length \( n \), is broken up into blocks
that are encoded by a block code that depends in some simple way on \( n \).
Its redundancy depends on the size of the input as a log power.
The main result in the theorem below shows a Turing machine simulating a fault-free
Turing machine computation in a fault-tolerant way.
It is best to think of the simulated machine \( G \) as some universal Turing machine.

\begin{theorem}\label{thm:main}
  For any Turing machine \( \G \) there are constants \( \alpha_{1},\alpha_{2}>0 \),
for each \( n \) a block code \( (\varphi_{*}, \varphi^{*}) \) of block size \( O((\log n)^{\alpha_{1}}) \),
a fault bound  \( 0\le\eps <1 \) and a Turing machine \( M_{1} \) with a 
function \( a\mapsto a.\Output \) defined on its alphabet,
such that the following holds.

Let \( M_{1} \) start its work from the initial tape configuration \( \varphi_{*}(x) \) with the head
in position 0,
running through a random sequence of configurations whose faults are \( \eps \)-bounded in the sense
of Definition~\ref{def:faults-eps-bounded}.
Suppose that at time \( t \) the machine \( \G \) writes a value \( y\ne * \) 
into the \( \Output \) field of the cell at position 0.
Then at any time greater than \(    t(\log t)^{\alpha_{2}} \),
 the tape symbol \( a \) of machine \( M_{1} \) at position 0
 will have \( a.\Output= y \) with probability at least \( 1 - O(\eps) \).
\end{theorem}


\section{Overview}

The overview, but even the main text, is not separated completely
into two parts: the definition of the Turing machine, followed by the proof of its reliability.
The definition of the machine is certainly translatable (with a lot of tedious work) into just a Turing
machine transition table (or \df{program}), but its complexity requires first to develop a \df{conceptual
apparatus} behind it which is also used in the proof of reliability.
We will try to indicate below at the beginning of each
section whether it is devoted more to the program or more to the conceptual apparatus.

\subsection{Isolated bursts of faults}\label{sec:bursts}

(Introducing some basic elements of the \emph{program}.)
In~\cite{burstyTuring13} we defined a Turing machine \( M_{1} \) that simulates ``reliably'' any other
Turing machine even when it is subjected to isolated \df{bursts} of faults (that is a group
of faults occurring in consecutive time steps) of constant size.
We will use some of the ideas of~\cite{burstyTuring13}, without relying directly
on any of its details, and will add several new ideas.
Let us give a brief overview of this machine \( M_{1} \).
% We break up the task of error correction into several 
% problems to be solved.
% The solution of one problem gives rise to another one, but the process converges.
% \begin{description}
% \item[Redundant information] The tape information of \( G \) is put into
% an error-correcting block code.
% \item[Redundant processing] The block code will be decoded, the retrieved information 
% will be processed, and the result encoded.
% The major processing steps will be 
% carried out on a working track three times within one work period,
% recording the result onto separate tracks.
% The information track is changed only in a final majority vote.
% \item[Local repairability] In order to resist a local burst of faults.
% the process is organized into a rigid, locally checkable structure
% with the help of local addresses, and some other tools like sweeps and 
% short switchbacks (zigzags).
% \item[Disturbed local repair] A conservatively organized healing procedure
% restores consistency in a way that cannot be abused by new bursts.
% \end{description}

% Here is some more detail.
Each tape cell of the simulated machine \( M_{2} \) will be represented by a block of
some size \( \Q \) called a \df{colony}, of the simulating machine \( M_{1} \).
Each step of \( M_{2} \) will be simulated by a computation of \( M_{1} \) called
a \df{work period}.
During this time, the head of \( M_{1} \) makes a number of sweeps over the
current colony-pair, decodes the represented cell symbols,
then computes and encodes the new symbols, and finally moves the head 
to the new position of the head of \( M_{2} \).
The major processing steps will be 
carried out on a working track three times within one work period,
recording the result onto separate tracks.
The information track is changed only in a final majority vote.

The organization is controlled by a few key fields, for example a field
called \( \Addr \) showing the position of each cell in the colony, and a field
\( \Sweep \), the number of the last sweep of the computation (along with its direction)
that has been performed already.
The most technical part is to protect this control information from faults.
For example, a burst of faults of constant size can reverse the head in the middle of a sweep.
To discover such structural disruptions locally before
the head would go far in the wrong direction, it will make frequent short zigzags.
A premature turn-back will be detected this way, triggering the healing procedure.

% This description uses in an informal way some words that
% will get precise definition later.
% For example, the word \df{colony} will have at least two formal definitions.
% From the point of view of the program (transition function), it is just a 
% sequence of addresses from \( 0 \) to \( \Q-1 \).
% From the point of view of the analysis, it is a sequence of
% actual adjacent tape cells with the address field having theses values.

\subsection{Hierarchy}\label{sec:hier}

(Starting to develop the \emph{conceptual apparatus}.)
In order to build a machine resisting faults 
occurring independently in each step with some small probability,
we take the approach used for one-dimensional cellular automata.
We aim at building a \df{hierarchy of simulations}:
machine \( M_{1} \) simulates machine \( M_{2} \) which simulates machine \( M_{3} \), and so on.
Machine \( M_{k} \) has alphabet
\begin{align}\label{eq:Sigma_k}
   \Sigma_{k}=\{0,1\}^{s_{k}},
\end{align}
that is its tape cells have ``capacity'' \( s_{k} \).
All these machines should be implementable on a universal Turing machine with
the same program (with an extra input, the number \( k \) denoting the level).
For ease of analysis, we introduce the notion of \df{cell size}:
level \( k \) has its own cell size \( \B_{k} \) and block (colony) size \( \Q_{k} \)
with \( B_{1}=1 \), \( \B_{k+1}=\B_{k}\Q_{k} \).
This allows
locating a tape cell of \( M_{k} \) on the same interval where the cells of \( M_{1} \) simulate it.
One cell of machine \( M_{k+1} \) is simulated by a colony of machine \( M_{k} \);
so one cell of \( M_{3} \) is simulated by \( \Q_{1}\Q_{2} \) cells of \( M_{1} \).
Further, one step of, say, machine \( M_{3} \) is simulated by one
work period of \( M_{2} \) of, say, \( O(\Q_{2}^{2}) \) steps.

Per construction, machine \( M_{1} \) can withstand
bursts of faults with size  \( \le \beta \) for some constant parameter \( \beta \),
separated by at least some constant \( \gamma \) work periods.
It would be natural now to expect that machine
\( M_{1} \) can withstand also some \emph{additional}, larger bursts
of size \( \le \beta \Q_{1} \) if those are separated
by at least \( \gamma \) work periods of \( M_{2} \).
However, a \emph{new obstacle} arises.
Damage caused by a big burst of faults spans several colonies.
The repair mechanism of machine \( M_{1} \) outlined in Section~\ref{sec:bursts} 
is too local to recover from such extensive damage, leaving
the whole hierarchy endangered.
So we add a new mechanism to \( M_{1} \) that
will just try to restore the colony structure of a large enough portion of the
tape (of the extent of several colonies).
The task of restoring the original information is left to higher levels (whose simulation
now can continue).

All machines above \( M_{1} \) in the hierarchy live only in simulation: the hardware is \( M_{1} \).
Moreover, the \( M_{k} \) with \( k>1 \)
will not be ordinary Turing machines, but \df{generalized} ones,
with some new features seeming necessary in a simulated Turing machine:
allowing for some ``disordered'' areas of the tape not obeying the transition function,
and occasionally positive distance between neighboring tape cells.

A tricky issue is ``forced self-simulation''.
Each machine \( M_{k} \) can be implemented on a universal machine using as inputs
the pair \( (p,k) \) where \( p \) is the common program and \( k \) is the level.
Eventually, \( p \) will just be hard-wired into the definition of \( M_{1} \),
and therefore faults cannot corrupt it.
While creating \( p \) for machine \( M_{1} \),
we want to make it simulate a machine \( M_{2} \) that has the same program \( p \).
The method to achieve this is not really different from the one
applied already in some of the cellular automata and tiling papers cited, 
and is related to the proof of Kleene's fixed-point theorem (also called the recursion theorem).

Forced self-simulation can give rise to an infinite sequence of simulations, achieving
the needed robustness.
But the simulation of \( M_{k+1} \) by \( M_{k} \) uses only certain tracks of the tape.
Another track  (which we will call \( \Rider \)) will be set aside
for simulating \( \G \).
If the simulation of \( \G \) on the \( \Rider \) track of a colony-pair
does not finish in a certain number of steps,
a built-in mechanism will \df{lift} its tape content to the \( \Rider \)
field of the simulated cell-pair, allowing it to be continued in a colony-pair of the next
level (with the corresponding higher reliability).

\subsection{Structuring the noise}\label{sec:sparsity-informal}

(Some definitions for analyzing the noise.)
The set of faults in the noise model of the theorem is a set of points in time.
It turns out more convenient to use an equivalent model:
an \( \eps \)-bounded \emph{space-time} set of points.
Let us make this statement more formal.

\begin{lemma}
  Let \( \cC=(C_{1},C_{2},\dots) \) be the random sequence of configurations of a Turing machine
  with an \( \eps \)-bounded set of faults \( \Noise_{1}\subseteq\bbZ_{+} \),
  as in Definition~\ref{def:faults-eps-bounded}.
  Let \( h(t) \) be the (random) position of the head at time \( t \).
  Then the random set \( \Noise_{2}=\setOf{(h(t),t)}{t\in\Noise_{1}} \).
is an \( \eps \)-bounded subset of \( \bbZ\times\bbZ_{+} \).
\end{lemma}
\begin{proof}
  Let \( A \) be a finite subset of  \( \bbZ\times\bbZ_{+} \), and
  \( A' = \setOf{t}{(p,t)\in A} \).
  If \( A\subseteq\Noise_{2} \) then \( A'\subseteq\Noise_{1} \) and \( |A'|=|A| \).
  Hence 
\begin{align*}
 \Pbof{A\subseteq\Noise_{2}}\le\Pbof{A'\subseteq\Noise_{1}}\le \eps^{|A'|}=\eps^{|A|} .
\end{align*}
\end{proof}

The construction outlined above counts with \emph{bursts} (rectangles of space-time
containing  \( \Noise \)) increasing in size and decreasing
in frequency---which is a combinatorial set of constraints.
To derive such constraints from the above probabilistic model
the  we stratify \( \Noise \) as follows.
We will have two series of parameters:  \( \B_{1}<\B_{2}<\dotsm \) and
\( \Tu_{1}<\Tu_{2}<\dotsm \).
Here \( \B_{k} \) is the size of cells of \( M_{k} \) as represented on the tape of \( M_{1} \),
and \( \Tu_{k} \) is a bound on the time needed to simulate one step of  \( M_{k} \).

Here are some informal definitions (precise ones are given in 
Section~\ref{sec:sparsity}).
For some constants \( \beta,\gamma>1 \),
a \df{burst} of noise of type \( \pair{a}{b} \)
is a space-time set that is essentially coverable by a  \( a\times b \).
For an integer \( k>0 \) it is of \df{level} \( k \) when it is of type \( \beta(\B_{k}\times\Tu_{k}) \).
It is \df{isolated} if it is 
essentially alone in a rectangle of type \( \gamma(\B_{k+1}\times\Tu_{k+1}) \) 
First we remove such isolated bursts of level 1, then of level 2
from the remaining set, and so on.
It will be shown that with not too fast increasing sequences \( \B_{k},\Tu_{k} \), with probability 1,
this sequence of operations eventually completely erases \( \Noise \): thus each fault belongs to
a burst of ``level'' \( k \) for some \( k \).

Machine \( M_{k} \) will concentrate only on correcting isolated bursts of level \( k \) and on restoring
the framework allowing \( M_{k+1} \) to do its job.
It can ignore the lower-level bursts and will need to work correctly
only in the absence of higher-level bursts.


\subsection{Difficulties}\label{sec:novelties}

(Referring both to the program and to the concepts behind it.)
We list here some of the main problems that the paper deals with, 
and some general ways in which they will be solved or avoided.
Some more specific problems will be pointed out later, along with their solution.

\begin{description}

\item[Non-aligned colonies] A large burst of faults in \( M_{1} \) can modify the order of
entire colonies or create new ones with gaps between them.
To deal with this problem, machines \( M_{k} \) for \( k>1 \)
will be \df{generalized Turing machines}, allowing for non-adjacent cells.

\item[Clean areas]
  The tape of a generalized Turing machine will be divided, based on its content, into
  some areas called \df{clean}, the rest \df{disordered}.
  Clean areas will be essentially where the analysis
  can count on an existing underlying simulation,
  and where therefore the transition function is applicable.
  Noise can disorder the areas where it occurs.

\item[Extending cleanness]
  The predictability of the machine is decreased when the head enters into disorder.
  But the model still provides some ``magical'' properties
  helping to restore cleanness (in the absence of new noise):
  \begin{Alphenum}
  \item escaping from any area in a bounded amount of time;
 \item extension of clean intervals as the head passes in and out of them;
 \item\label{i:many-slides}
   the cleaning of an interval when passed over a certain number of times.
 \end{Alphenum}
While an area is cleaned, it will also be re-populated with cells.
Their content is not important, what matters is the restoration of predictability.

\item[Rebuilding]
The need to reproduce the cleaning properties in simulation is the
main burden of the construction.
The part of the program devoted to this is the rebuilding procedure,
invoked when local repair fails.
  It reorganizes a part of the tape having the size of a few colonies.
\end{description}

% \section{Notation}\label{sec:notation}

% Most notational conventions given here are common; some other ones will
% also be useful.

% \begin{description}

% \item [Natural numbers and integers] 
% By \( \bbZ \) we denote the set of integers.
% \begin{align*}
%    \bbZ_{>0}&=\setOf{x}{x\in \bbZ,\;  x>0}, \\
%    \bbZ_{\ge 0}&=\bbN=\setOf{x}{x\in \bbZ,\;  x\ge 0}.
% \end{align*}

% \item [Intervals]
% We use the standard notation for intervals:
% \begin{align*}
%    \clint{a}{b}&=\setOf{x}{a\le x \le b},\quad \lint{a}{b}=\setOf{x}{a\le x < b}, \\
%    \rint{a}{b}&=\setOf{x}{a< x \le b}, \quad  \opint{a}{b}=\setOf{x}{a< x < b}.
% \end{align*}
% We will also write \( \lint{a}{b} \) in place of \( \lint{a}{b}\cap \bbZ \), 
% whenever this leads to no confusion.
% Instead of \( \lint{x+a}{x+b} \), sometimes we will write 
% \begin{align*}x + \lint{a}{b}.\end{align*}

% \item [Ordered pairs]
% Ordered pairs are also denoted by \( \pair{a}{b} \),
% but it will be clear from the context whether we are
% referring to an ordered pair or open interval.

% \item [Comparing the order of a number and an interval]
% For a given number \( x \) and interval \( I \), we
% write
% \begin{align*} x \ge I \end{align*}
% if for every \( y\in I \),  \( x \ge y \).

% \item [Distance]
% The distance between two real numbers \( x \) and \( y \) is defined
% in a usual way:
% \begin{align*}
%     d(x,y)= \abs{x-y}.
% \end{align*}

% The \df{distance of a point \( x \) from interval \( I \)}  is
% \begin{align*}
%     d(x,I)= \min_{y\in I}d(x,y).
% \end{align*}

% \item [Ball, neighborhood, ring, stripe]
% A \df{ball of radius \( r>0 \), centered at point \( x \)} is
% \begin{align*}
%     B(x,r)= \setOf{y}{d(x,y)\le r}.
% \end{align*}
% An \df{\( r \)-neighborhood of interval } \( I \) is
% \begin{align*}
%     \setOf{x}{d(x,I)\le r}.
% \end{align*}
% An \df{\( r \)-ring} around interval \( I \) is
% \begin{align*}
%     \setOf{x}{d(x,I)\le r \txt{ and } x \notin I}.
% \end{align*}
% An \df{\( r \)-stripe to the right of interval \( I \)} is
% \begin{align*}
%     \setOf{x}{d(x,I)\le r \txt{ and } x \notin I \txt{ and } x>I}.
% \end{align*}

% \item[Logarithms] Unless specified differently,
% the base of logarithms throughout this work is 2.

% \end{description}

\subsection{Structuring the noise formally}\label{sec:sparsity}

(This purely mathematical section derives the combinatorial noise
constraints from the probabilistic one.)
If we modeled noise as a set of time points then a burst of faults would be a time
interval of size \( \beta\Tu_{k} \) and might affect a space interval as large as \( \beta\Tu_{k} \),
covering many times more simulated cells of level \( k \).
Therefore we model noise as a set of space-time points; this does not change the independence
assumption of the main theorem.

\begin{definition}\label{def:isolation}
Let \( \r=\pair{r_{1}}{r_{2}} \), \( r_{1}, r_{2}> 0 \)
be a two-dimensional nonnegative vector.
A \df{rectangle of radius} \( \r  \) \df{centered} at point  \( \x \) is
\begin{align}\label{eq:ball1}
  B(\x,\r) = \setOf{\y}{\abs{y_{i} - x_{i}} < r_{i}, i=1,2}.
\end{align}  
Let \( E\subseteq \bbZ\times\bbZ_{\ge 0} \) be a space-time set (to be considered our noise set).
A point \( \x \) of \( E \) is \df{\( \pair{\r}{\r^{*}} \)-isolated} if
\(  E \cap B(\x,\r^{*})\subseteq B(\x, \r)  \),
that is all points of \( E \) that are \( \r^{*} \)-close to \( x \) are also \( \r \)-close.
A set \( E \) is called \( \pair{\r}{\r^{*}} \)-\df{sparse} if each of its points is \( \pair{\r}{\r^{*}} \)-isolated.
\end{definition}

The following lemma will justify talking about bursts of faults.

\begin{lemma}\label{lem:bursts}
  Suppose that the set \( E \) is \( \pair{\r}{\r^{*}} \)-sparse.
  Then in every rectangle \( U \) of size \( r^{*}_{1}\times r^{*}_{2} \), 
\( U\cap E \) is covered by some rectangle of size \( r_{1}\times r_{2} \).
\end{lemma}
\begin{proof}
  Let \( U \) be a rectangle of size \( r^{*}_{1}\times r^{*}_{2} \), and suppose that \( U\cap E \) is not
  covered by a rectangle of size \( r_{1}\times r_{2} \).
  Then either the horizontal projection of \( E\cap U \) is larger than \( r_{1} \) or the vertical one
  is larger than \( r_{2} \), hence there is a pair of points \( \x,\y\in E \) not covered by a single
  rectangle of size \( r_{1}\times r_{2} \).
  But then \( U\subseteq B(\x,\r^{*}) \) and \( \y\not\in B(\x,\r) \), hence \( \x \)
  is not \( (\r,\r^{*}) \)-isolated contrary to the assumption of \( (\r,\r^{*}) \)-sparsity.
\end{proof}

\begin{definition}\label{def:sparsity}
  Let \( \gamma> 1 \), \( \beta> 4\gamma \) be parameters, and let
  \begin{align*}
  1 &=\B_{1}<\B_{2}<\dotsm,\quad
  1=\Tu_{1}<\Tu_{2}<\dotsm,
\\ &\Tu_{k+1}/\Tu_{k},\; \B_{k+1}/\B_{k}\ge 2\beta
\end{align*}
be sequences of integers to be fixed later.
For a space-time set \( E\subseteq\bbZ\times\bbZ_{\ge 0} \), let \( E^{(1)} = E \).
For \( k>1 \) let \( E^{(k+1)} \) be obtained by deleting from \( E^{(k)} \) the
\( \pair{\beta\pair{\B_{k}}{\Tu_{k}}}{\gamma\pair{\B_{k+1}}{\Tu_{k+1}}} \)-isolated points.
Set \( E \) is \( k \)-\df{sparse} if \( E^{(k+1)} \) is empty.
It is simply \df{sparse} if \( \bigcap_{k}E^{(k)}=\emptyset \).
When \( E=E^{(k)} \) and \( k \) is known
then we will denote \( E^{(k+1)} \) simply by \( E^{*} \).
\end{definition}

The following lemma connects the above defined sparsity notions to the requirement
of small fault probability.

\begin{lemma}[Sparsity]\label{lem:sparsity}
Let \( \Q_{k} = \B_{k+1}/\B_{k} \),  \( \U_{k} = \Tu_{k+1}/\Tu_{k} \), and
\begin{align}\label{eq:growth-assumption}
  \lim_{k\rightarrow\infty}\frac{\log \Q_{k}\U_{k}}{1.5^k}=0.
\end{align}
For sufficiently small \( \eps \), for every \( k\ge 1 \) the following holds.
Let \( E\subseteq \bbZ\times\bbZ_{\ge 0} \) 
be a random set that is \( \eps \)-bounded as in Definition~\ref{def:faults-eps-bounded}.
Then for each point \( \x \)  and each \( k \),
 \begin{align*}
   \Pbof{B(\x,\pair{\B_{k}}{\Tu_{k}})\cap E^{(k)}\neq\emptyset} < \eps \cdot 2^{-1.5^{k-1}}.
 \end{align*}
As a consequence, the set \( E \) is sparse with probability 1.
\end{lemma}

This lemma allows a doubly exponentially increasing sequence \( \U_{k} \), resulting
in relatively few simulation levels as a function of the computation time.


\subsection{Generalized Turing machines}\label{sec:TM}

(This section, together with Section~\ref{sec:traj},
introduces the central concept of the proof.)
Let us recall that a one-tape Turing machine is defined by a
finite set \( \Gamma \) of \df{internal states},
a finite alphabet \( \Sigma \) of \df{tape symbols}, a transition function \( \delta \),
and possibly some distinguished states and tape symbols.
At any time, the head is at some integer position \( \h \), and is observing the tape
symbol \( A(\h) \).
The meaning of \( \delta(a,q)=(a',q',d) \) is that if \( A(\h)=a \) and the state is \( q \) then
the \( A(\h) \) will be rewritten as \( a' \) and \( \h \) will change to \( \h+d \).

We will use a model that is slightly different, but has clearly the same expressing power.
(Its advantage is that it is a little more convenient to describe its simulations.)
There are no internal states, but the head observes and modifies a \emph{pair} of
neighboring tape cells at a time; in fact, we imagine it to be positioned between these
two cells called the \df{current cell-pair}.
The \df{current cell} is the left element of this pair.
Thus, a Turing machine is defined as \(    (\Sigma,\tau) \) where
the tape alphabet \( \Sigma \) contains at least the distinguished
symbols \( \blank,0,1 \) where \( \blank \) is called the \df{blank symbol}.
The \df{transition function} is
\(  \tau\colon\Sigma^{2}\to \Sigma^{2}\times\{-1,1\} \).
A \df{configuration} is a pair \( \xi = (A,\h) = (\xi.\tape,\xi.\pos) \)
where  \( \h\in\bbZ \) is the \df{current} (or \df{observed)}
head position, (between cells \( h \) and \( h+1 \)),
and \( A\in\Sigma^{\bbZ} \) is the \df{tape content}, or \df{tape configuration}:
in cell \( p \), the tape contains the symbol \( A(p) \).
Though the tape alphabet may contain
non-binary symbols, we will restrict input and output to binary.
The tape is blank at all but finitely many positions.

As the head observes the pair of tape cells
with content \( \va=(a_{0},a_{1}) \) at positions \( \h \), \( \h+1 \) denote \(  (\va',d)=\tau(\va)  \).
The transition \( \tau \) will
change tape content at positions \( \h \), \( \h+1 \) to \( a'_{0} \), \( a'_{1} \),
and move the head to tape position to \( \h+d \).
A \df{fault} occurs at time \( t \) if the output \( (\va',d) \)
of the    transition function at this time is replaced with some other value
(which then defines the next configuration).

The machines that occur in simulation will be a generalized version of the above model,
allowing non-adjacent cells and areas called ``disordered''
in which the transition function is non-applicable.
A mere convenience feature is two integer parameters:
the cell body size \( \B\ge 1 \) and and an upper bound \( \Tu\ge 1 \) on the transition time.
These allow placing all the different Turing
machines in a hierarchy of simulations onto
the same linear space and the same time line.

\begin{definition}[Generalized Turing machine]\label{def:gen-TM}
    A \df{generalized Turing machine} \( M \) is defined by a tuple
        \begin{align}\label{eq:gen-TM}
             (\Sigma, \tau, \B, \Tu, \passno),
       \end{align}
       where \( \Sigma \) is the \df{alphabet},
       \(  \tau: \Sigma^{2}\times\{0,1\}\to \Sigma^{2}\times\{-1,1\}  \)
    is the \df{transition function}.
    In  \( \tau(a,b,\alpha) \) the argument \( \alpha \) is 1 if the pair of observed cells is
    adjacent (no gap between them), and 0 otherwise.
The integer \( \passno \) will play the role of the number of passes needed to clean an area (see below).
Among the elements of the tape alphabet
we distinguish \( 0,1,\Bad,\Vacant \), where \( \Vacant \) plays the role of a blank symbol.
Symbol \( \Bad \) will mark \df{disordered} areas of the tape.
\end{definition}

A formal definition of a configuration of a generalized Turing machine is given in
Section~\ref{sec:gen-TM}, though it is essentially defined by the tape content \( A \) and the head
position.
A point \( p \) is \df{clean} if  \( A(p)\ne\Bad \).
A set of points is \df{clean} if it consists of clean points.
We say that there is a \df{cell} at a position \( p\in\bbZ \) if the interval
\( p+\lint{0}{\B} \) is clean and \( A(p)\ne \Vacant \).
In this case, we call the interval \( p+\lint{0}{\B} \) the \df{body} of this cell.
Cells must be at distance \( \ge\B \) from each other, that is their
bodies must not intersect.
If their bodies are at a distance \( <\B \) from each
other (with a clean interval containing both) then they are called \df{neighbors}.
They are called \df{adjacent} if this distance is \( 0 \).

A sequence of configurations conceivable as a computation will be called a ``history''.
For standard Turing machines, 
the histories that obey the transition function could be called ``trajectories''.
For generalized Turing machines the definition of trajectories is more complex; it
allows some limited violations of the transition function, while providing the mechanisms
for eliminating disorder.
Let \(    \Noise\subseteq \bbZ\times\bbZ_{\ge 0} \)
denote the set of space-time points at which faults occur.
Section~\ref{sec:traj} below will define a certain subset of possible histories
called \df{trajectories}.
In order to motivate their choice, we first introduce the notion of simulation.

 \subsection{Simulation}\label{sec:sim}

 (Here we introduce the notion of simulation used in the proof.
 It relies on the concept of trajectories defined in Section~\ref{sec:traj},
 but also motivates it.)
Until this moment, we used the term ``simulation'' informally, to denote
a correspondence between configurations of
two machines which remains preserved during the computation.
In the formal definition, this correspondence will essentially be a code
\( \varphi=(\varphi_{*},\varphi^{*}) \).
The \emph{decoding} part of the code is the more important.
We want to say that machine \( M_{1} \) simulates machine \( M_{2} \) via
simulation \( \varphi \) if whenever \( (\eta, \Noise) \) is a trajectory of \( M_{1} \) 
then \( (\eta^{*},\Noise^{*}) \),
defined by \( \eta^{*}(\cdot,t)=\varphi^{*}(\eta(\cdot,t)) \), is a
trajectory of \( M_{1} \).
Here, \( \Noise^{*} \) is computed by the residue operation (deleting isolated bursts)
as in Definition~\ref{def:sparsity}.
We will make, however, two refinements.
First, we require the above condition only for
those \( \eta \) for which the initial configuration
 \( \eta(\cdot,0) \) has been obtained by encoding, that is it has the form 
\( \eta(\cdot,0)=\varphi_{*}(\xi) \).
Second, to avoid the transitional ambiguities in a history,
we define the simulation decoding as a mapping \( \Phi^{*} \)
between \emph{histories}, not just configurations:
\( \Phi^{*}(\eta,\Noise)=(\eta^{*},\Noise^{*}) \).

\begin{sloppypar}
\begin{definition}[Simulation] \label{def:simulation-central}
  Let \( M_{1},M_{2} \) be two generalized Turing machines, and let
  \( 
    \varphi_{*}:\Configs_{M_{2}} \to \Configs_{M_{1}} \)
be a mapping from configurations of \( M_{2} \) to those of \( M_{1} \), such that it maps
starting configurations into starting configurations.
Let \(    \Phi^{*}:\Histories_{M_{1}} \to \Histories_{M_{2}} \) be a mapping.
The pair \( (\varphi_{*}, \Phi^{*})  \)
is called a \df{simulation} (of \(  M_{2}  \) by \(  M_{1}  \)) if for every
trajectory \(  (\eta, \Noise)  \) of \( M_{1} \) with initial
configuration \(  \eta(\cdot,0)=\varphi_{*}(\xi)  \),
the history \(  (\eta^{*},\Noise^{*})=\Phi^{*}(\eta,\Noise)  \) is
a trajectory of machine \(  M_{2}  \).
\end{definition}
  \end{sloppypar}

In the noise-free case it is easy to find examples of simulations.
However, in the cases with noise, finding any nontrivial example 
is a challenge, and depends on a careful definition of trajectories for generalized Turing machines.

\subsection{Trajectories}\label{sec:traj}

(This completes the definition of the central concept of the
proof---modulo the natural definitions spelled out in Section~\ref{sec:gen-TM}.)
The set of trajectories of a generalized Turing machine \( M \)
is defined in terms of constraints imposed on the fault-free parts of a history.
We discuss these properties first informally.

\begin{description}
\item[Transition Function] This property says (in more precise terms)
that in a clean area, the transition function is obeyed.

\item[The Spill Bound] limits the extent to which a disordered interval can spread while
the head is in it.

\item[Escape] limits the time for which the head can be trapped in a small area.

\item[Attack Cleaning] erodes disorder as the head repeatedly enters and leaves it.

\item[Pass Cleaning] cleans the interior of an
  interval if the head passes over it enough times.
  
\end{description}

\begin{remark}
The Pass Cleaning property is only used in the proof of the other trajectory properties
(and itself) for the simulated trajectory \( \eta^{*} \).
Without it, it could happen that
the head slides over a large disordered area many times (staying inside) and, occasionally,
extends it by new faults.  
\end{remark}

The definition below depends on the notions of current cell-pair, switch and dwell period given in
 in Section~\ref{sec:gen-TM}, but should be understandable as it is.

\begin{definition}
Suppose that at times \( t' \) before a switching time \( t \) but after 
any previous switch, the current cell-pair has state \( \va = (a_{0},a_{1})\),
further after the switch the same cell-pair has state \( \va' \)
(including when one of these cells dies, that is gets state \( \Vacant \)).
Let \( (\vek{b},d) =\tau(\va,\alpha) \), where \( \alpha=0 \) if the cell-pair is adjacent and 0 otherwise.
We say that the switch is \df{dictated by the transition function} if \( \vek{b}=\va' \), and
\( \sign(\hc_{0}(t')-\hc_{0}(t)) = d \).
\end{definition}

We will use the following constants:
\begin{align}\label{eq:cns.traj}
  \CSpill =2,\;
  \CMarg = 13,\;
  \CPass = 39,\;
  \CEsc = 19.
 \end{align}

% \begin{definition}
%   For a space-time path \( P \) of the head and an interval \( I \),
% let \( P_{u:v} \) be its part over the time interval \( \lint{u}{v} \).
% We write \( P_{:v} \) for the part from start to \( v \) and \( P_{u:} \)
% for the part from \( u \) to end.
% A part \( P_{u:v} \)
% is a \df{pass} of \( I \) if \( P \) enters \( I \) at time \( u \) and leaves it on the other side
% at time \( v \) (without leaving \( I \) before).
% \end{definition}

 \begin{definition}\label{def:interior}
  For an interval \( I=\lint{a}{b} \) and some real \( c \) let us define its \( c \)-\df{interior}
  \(  \Int(I,c) = \lint{a+c}{b-c} \).
  We will use it also  with negative \( c \), in which case this ``interior'' is really an extended
  neighborhood of \( I \).
 \end{definition}

  In the following definition, it is important to keep in mind the difference between noise and disorder.
  A noise-free space-time rectangle can very well contain disordered areas on the tape.
  
  \begin{definition}[Trajectory]\label{def:traj}
\begin{sloppypar}
   A history  \( (\eta, \Noise) \) of a generalized Turing 
machine~\eqref{eq:gen-TM} with \(\eta(t) =\)
\( (A(t), \h(t), \vhc(t)) \)
is called a \df{trajectory} of \( M \) if the following conditions hold, in any 
noise-free space-time interval \( I\times J \).
  \end{sloppypar}
\begin{description}

\item[Transition Function]\label{i:def.traj.transition}
Consider a switch, where the current cell-pair \( \vhc \)
is inside a clean area, by a distance of at least \( 2.5\B \).
Then the new state of the current cell-pair and
the direction towards the new current head position
are dictated by the transition function.
We further require that if the head moved right and \( \vhc \) has no 
right neighbor cell then a new adjacent right neighbor cell is created.
Similarly in the left direction.

The only change on the tape occurs on the interval enclosing the new and old current cells.
Further, the length of the dwell period before the switch is bounded by \( \Tu \).

\item[Spill Bound]\label{i:spill-bound}
  A clean interval can shrink by at most \( \CSpill \B \) while it does not contain the head.

\item[Escape] \label{i:def.traj.escape}
  The head will leave any interval of size \( \le \lambda\B \) with \( 1\le\lambda\le 3\beta \)
  within time \( \CEsc\lambda^{2}\Tu \).

\begin{sloppypar}
\item[Attack Cleaning] \label{i:def.traj.attack-cleaning}
Suppose that the current cell-pair is at the end of a clean interval \( \lint{a}{b} \) of size
\( \ge (\CMarg+1)\B \), with the head at position \( x \).
Suppose further that the transition function directs the head right.
Then by the time the head comes back to \( x-\CMarg \B \), the clean interval 
extends at least to \( \lint{a}{b+\B/2} \).
Similarly when ``left'' and ``right'' are interchanged.
 \end{sloppypar}

\item[Pass Cleaning]
    Suppose that a path \( P \) makes at least \( \passno \) passes over an interval \( I \)
  of size \( \le\CPass\B \).
  Then at some time during \( P \) the interior \( \Int(I, \CMarg\B) \) of \( I \) becomes clean.

\end{description}
\end{definition}

Recall that we will have a hierarchy of simulations \( M_{1}\to M_{2}\to \dotsm \) where
machine \( M_{k} \) simulates machine \( M_{k+1} \).
Our construction will set \( \passno=8k + O(1) \) for \( M_{k} \).
This can be interpreted as saying that each 8 passes will raise the ``organization level''.

\subsection{Scale-up}

Above, we have set up the conceptual structure of the construction and the proof.
Here are some of the parameters: 

\begin{definition}\label{def:hier-params}
  Let
  \(
   \Q_{k}=c_{1}\cdot 2^{1.2^{k}},\;
   \U_{k} = \Q_{k}^{3}=c_{1}^{3}\cdot 2^{3\cdot 1.2^{k}},\;
   \passno_{k}=8 k + c_{2}
\)
for appropriate constants \( c_{1},c_{2}>0 \).
 These sequences clearly satisfy~\eqref{eq:growth-assumption}, and
 define \( \B_{k}=\B_{1}\prod_{i<k}\Q_{i} \), 
 \( \Tu_{k}=\Tu_{1}\prod_{i<k}\U_{i} \).
\end{definition}

What remains is the definition of the simulation program and the decoding \( \Phi^{*} \),
and the proof that with this
program, the properties of a trajectory \( (\eta,\Noise) \) of machine \( M=M_{k} \) imply
that the history \( \Phi^{*}(\eta,\Noise)=(\eta^{*},\Noise^{*}) \) obeys the same trajectory
requirements on the next level.
The program is described in Sections~\ref{sec:sim-struc}-\ref{sec:healing}.
The most combinatorially complex part of the proof of trajectory properties
is in the key Section~\ref{sec:cleaning}, proving the trajectory properties related to bounding and
eliminating disorder.
Section~\ref{sec:computation} wraps up the proof of the main theorem.
% (Some parts of the paper will get added detail in the final version, but the present version
% leaves no essential question unanswered.)

% \newpage


\section{Some formal details}

We give here some details that were postponed from the overview section.

\subsection{Examples}\label{sec:examples}

The following examples show the difficulties responsible for various complexities of the
construction and proof.
Some of them may not be completely understandable without the details of the following program.
You can skip these examples safely, and return to them later when wondering
about the motivation for some feature.

\begin{example}[Need for zigging]\label{xmp:zig}
  When the head works in a colony of machine \( M_{1} \)
  performing a simulation of a cell of machine \( M_{2} \), it works in sweeps across the colony.
  But a small burst of faults could reverse the head in the middle of a sweep, leaving it uncompleted.
  This way local faults could create non-local inconsistency.

\end{example}

To handle the problem of Example~\ref{xmp:zig}, the head will proceed in zigzags: every
  step advancing the head in the simulation
  is followed by \( \Z \) steps 
  of going backward and forward again (with parameter \( \Z \) chosen appropriately), checking consistency
  (and starting a healing process if necessary).
  This will also enable the head to progress into a large
  disordered area, preventing it from being fooled repeatedly into going away, this way
  solving an even more serious problem.

\begin{example}[Need for feathering]\label{xmp:feather}
  Some big noise can create a number of intervals \( I_{1},I_{2},\dots,I_{n} \)
  consisting of colonies of machine \( M_{1} \), each interval with its own simulated head,
  where the neighboring intervals are in no relation to each other.
  When the head is about to return from the end of \( I_{k} \)
  (never even to zig beyond it, see the discussion after Example~\ref{xmp:zig}),
  a small burst of faults can carry it over to \( I_{k+1} \) where
  the situation may be symmetric: it will continue the simulation that \( I_{k+1} \) is performing.
  (The rightmost colony of \( I_{k} \) and the leftmost colony of \( I_{k+1} \) need not be complete:
  what matters is only that the simulation in \( I_{k} \) would not bring the head beyond its right end,
  and the simulation in \( I_{k+1} \) would not bring the head beyond its left end.)

  The head can be similarly captured to \( I_{k+2} \), then much later back from \( I_{k+1} \) to \( I_{k} \),
  and so on.
  This way the restoration of structure in \( M_{2} \) may be delayed too long.
\end{example}

The device by which we will mitigate the effect of this kind of capturing is another property of the
the movement of the head which will call \df{feathering}:
if the head turns back from a tape cell then next time it must go beyond.
This requires a number of adjustments to the program (see later).

\begin{example}[Two slides over disorder]
  This example shows the possibility for the head to slide twice over disorder without cleaning it.
  
Consider two levels of simulation as outlined in Section~\ref{sec:hier}: 
machine \( M_{1} \) simulates \( M_{2} \) which simulates \( M_{3} \).
The tape of \( M_{1} \) is subdivided into colonies of size \( \Q_{1} \).
A burst of faults on level 1 has size \( O(1) \), while a burst on level 2 has size \( O(\Q_{1}) \).

Suppose that \( M_{1} \) is performing a simulation in colony \( C_{0} \).
An earlier big burst may have created a large interval \( D \) of disorder
on the right of \( C_{0} \), even reaching into \( C_{0} \).
For the moment, let \( C_{0} \) be called a \df{victim} colony.
Assume that the left edge of \( D \) represents the last stage of a transfer operation to the right neighbor 
colony \( C_{0}+Q_{1} \).
When the head, while performing its work in \( C_{0} \), moves close to its right end, a small burst may 
carry it over into \( D \).
There it will be ``captured'', and continue the (unintended) right transfer operation.
This can carry the head, over several successful colony simulations in \( D \), to some
victim colony \( C_{1} \) on the right from which it will be captured to the right similarly.
This can continue over new and new victim colonies \( C_{i} \) (with enough space between them to
allow for new faults to occur), all the way inside the disorder \( D \).
So the \( M_{2} \) cells in \( D \) will fail to simulate \( M_{3} \).

After a while the head may return to the left in \( D \)
(performing the simulations in its colonies).
When it gets at the right end of a victim colony \( C_{i} \), a burst of faults might move it back there.
There is a case when \( C_{i} \) now can just continue its simulation and then send the head
further left: when before the head was captured on its right,
it was in the last stage of simulating a left turn of the head of machine \( M_{2} \).

In summary, a big burst of faults
can create a disordered area \( D \) which can capture the head and on which the head can slide
forward and back without recreating any level of organization beyond the second one.
\end{example}

\begin{example}[Many slides over disorder]\label{xpl:unbounded}
  Let us describe a certain ``organization'' of a disordered area in which an unbounded number of passes
  may be required to restore order.
For some \( n<0 \), let the cells of \( M_{1} \) at positions
\( x_{-\Q_{1}},\dots,x_{n} \), where \( x_{i+1}=x_{i}+\B_{1} \),
represent part of a healthy colony \( C(x_{-\Q_{1}}) \) starting at \( x_{-\Q_{1}} \), where \( x_{n} \)
is the rightmost cell of \( C(x_{-\Q_{1}}) \)
to which the head would come in the last sweep before
the simulation will move to the \emph{left} neighbor colony \( C(x_{-2\Q_{1}}) \).
Let them be followed by cells \( x_{n+1},\dots, x_{\Q_{1}-1},\dots\)
which represent the last sweep of a transfer operation to the \emph{right} neighbor colony \( C(x_{0}) \).
If the head is in cell \( x_{n} \), a burst of faults can transfer it to \( x_{n+1} \).
The cell state of \( M_{2} \) simulated by \( C(x_{-\Q_{1}}) \) need to be in \emph{no relation} to 
the cell state of \( M_{2} \) simulated by \( C(x_{0}) \).
This was a capture of the head by a burst of \( M_{1} \) across the point 0, to the right.

We can repeat the capture scenario, say around points \( i \Q_{1}\Q_{2} \) for \( i=1,2,\dots \),
and this way cells of \( M_{3} \) simulated by \( M_{2} \) (simulated by \( M_{1} \))
can be defined arbitrarily, with no consistency needed between any two neighbors.
(We did not write \( i \Q_{1} \) just in case bursts are not allowed in neighboring colonies.)
In particular, we can define them to implement a \emph{leftward} capture scenario
via level 3 bursts at points \( i \Q_{1}\Q_{2}\Q_{3}\Q_{4} \), allowing to simulate arbitrary cells of \( M_{5} \)
with no consistency requirement between neighbors.
So \( M_{5} \) could again implement a rightward capture scenario, and so on.
In summary, a malicious arrangement of disorder and noise allows \( k \) passes
after which the level of organization is still limited to level \( 2 k + 1 \).
\end{example}

Our construction will ensure that, on the other hand,
\( O(k) \) passes (free of \( k \)-level noise) will restore organization to level \( k \).
This property of the construction will be incorporated into our definition of a generalized
Turing machines as the ``magical'' property~\eqref{i:many-slides} above.

\subsection{Codes}\label{sec:codes}

The input of our computation will be encoded by some error-correcting code,
to defend against the possibility of losing information even at the first reading.

\begin{definition}[Codes]\label{def:codes}
    Let \( \Sigma_{1},\Sigma_{2} \) be two finite alphabets.
    A \df{block code} is given by a positive integer \( \Q \)---called
    the \df{block size}---and a pair of functions
    \begin{align*}
            \psi_{*} :\Sigma_{2}\to\Sigma_{1}^{\Q},
            \quad
            \psi^{*}:\Sigma_{1}^{\Q}\to\Sigma_{2}
    \end{align*}
    with the property \( \psi^{*}(\psi_{*}(x))=x \).
    Here \( \psi_{*} \) is the encoding function (possibly introducing redundancy)
    and \( \psi^{*} \) is the decoding function (possibly correcting errors).
    The code is extended to (finite or infinite) strings by encoding each letter individually:
\begin{align*}
 \psi_{*}(x_{1},\dots,x_{n})=\psi_{*}(x_{1})\dotsm\psi_{*}(x_{n}) .
\end{align*}

\end{definition}

\subsection{Proof of the sparsity lemma}

\begin{proof}[Proof of Lemma~\ref{lem:sparsity}]
  The proof uses slightly more notation than it would if we simply assumed independence
  of faults at different space-time sites, but it is essentially the same.

  Let \( \cE_{k}(\x) \) be the event  \( B(\x,\pair{\B_{k}}{\Tu_{k}})\cap E^{(k)}\neq\emptyset \).
  Let \( \cM=\cM_{k}(\x) \) be the set of minimal sets \( A\subseteq \bbZ\times\bbZ_{+} \)
with  \( A\subseteq E\imp \cE_{k}(\x) \).

  \begin{claim}
    Each set in \( \cM_{k}(\x) \) is contained in \( B(\x,2\gamma(\B_{k},\Tu_{k})) \).
  \end{claim}
  \begin{proof}
    The statement is clearly true for \( k=1 \).
    Suppose it is true for \( k \), let us prove it for \( k+1 \).
    The event \( \cE_{k+1}(\x) \) holds if and only if \( \x\in E \) and there is some point \( \y \)
    in \( E^{(k)}\cap B(\x,\gamma(\B_{k+},\Tu_{k+1}))\setminus B(\x,\beta(\B_{k},\Tu_{k})) \).
    Then by the inductive assumption, \( A\subseteq E \) for some
    set \( A\subseteq B(\y,2\gamma(\B_{k},\Tu_{k})) \),
    with the property \( A\subseteq E\imp \cE_{k}(\y) \).
    Then \( A\cup\{\x\}\subseteq E\imp \cE_{k+1}(\x) \).
    Also 
\begin{align*}
  A\subseteq B(\x,(\gamma\B_{k+1}+2\gamma\B_{k},\gamma\Tu_{k+1}+2\gamma\Tu_{k}))
\subseteq B(\x,2\gamma(\B_{k+1},\Tu_{k+1})),
\end{align*}
since \( \Tu_{k+1}/\Tu_{k}> 2 \), \( \B_{k+1}/\B_{k}> 2 \).
\end{proof}

Let \(  f_{k}(\x)= \sum_{A\in\cM}\eps^{|A|} \).
By the union bound we have \( \Prob(\cE_{k}(x))\le f_{k}(\x) \).

Let \( p_{k}= \eps\cdot 2^{-1.5^{k-1}} \).
We will prove \(   f_{k}(\x) < p_{k} \) by induction.
For \( k=1 \), rectangles \( B(\x_{i},\pair{\B_{1}}{\Tu_{1}}) \) have size \( 1 \), so
by the \( \eps \)-boundedness, \( f_{1}(\x)<\eps \).
 Assume that the statement holds for \( k \), we will prove it for \( k+1 \).

 Suppose \( \y \in E^{(k)}\cap B(\x,\pair{\B_{k+1}}{\Tu_{k+1}}) \).
According to the definition of \( E^{(k)} \),  there is a point
\begin{align}\label{eq:sparse-as}
 \vek{z} \in
 B(\y,\gamma\pair{\B_{k+1}}{\Tu_{k+1}})\cap E^{(k)}\setminus B(\y,\beta\pair{\B_{k}}{\Tu_{k}}).
 \end{align}
Consider a standard partition of space-time into rectangles \(  K_{p}=B(\vek{c}_{p},(\B_{k},\Tu_{k})) \).
Let 
\begin{align*}
       I&=\setOf{p}{K_{p}\cap B(\x,\gamma\pair{\B_{k+1}}{\Tu_{k+1}})\ne\emptyset}.
 \end{align*}
 We are only interested in rectangles \( K_{p} \) with \( p\in I \).
 Let
\begin{align*}
  K'_{p}=B(\vek{c}_{p},((2\gamma+1)\B_{k},(2\gamma+1)\Tu_{k})) .
\end{align*}
If \( K_{i},K_{j} \) are the rectangles in this partition containing \( \y \) and \( \vek{z} \), then
\( K'_{i}\cap K'_{j}=\emptyset \).
This follows from the fact that \( \abs{y_{1} - z_{1}}>\beta\B_{k} \),
\( \abs{y_{2} - z_{2}}>\beta\Tu_{k} \), and \( \beta> 4\gamma \) 
in Definition~\ref{def:sparsity}.
By the above Claim, the event \( \y\in E^{k} \) can be written as
\( \bigcup_{A\in\cM_{k}(\y)}\{A\subseteq E\} \)
where \( A\subseteq B(\y,2\gamma(B_{k},\Tu_{k}))\subseteq K'_{i} \)
for each \( A\in\cM_{k}(\y) \).
Similarly for \( \vek{z} \).
Let \( \cM(i)=\bigcup_{\y\in K_{i}}\cM_{k}(\y) \), then each set \( A\in\cM(i) \) is in \( K'_{i} \).
The disjointness of \( K'_{i} \) and \( K'_{j} \) and the inductive assumption implies
\begin{align}\nonumber
  f_{k+1}(\x) &\le \sum_{i,j\in I, K'_{i}\cap K'_{j}=\emptyset}\sum_{A\in\cM(i),A'\in\cM(j)}\eps^{|A|+|A'|}
   =\sum_{i,j\in I, K'_{i}\cap K'_{j}=\emptyset}f_{k}(\vek{c}_{i})f_{k}(\vek{c}_{j})
  \\\label{eq:sparsity.last-line}&\le |I|^{2}p_{k}^{2} = |I|^{2}\eps^{2} 2^{-1.5^{k}}\cdot 2^{-0.5\cdot 1.5^{k-1}}
       = p_{k+1}\eps |I|^{2}2^{-0.5\cdot 1.5^{k-1}}.
\end{align}
We have  \( |I|\le (2\gamma \Q_{k}+1)(2\gamma\U_{k}+1) \).
Since \( \lim_{k}\frac{\log\U_{k}\Q_{k}}{1.5^k}=0 \), 
the multiplier of \( p_{k+1} \) in~\eqref{eq:sparsity.last-line} is \( \le 1 \) for sufficiently small  \( \eps \).
\end{proof}

\subsection{Configuration, history}\label{sec:gen-TM}

A configuration, as defined below, contains a pair of positions
\( \vhc = (\hc_{0},\hc_{1}) \) called the \df{current cell-pair}:
In difference to the Turing machines of Section~\ref{sec:TM},
the position of the head may not be exactly between the current cells: this allows the model
to fit into the framework where 
a generalized Turing machine \( M^{*} \) is simulated by some (possibly
generalized) Turing machine \( M \).
The head \( h \) of \( M \)---made equal to that of \( M^{*} \)---may
oscillate inside and around the current cell-pair of \( M^{*} \).

\begin{definition}[Configuration]\label{def:config}
    A \df{configuration} \( \xi \) of a generalized Turing machine~\eqref{eq:gen-TM} is a tuple
    \begin{align*}
      (A,\h,\vhc) = (\xi.\tape,\xi.\pos, (\xi.\curcell_{0},\xi.\curcell_{1}))
    \end{align*}
    where \( A:\bbZ\to\Sigma \) is the tape, \( \h \in\bbZ \) is the head position, \( \vhc\in\bbZ^{2} \)
    is the current cell-pair.
    We have \( A(p)=\Vacant \) in all but finitely many positions \( p \).
Whenever the interval \( \h+\lint{-4\B}{4\B} \) is clean the current cell-pair
must be within it.
Let
    \begin{align*}
         \Configs_{M}
    \end{align*}
    denote the set of all possible configurations of a Turing machine \( M \).
\end{definition}

The above definitions can be localized to define a configuration 
over a space interval \( I \) containing the head.

\begin{definition}[History]
For a generalized Turing machine~\eqref{eq:gen-TM}, consider
a sequence \( \eta = \) \( ( \eta(0,\cdot),\) \( \eta(1,\cdot),\) \( \dots) \),
along with a noise set \( \Noise \).
Let \( \h(t)= \eta(t,\cdot).\pos \) be the position of head.

A \df{switching time} is a noise-free time when any part of \( \eta \) other than \( \h(t) \)
changes.
A \df{dwell period} is the interval between any consecutive pair of
switching times with the property that the
space-time rectangle between them and containing the head is clean and noiseless.

The pair \(  (\eta,\Noise) \) will be called a \df{history}
of machine \( M \) if the following conditions hold.
\begin{itemize}
\item \( \abs{\h(t) - \h(t')} \le \abs{t' - t} \).
  
\item In two consecutive configurations, the content \( A(p,t) \) of the positions \( p \)
  not in \( \h(t) + \lint{-2\B}{2\B} \) remains the same.
\item At each noise-free switching time the head is on the new current cell-pair:
  \( \hc_{0}(t)=\h(t) \).
(In particular, when at a switching time a current cell becomes
\( \Vacant \), the head must already be elsewhere.)

\item The length of dwell periods is at most \( \Tu \).

\end{itemize}
The above definition can be localized to define a history \( I\times J \) containing the head.
Let
\begin{align*}
  \Histories_{M}
\end{align*}
denote the set of all possible histories of \( M \).
\end{definition}



\subsection{Hierarchical codes}\label{sec:hier-codes}

Recall the notion of a code in Definition~\ref{def:codes}.

\begin{definition}[Code on configurations]\label{def:configuration-code}
\begin{sloppypar}
 Consider two generalized Turing machines \( M_{1},M_{2} \) with the corresponding
alphabets and transition functions, where \( \B_{2}/\B_{1} \) is an integer denoted \( \Q=\Q_{1} \).
Assume that a block code
\(
   \psi_{*}:\Sigma_{2}\to\Sigma_{1}^{\Q}
\)
is given, with an appropriate decoding function, \( \psi^{*} \).
Symbol \( a\in\Sigma_{2} \), is interpreted as the content of some tape square.
\end{sloppypar}

This block code gives rise to a \df{code on configurations}, that is a pair of functions
    \begin{align*}
        \varphi_{*} :\Configs_{M_{2}} \to \Configs_{M_{1}},
        \quad
        \varphi^{*}:\Configs_{M_{1}} \to \Configs_{M_{2}}
    \end{align*}
    that encodes some (initial) configurations \( \xi \) of \( M_{2} \) into configurations of \( M_{1} \):
    each cell of \( M_{2} \) is encoded into a colony of \( M_{1} \) occupying the same interval.
    Formally, assuming \( \xi.\curcell_{j}=\xi.\pos+(j-1)\B_{2} \), \( j=0,1 \) 
    we set \( \varphi_{*}(\xi).\pos = \xi.\pos \),
    \( \varphi_{*}(\xi).\curcell_{j}= \varphi_{*}(\xi).\pos+(j-1)\B_{2} \),
  and
\begin{align*}
 \varphi_{*}(\xi).\tape(i\B_{2},i\B_{2}+\B_{1}, \dots, (i+1)\B_{2} - \B_{1}) = \psi_{*}(\xi.\tape(i)).
 \end{align*}
 \end{definition}


\begin{definition}[Hierarchical code]\label{def:hierarchical-code}
For \( k\ge 1 \), let \( \Sigma_{k} \) be an alphabet, of a generalized Turing machine \( M_{k} \).
Let \( \Q_{k}=\B_{k+1}/\B_{k} \) be an integer (viewed as colony size), let \( \varphi_{k} \)
be a code on configurations defined by a block code
  \begin{align*}
       \psi_{k}: \Sigma_{k+1}\to \Sigma_{k}^{\Q_{k}}
  \end{align*}
as in Definition~\ref{def:configuration-code}.
The sequence \( (\Sigma_{k},\varphi_{k}) \), (\( k\ge 1) \),  is
called a \df{hierarchical code}.
For this hierarchical code, configuration \( \xi^{1} \) of \( M_{1} \)
is called a \df{hierarchical code configuration} of height \( k \) if a sequence
of configurations \( \xi^{2},\xi^{3},\dots,\xi^{k} \) of \( M_{2},M_{3},\dots,M^{k} \) exists with
\begin{align*}
 \xi^{i}=\varphi_{*i}(\xi^{i+1})
 \end{align*} 
for all \( i \).
If we are also given a sequence of mappings \( \Phi^{*}_{1} \), \( \Phi^{*}_{2} \), \( \dots \) 
such that for each \( i \), the pair \( (\varphi_{i*},\Phi_{i}^{*}) \),
is a simulation of \( M_{i+1} \) by \( M_{i} \) 
then we have a \df{hierarchy of simulations} of height \( k \).
\end{definition}

We will construct a hierarchy of simulations whose height grows during the
computation---by a mechanism to be described later.


\section{Simulation structure}\label{sec:sim-struc}

In what follows we will describe the program of the reliable Turing machine:
a hierarchical simulation in which simultaneously each \( M_{k+1} \) is simulated
by \( M_{k} \), with an added mechanism to
raise the height of the hierarchy when needed.
Most of the time, we will write \( M=M_{k} \),  \( M^{*}=M_{k+1} \).
Ideally, cells will be grouped into colonies of size \( \Q=\B^{*}/\B \).
Simulating one step of \( M^{*} \) takes a sequence of steps of \( M \)
constituting a \df{work period}.
Machine \( M \) will perform the simulation as long as the noise
in which it operates is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse
(as in Definition~\ref{def:isolation}).
This means, informally, that a burst of noise affects at most \( \beta \) consecutive tape cells,  
and there is at most one burst in any \( \gamma \)  neighboring work periods.
A design goal for the program is to
correct a burst within space much smaller than a colony.

\df{Modes} were introduced in Section~\ref{sec:language}.
Ordinary simulation proceeds in the normal mode. 
To see whether consistency, that is the basic tape pattern
supporting simulation, is broken somewhere, a very local precaution will be taken in each step:
each step will check whether the current cell-pair is \df{coordinated}
(see Definition~\ref{def:coordinated}).
If not then the mode of the current pair will be changed into \df{healing}.
We will also say that \df{alarm} will be called.
On the other hand, the state may enter into \df{rebuilding} mode on some indications that
healing fails.


\subsection{Error-correcting code}\label{sec:coding}

Let us add error-correcting features to the block codes introduced in
Definition~\ref{def:codes}.

\begin{sloppypar}
\begin{definition}[Error-correcting code]\label{def:err-code}
A block code is \( (\beta,t) \)-\df{burst-error-correcting},
if for all \( x\in\Sigma_{2} \), \( y\in\Sigma_{1}^{\Q} \) we
have \( \psi^{*}(y)=x \) whenever \( y \) differs from
\( \psi_{*}(x) \) in at most \( t \) intervals of size \( \le\beta \).
For such a code, we will say that a word \( y\in\Sigma_{1}^{\Q} \) is \( r \)-\df{compliant}
if it differs from a codeword of the code by at most \( r \) intervals of size \( \le\beta \).
\end{definition}
  \end{sloppypar}

\begin{example}[Repetition code]\label{xmp:tripling}
  Suppose that \( \Q\ge 3\beta \) is divisible by 3,
  \( \Sigma_{2}=\Sigma_{1}^{\Q/3} \), \( \psi_{*}(x)=xxx \).
  If \( y=y(1)\dots y(\Q) \), then \( x=\psi^{*}(y) \) is defined by
    \( x(i)=\maj(y(i),y(i+\Q/3),y+2\Q/3) \).
    For all \( \beta\le \Q/3 \), this is a
    \( (\beta,1) \)-burst-error-correcting code.
    If we repeat 5 times instead of 3, we get a \( (\beta,2) \)-burst-error-correcting
    code.
  \end{example}

  \begin{example}[Reed-Solomon code]\label{xmp:Reed-Solomon}
    There are much more efficient such codes than just repetition.
    One, based on the Reed-Solomon code, is outlined in Example 4.6
    of~\cite{GacsSorg01}.
    If each symbol of the code has \( l \) bits then the code can be up to \( 2^{l} \) symbols long.
    Only \( 2 t\beta \) of its symbols need to be redundant in order
    to correct \( t \) faults of length \( \beta \).
  \end{example}

Consider a (generalized) Turing machine 
\( (\Sigma,\tau) \) simulating some Turing machine \( (\Sigma^{*},\tau^{*}) \).
We will assume that the alphabet \( \Sigma^{*} \) is a subset of the set of  binary strings
\( \{0,1\}^{l} \) for some \( l<\Q \).

% \begin{definition}\label{def:colony-interior}
% Let 
% \begin{align*}
%   \PadLen 
% \end{align*}
% be a parameter to be defined later (in~\eqref{eq:PadLenDef}).
% A cell belongs to the \df{interior} of a colony spanning an interval \( I \)
% if it is in \( \Int(I,\PadLen) \) (with the interior as in Definition~\ref{def:interior}).
% The set \( \Int(I,\F)\setminus\Int(I,\PadLen) \) will be called the \df{turn region}.
% \end{definition}

% We will store the coded information in the interior of the colony, since it is more exposed 
% to errors near the boundaries.
Let \( (\upsilon_{*}, \upsilon^{*}) \) be a \( (\beta,3) \)-burst-error-correcting block code
with
\begin{align*}
  % \upsilon_{*}: \{0,1\}^{l} \cup \set{\emptyset}
  %  \to\{0,1\}^{(\Q-2\cdot\PadLen)\B}.
  \upsilon_{*}: \{0,1\}^{l} \cup \set{\emptyset}
   \to\{0,1\}^{Q\B}.
\end{align*}
We could use, for example, the repetition code of Example~\ref{xmp:tripling}.
Other codes are also appropriate, but we require that there are some fixed Turing machines
\( \Encode \) and \( \Decode \) computing them:
 \begin{align*}
   \upsilon_{*}(x)=\Encode(x),\quad 
   \upsilon^{*}(y)=\Decode( y).
 \end{align*}
Also, these programs must work in quadratic time and linear space on a one-tape
Turing machine (as the repetition code certainly does).

% Let us now define the block code \( (\psi_{*}, \psi^{*}) \) used below in the
% definition of the configuration code \( (\varphi_{*}, \varphi^{*}) \)  
% outlined in Section~\ref{sec:hier-codes}:
% \begin{equation}\label{eq:psi}
%    \psi_{*}(a)  = 0^{\PadLen}\upsilon_{*}(a)0^{\PadLen}.
% \end{equation}
% The decoded value \( \psi^{*}(x) \) is obtained by first removing \( \PadLen \)
% symbols from both ends of \( x \) to get \( x' \), and then computing \(
% \upsilon^{*}(x') \).
Let us now define the block code \( (\psi_{*}, \psi^{*}) \) used below in the
definition of the configuration code \( (\varphi_{*}, \varphi^{*}) \)  
outlined in Section~\ref{sec:hier-codes} will be equal to \( (\upsilon_{*},\upsilon^{*}) \).
 It will be easy to compute the configuration code from \( \psi_{*} \),
once we know what fields there need initialization.

% \section{Specifying a Turing machine}\label{sec:specifying}

% \subsection{Universal Turing machine}\label{sec:UTM}

% We define universal Turing machines in a way that allows for rather general ``programs''.

% \begin{definition}[Standard pairing]
% For a (possibly empty) binary string\\ \( x=x(1)\dotsm x(n) \) let us introduce the map
%  \[
%    \ang{x} = 0^{\abs{x}}1 x,
%  \]
% Now we encode pairs, triples, and so on, of binary strings as follows:
%  \begin{align*}
%         \ang{s,t} &=\ang{s}t,
% \\ \ang{s,t,u} &= \ang{\ang{s,t},u},
%  \end{align*}
% and so on.

% As in~\eqref{eq:Sigma_k}, we will view
% tape symbols as binary strings of a certain length.
% Also, if we write \( \ang{i,u} \) where \( i \) is some number, it is understood
% that the number \( i \) is represented in a standard way by a binary string.
% \end{definition}

% In the main result, we only consider the reliable simulation of a machine that outputs a single symbol.
% But the construction itself uses also machines with longer outputs, so here is a convention.

% \begin{definition}[Computation result, universal machine]
%  Assume that a Turing machine \( M \) starting on binary \( x \),
\( 
 \)
%  halts at some time \( t \) (``halting''' can be defined by any convention, even just writing a blank at position 0).
%  Then the longest (possibly empty) binary string found starting at position
%  1 is called the \df{computation result} \( M(x) \).
% We will write
%  \begin{align*}
%    M(x,y)=M(\ang{x,y}),\quad M(x,y,z)=M(\ang{x,y,z}),
%  \end{align*}
% and so on.

% A Turing machine \( U \) is called \df{universal} among Turing machines with
% binary inputs and outputs, if for every Turing machine \( M \),
% there is a binary string \( p_{M} \) such that for all \( x \) we have
% \( U(p_{M},x)=M(x) \).
% \end{definition}

% Let us introduce a special kind of universal Turing machines, to be
% used in expressing the transition functions of other Turing machines.
% % These are just the Turing machines for which the so-called \( s_{mn} \) theorem
% % of recursion theory holds with \( s(x,y)=\ang{x,y} \).

% \begin{definition}[Flexible universal Turing machine]\label{def:univ-TM}
% A universal Turing machine will be called \df{flexible} if 
% whenever \( p \) has the form \( p=\ang{p',p''} \) then
% \begin{align*}
%  U(p,x)= U(p',\ang{p'',x}).
%  \end{align*}
% % (Even if \( x \) has the form \(x =\ang{x',x''} \), this definition chooses
% % \( U(p',\ang{p'',x}) \) over \( U(\ang{p,x'},x'') \), that is starts with 
% % parsing the first argument.)
% \end{definition}

% On input \( \ang{p,x} \), a flexible machine first checks whether its ``program'' \( p \) 
% has the form \( p=\ang{p',p''} \).
% If yes, then it applies \( p' \) to the pair \( \ang{p'',x} \).
% (Otherwise it just applies \( p \) to \( x \).)
% It is easy to see that there are flexible universal Turing machines.

% \begin{definition}[Transition program]
%   Consider an arbitrary Turing machine \( M \) with alphabet
% \( \Sigma \), and transition function \( \tau \).
% A binary string \( \pi \) will be called a \df{transition program} of \( M \) if
% whenever \( \tau(\va)=(\va',j) \) we have \( U(\pi,\va)=\ang{\va,j} \).
% We will also require that the computation induced by the program makes
% \( O(\abs{\pi}+\abs{a}) \) left-right turns, over a length of tape \( O(\abs{\pi}+\abs{a}) \).
% \end{definition}

% In our simulations of a Turing machine \( M_{2} \) on some other machine \( M_{1} \),
% the transition program of \( M_{2} \) will just provide a way to compute
% the (local) transition function of \( M_{2} \) by the universal machine;
% it does not organize the rest of the simulation.

% \begin{remark}
%  In the construction of universal Turing machines provided by the textbooks
% (though not in the original one given by Turing), the program is generally a string
% encoding a table for the transition function \( \tau \) of the simulated machine \( M \).
% Other types of program are imaginable: some simple transition functions can
% have much simpler programs.
% However, our fixed machine is good enough (similarly to the optimal machine
% for Kolmogorov complexity).
% If some machine \( U' \) simulates \( M \) via a
% very simple program \( q \), then
%  \begin{align*}
%      M(x)=U'(q,x) = U(p_{U'},\ang{q,x}) = U(\ang{p_{U'},q},x),
%  \end{align*}
% so \( U \) simulates this computation via the program \( \ang{p_{U'},q} \).
% \end{remark}

\subsection{Rule language}\label{sec:language}

The generalized Turing machines \( M_{k} \) to be defined
differ only in the parameter \( k \).
We will denote therefore \( M_{k} \) frequently simply by \( M \),
and \( M_{k+1} \), simulated by \( M_{k} \),  by \( M^{*} \).
Similarly we will denote the colony size \( \Q_{k} \) by \( \Q \).

We will describe the transition function
\( \tau_{k}=\tau \)  mostly in an informal way, as procedures of a program,
these descriptions are readily translatable into a set of \df{rules}.
Each rule consists of some (nested) conditional statements,
similar to the ones seen in an ordinary program:
 ``\textbf{if} \textit{condition} \textbf{then} \textit{instruction}
\textbf{else} \textit{instruction}'', 
where the condition is testing values of some fields of the observed cell-pair, and
the instruction can either be elementary, or itself a conditional statement. 
The elementary instructions are an \df{assignment} of a value to a field
of a cell symbol, or a command to move the head.
It will then be possible to write one fixed \emph{interpreter} Turing machine that carries
out these rules, assuming that the whole set of rules is a string and each field is also represented
as a string.

Assignment of value \( x \) to a field \( y \) of the state or cell symbol will
be denoted by \( y \gets x \).
% We will also use some conventions introduced by the C language:
% namely,
% \( x\gets x+1 \) and \( x\gets x-1 \) are abbreviated to \( \increment{x} \) and
% \( \decrement{x} \) respectively.

% Rules can also have parameters, like \( \ruSwing(a,b,u,v) \).
% Since each rule is called only a constant number of times in the whole program,
% the parametrized rule can be simply seen as a shorthand.


\subsection{Fields}\label{sec:fields}

% \begin{sloppypar}
% For the machine \( M \) we are constructing, each tape symbol will 
% be a tuple \( q=(q_{1},q_{2},\dots,q_{k}) \),
% where the individual elements of the tuple will be called \df{fields}, and will
% have symbolic names.
% For example, we will have fields \( \Addr \) and \( \Drift \),
% and may write \( q_{1} \) as \( q.\Addr \) or just \( \Addr \), 
% \( q_{2} \) as \( q.\Drift \) or \( \Drift \), and so on.
% \end{sloppypar}

% The array of values of the same field of the cells will be called a \df{track}.
% Thus, we will talk about the \( \Hold \) track of the tape, carrying the
% \( \Hold \) fields of the cells.
%  Each field of a cell has a possible value
% \( \emptyset \) whose approximate meaning is ``undefined''.

In what follows we describe some of the most important fields we will use in the cells;
others will be introduced later.

A properly formatted configuration of \( M \) splits the tape into blocks of \( \Q \)
consecutive cells called \df{colonies}.
One colony of the tape of the simulating
machine represents one cell of the simulated machine.
The two colonies that correspond to the cell-pair that the
simulated machine is scanning is called the \df{base colony-pair}.
The formal definition of a colony-pair (as well as some other concepts) must have two
forms: one for the program, based on some field values in cells,
and one for our reasoning about the history.

Sometimes the left base colony will just be called the \df{base colony}.
Most of the computation proceeds over the base colony-pair.
The direction of the simulated head movement, once figured out by the computation,
is called the \df{drift}.
Two neighbor colonies may not be adjacent, in which case the cells will form
a \df{bridge} between them.

Here is a description of some of the fields:
\begin{description}
\item[Mode] The present behavior of the computation will be characterized by
a field of the current cell called the \df{mode}:
 \begin{align*}
   \Mode\in\{ \Normal,\Healing, \Rebuilding,\Booting \}.
 \end{align*}
 If its value is \( \Normal \) we will say that the computation is in \df{normal} mode.
 In this case, the machine is engaged in the regular business of simulation.
The \df{healing} mode tries to correct some local fault due to a couple of neighboring
bursts of faults (of size \( \beta \)),
while the \df{rebuilding} mode attempts to restore the colony structure
on the scale of a couple of colonies.
The \df{booting} mode is used for setting up the structure initially and for starting or continuing the
simulation of the Turing machine \( \G \) of the main theorem.

\item[Info] The  \( \Info \) track of a colony of \( M \)
  contains the string that encodes the content of the simulated cell of \( M^{*} \).

\item[Address] The field \( \Addr \)
of the cell shows the position of the cell in its colony:
it takes values in \( \lint{0}{\Q} \).

\item[Drift] The direction in \( \{-1,1\} \) in which the simulated head moves will be recorded on the track
 \( \Drift \).

\item[Sweep] The \( \Sweep \)
  field keeps track of the number of sweeps that the head made during the work period.

\item[Kind] Cells will be designated as belonging to a number of
  possible \df{kinds}, signaled by the field \( \Kind \)
with values \(  \Booting, \Stem, \Member_{0},\Member_{1},\Bridge,\Outer \).
Here is a description of their role.
\begin{itemize}

\item A cell is of the \( \Booting \) kind if it is on the top level of simulation (see Section~\ref{sec:hier}).

\item \( \Stem \) is the kind of newly created cells
  whose role in the colony structure is not clear yet, or of cells whose previous
  role has been erased.

\item Cells of the base colony-pair are of type \( \Member_{0} \) and \( \Member_{1} \) respectively.
  Members of other colonies have the kind \( \Outer \).

\item If two colonies are close but not adjacent then there will be \( <\Q \)
adjacent cells of type \( \Bridge \) between them.
Some fields characterize bridges further.
Field \( \BridgeDir \) has values \( -1,1 \) showing the direction of the bridge.
If it is \( 1 \) then the bridge is adjacent on the right to a colony and continues (modulo \( \Q \))
its addresses.
If it is \( -1 \) then it is doing this towards the left.
Field \( \BridgeKind \) is \( \Inner \) or \( \Outer \) depending on whether it is between the base colony pair
or not.

\end{itemize}

\item[Heal, Rebuild] During healing, some special fields of the state and cell are used,
  treated as subfields of the field \( \Heal \).
In particular, there will be a \( \Heal.\Sweep \) field.
During rebuilding, we will work with subfields of the field \( \Rebuild \).
A cell will be called \df{marked for rebuilding} if \( \Rebuild.\Sweep\ne 0 \).

\end{description}

\begin{remark}
  The \( \Outer \) kind is redundant:
  whether a cell is outer can be computed from its \( \Drift \) and \( \Sweep \) fields.
  But we use it for clarity.
\end{remark}

\subsection{Head movement}\label{sec:sweep}

The global structure of a work period is this:
\begin{description}

\item[Simulation phase]
Compute the new state of the simulated cell-pair, and the simulated direction (called the drift).
Then check the ``meaningfulness'' of the result.

\item[Transfer phase]
  The head moves into the neighbor colony-pair
  in the simulated head direction called drift (passing a bridge if needed).
  Redundancy is used to protect this movement from faults.
\end{description}

During the work period the head sweeps, roughly, back-and-forth between
the ends of the base colony-pair.
The field \( \Sweep \) keeps track of the number of sweeps made within the work period.
This together with the address and kind of the current cell is used by the program to determine
the actions to be performed.

\begin{definition}[Front]\label{def:front}
As the head sweeps in a certain direction, it increases \( \Sweep \) field by 1.
  The last site in which this increase occurred will be called the \df{front}. 
\end{definition}

Globally in a configuration, due to earlier faults, there may be more than one front, but locally
we can talk about ``the'' front without fear of confusion.
The direction of the sweep \( s \) is determined by its parity:
\begin{align*}
  \dir(s)=(-1)^{s + 1}.
\end{align*}

 Bridges between colonies present some extra complication---let us address it.

\begin{definition}[Gaps]\label{def:gaps}
If the bodies of two cells are not adjacent, but are at a distance \( <\B \) then the space
between them is called a \df{small gap}.
We also call a small gap such a space between the bodies of two colonies.
On the other hand, if the distance of the bodies of two colonies is \( >\B \) 
but \( <\Q\B \) then the space between them is called a \df{large gap}.
\end{definition}

A large gap between two colonies will be filled by a bridge.
A bridge is always adjacent to one of the two colonies and consists of cells
adjacent to each other.

Building a bridge or making repairs may involve
``killing'' some cells that are in the way and replacing them with new ones.
Each of these actions happens in three steps.
For cell pair \( (x,y) \) to kill its left (possibly non-adjacent) neighbor cell \( z \),
the cell pair first moves
into \( (z,x) \), then it moves back to \( (x,y) \) while making \( z \) vacant, then moves left again,
creating a new cell \( z' \) adjacent to \( x \) (space became available now). 

 Due to the zigging and feathering requirements mentioned in Section~\ref{sec:novelties},
 there will be two kinds of turns: \df{small} turns and \df{big} turns.
 The process uses the parameters
\begin{align}\label{eq:FDef}
   \f = 6,\; \Z = \passno^{1+\rho},\; \F = \Z\passno^{2+\rho}
\end{align}
with \( \rho>0 \).
The choices will be motivated in Sections~\ref{sec:feathering} and~\ref{sec:pass-cleaning}.

\subsubsection{Zigging}\label{sec:zigging}

A \df{zigzag} movement will be superimposed on sweeping,
so that the head can check the consistency of a few cells around the front.
The process creates a \df{frontier zone} of about \( 2\Z \) cells in both directions around the front,
where \( \Z \) was defined in~\eqref{eq:FDef}.
This interval is marked by the values of the field
\begin{align*}
  \ZigAddr\in\lint{-2\Z}{2\Z}
\end{align*}
which is undefined elsewhere.
After every \( \f \) steps of progress, the head will perform a forward-backward-forward
zigzag checking and updating the zone.
For this it uses \df{small turns} defined in Section~\ref{sec:feathering},
so it may need a few more steps to find a turning point.
Due to the spacing of all turns, this will need
no more than \( \f \) steps under normal conditions. % Proof?
% On backward zig (with respect to the sweep direction), the program expects the zig addresses to
% grow continuously from \( -\Z \) to \( \Z \); the head does not change them.
% For example when the sweep direction is rightward, then on forward zig,
% the addresses will look like \( \dots, i-1,i,i+j,i+j+1,\dots \), for some \( 0<j\le\f \).
% Value when \( i+j \) will be changed to \( i+1 \). 

\begin{remarks}\label{rem:zigging-choices}
  \begin{enumerate}
  \item The need for backward zigging was explained in Example~\ref{xmp:zig}.
  \item The forward-zigging property and the frontier zone
    will be used in the proof of Lemma~\ref{lem:pass-clean}.
    We will need to show there that
    in the absence of new noise, after a constant number of passes, a clean interval \( J \)
    of size, say, \( \ge 6\Z\B \) will extend to the right until it reaches
    the end of a colony-pair or of a rebuilding area.
    Forward zigging will prevent any disorder from turning the head back prematurely.
  \item The choice of  \( \Z \) in~\eqref{eq:FDef} will be justified in the same proof; 
    it also (generously) lower-bounds the size of new noise capable of turning back the head.
  \end{enumerate}
\end{remarks}

\subsubsection{Feathering}\label{sec:feathering}

Example~\ref{xmp:feather} above indicated the need for the
following feature of the simulation program.

\begin{definition}[Feathering]\label{def:feathering}
A Turing machine execution is said to have the \( c \)-\df{feathering} property if the following holds.
If the head turned back at a position \( x \) at some time, then next time it comes within distance \( c \)
of \( x \), it can turn back only at least \( c \) steps beyond \( x \).
\end{definition}

(The name ``feathering'' refers to the picture of the path of the head in a space-time diagram.)
The following example suggests that any computation can be reorganized to accomodate feathering,
at the price of a logarithmic factor in time.

\begin{example}[1-feathering]\label{xmp:feathering}
Suppose that, arriving from the left at position 1, the head decides to turn left again.
In repeated instances, it can then turn back at the following sequence of positions:
\begin{align*}
 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, 5, 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, \dots
 \end{align*}
\end{example}

If in the original execution the head turned back \( t \) consecutive
times to the left from position \( p \), then now it will 
turn back from somewhere in a zone of size \( O(\log t) \) to the right of \( p \) in 
each of these times.
(Computing the exact turning point is not necessary.)
To remember a turn, a field
\begin{align*}
 \Turned,
\end{align*}
whose default value is 0, will be set to 1.
When a cell is passed without turning, we reset \( \Turned \) to 0.
Moreover, a ``big turn'' (see below)
will be allowed only in a distance at least \( \F \) from the just passed place of a previous
big turn.

\begin{definition}[Digression]\label{def:digression}
Whenever a turn is postponed since the head is not allowed to turn due to feathering,
the simulation to be carried out by the head is suspended until the head returns.
This is called a \df{digression}.
\end{definition}

\begin{definition}\label{def:PadLen}
Let 
\begin{align}\label{eq:PadLen}
  \PadLen &= 4\F\log\Q.   
\end{align}
For an interval \( I \) containing a colony pair we call the 
the \df{turn region} the set \( \Int(I,-\PadLen)\setminus C \), that is the close
outside neighborhood of \( I \).
\end{definition}



\begin{description}
\item[Small turn]
  Small turns will be done at zigging in during normal and healing mode (see later).
  Before turning, the head moves ahead at least \( \f \) steps, then moves more until it finds a
  place with \( \Turned=0 \).
  If one is not found within \( 2\f \) steps then call alarm but still don't turn.
  Such an event will be called \df{small turn starvation}.

\item[Big turn]
  Big turns will be done during booting, and the ends of normal and rebuilding sweeps.
% A big turn will always be attempted as soon as the head is outside the interior \( \Int(C,\PadLen) \)
% of the colony (as in Definition~\ref{def:colony-interior}) at the end of which it needs to happen.
  A big turn will be attempted at the colony end where it needs to happen.
Due to the organization below and the example above, in all ``normal'' circumstances,
the turn will succeed in the turn region outside, as in Definition~\ref{def:PadLen}.

Big turns are governed with the help of a pair of fields
called \( \FCount_{1},\FCount_{2} \) with default value 0.
Consider a big turn to the left (the other direction is analogous).
The execution of a big turn will be controlled by a segment of \( \Z/2 \) consecutive cells
called a \df{train}, marked by \( \FCount_{j}>0 \) for \( j=1,2 \).
When a big turn is initiated, a train is set up with \( \FCount_{j}= 1 \), \( j=1,2 \) in the frontier zone.
Then in consecutive zigging, it is moved to the right along with the frontier zone,
leaving behind cells with \( \FCount_{j}=0 \), and increasing its \( \FCount_{j} \) by 1.
When a value \( \FCount_{1}\ge\F \) is reached and a cell is encountered with \( \Turned=0 \),
the big left turn will be made.
The train with \( \FCount_{1}>0 \) that is left behind will be called the \df{footprint} of the big left turn.

If a segment of \( >3\beta \) cells with \( \FCount_{1}>0 \)
is encountered before the above conditions allow a turn,
then the train is reset to \( \FCount_{1}= 1 \) and continues forward (\( \FCount_{2} \) is not reset).
This way the footprint of a big left turn forces the next big left turn to be at least \( \F \) steps
further to the right.
If this resetting happens repeatedly then eventually 
\( \FCount_{2}=2\Q \) is reached; then start (or restart) rebulding,
set \( \FCount_{2}=1 \) but still don't turn: such an event will be called a \df{big turn starvation}.
\end{description}

\begin{definition}\label{def:safe-for-turns}
  A clean interval will be called \df{safe for turns} if every sequence of \( 2\f \) cells in it contains a cell with
  \( \Turned=0 \) and it has no sequence of more than \( 3\F\log\Q \) footprints of a big turn closer than \( 2\F \)
  cells to each other.
\end{definition}

An interval rewritten by noise can, of course, be unsafe for turns, even if it is clean.
But our machine \( M \) will have the property that when a fault-free
path passes over a clean interval then
the interval becomes safe for turns.
We give here only an informal argument; formal proof must wait until 
a complete definition of the simulation.
Zigs and the turns during healing are by the definition spaced by \( \ge\f \) cells apart, so
there will be at most 2 cells with \( \Turned=1 \) in any interval of size \( \f \)
contributed by zigging and at most 2 contributed by healing.
As for big turns, as indicated in Example~\ref{xmp:feathering} (for \( \F=1 \)),
a big turn attempt will be delayed by at most \( \F \) times the logarithm
of the total number of big turns inside a colony or a rebuild area.

The simulated Turing machine will also have the feathering property,
therefore the simulation will not turn back 
from one and the same colony repeatedly, without having passed it in the meantime.

\begin{sloppypar}
\begin{remark}\label{rem:big-turns}
  The size of the parameter \( \F \) is motivated by the proof of Lemma~\ref{lem:pass-clean}.
  Here is a sketch of the argument (it can be safely skipped now).
   At some time \( t_{0} \) in some interval \( I \)
  we will have clean subintervals \( J_{k}(t_{0}) \), \( k=1,2,\dots \)
  of size \( \ge 6\Z\B \) in which no fault will appear, and which are separated from each other by areas of
  size \( O(\passno^{2}\Z\B)\ll\F\B \).
  For times \( t>t_{0} \) we will track the maximal clean intervals \( J_{k}(t) \) containing
  the middle of \( J_{k}(t_{0}) \).

  Assume that the head passes over \( I \) noiselessly left to right and later
  also noiselessly from right to left.
  If the head moves in a zigging way to the
  right then the Attack Cleaning property will clean out the area between \( J_{i}(t) \) and \( J_{i+1}(t) \),
  joining them.
  This does not happen only in case of a programmed turn
  from the right end of \( J_{i}(t) \), in the course of the simulation or rebuilding.
  But then in the next pair of passes over \( I \), the feathering property implies that
  the programmed turn from the end of \( J_{i}(t) \) is at least a distance \( \F\B \) to the right.
  Our choice of \( \F \) implies that then \( J_{i}(t) \) will be joined to \( J_{i+1}(t) \).
  So two noise-free passes would join all the intervals \( J_{i}(t) \) into a clean area.  
\end{remark}  
\end{sloppypar}

\subsection{Simulation phase}\label{sec:simulation-phase}

The simulation phase, called the \( \Compute \) rule, computes new values for current cell-pair of the
simulated machine \( M^{*} \) represented by the current (base) colony-pair,
and the direction of the move of the head of  \( M^{*} \).
The cell state of \( M^{*} \) will be stored on the track \( \Info \) of the
representing colony.
The move direction of \( M^{*} \) 
will be written into the \( \Drift \) field of \emph{each} cell of the base colony-pair
(so the whole track must be filled with the same symbol \( d\in\{-1,1\} \).

The rule \( \Compute \) relies on some fixed \( (\beta,3) \) burst-error-correcting
code, moreover it expects each of the words found on the \( \Info \) track to be
2-compliant (Definition~\ref{def:err-code}).  % Probably \( (\beta,2) \) would be sufficient.
There is a rule \( \rul{ComplianceCheck} \) to check whether a word is \( 2 \)-compliant.

The rule \( \Compute \) essentially repeats 3 times % why was here 5?
the following \df{stages}: decoding, applying the transition, encoding.
Then it calls \( \rul{ComplianceCheck} \); if the latter fails
it will mark the colony for rebuilding.
It uses some additional tracks like \( \Work \) for most of the computation.
 The \( \Info \) track will not be modified before all the \( \Hold[j] \) tracks are written.

In more detail:
\begin{enumerate}
\item At the start, the current cell-pair is the left pair of cells of the left
  member of the base colony-pair.
  The \( \Sweep \) values of all cells are maximal.
  Everywhere outside the base colony-pair, the \( \Drift \) values are pointing
  towards it.

\item\label{i:turn-region}
  During the execution of this rule, the head sweeps the base colony-pair.
The big turns for changing the sweep direction will happen in the turn region outside as in
Definition~\ref{def:PadLen}.
If there is no bridge or colony adjacent to the end then outer bridge cells will be added as needed.
If stem cells are in the way they will be killed, if others then alarm will be called.
  
\item For \( j=1,\dots,3 \)       % Why was here 5 times instead of 3?
  % if \( \Addr \in \set{0, \dots, \Q-1} \)
  do 
  \begin{enumerate}
    
  \item Calling by \( \va \) the pair of strings found on the \( \Info \) track of
    % the interiors \( \Int(C,\PadLen) \) of
    the base colonies \( C \),
    decode it into the pair of strings \( \tilde\va=\upsilon^{*}(\va) \)
    (the current state of the simulated cell-pair), and
    store it on some auxiliary track in the base colony-pair.
    Do this by computing \( \tilde\va = \Decode(\va) \)
    on some simulated universal Turing machine.

    Each cell stores a track segment of the tape of the machine for \( \Decode \).
    When the head of \( M \) is on the cell with the simulated head of \( \Decode \)---let us
    call this the \df{head segment}---then
    one step of \( M \) simulates a number of  steps of \( \Decode \) as large as the size of this
    segment or until the simulated head leaves the segment (thus changing the head segment).

    For simplicity of analysis, each pair of sweeps
    changes only the head segment (and possibly the neighbor that becomes the new head segment).
    We accept this slowdown now.
    
  \item \label{i:comp.trans}
    Compute \( (\va',d)=\tau^{*}(\tilde\va,\alpha) \),
    where \( \alpha=1 \) if the pair of colonies is adjacent, else 0.
    Since the program of the transition function \( \tau^{*} \) is not written explicitly anywhere, 
    this ``self-simulation'' step is discussed in detail in Section~\ref{sec:self-simulation}.
    
  \item\label{i:comp.write}
    Write the encoded new cell states \( \upsilon_{*}(\va') \) onto the
    \( \Hold[j].\Info \) track of the interior of the base colony-pair.
    Write \( d \) into the \( \Hold[j].\Drift \) field of \emph{each cell} of
    the left base colony.

    If the new state \( a'_{i} \) is a vacant one for \( i=0 \) or 1, that is
    \( a'_{i}.\Kind^{*}=\Vacant^{*} \),
    then write \( 1 \) onto the \( \Hold[j].\Doomed \) track of the corresponding (left or right)
    base colony---else write 0.
    
  \end{enumerate}

\item
  % \item Repeat the following twice (hoping that at least
  %   one repetition will be burst-free):  % Why twice?
  Sweeping through the base colony-pair,
  at each cell compute the majority of \( \Hold[j].\Info \), \( j=1,\dots,3 \),
  and write it into the field \( \Info \).
  Proceed similarly, and simultaneously, with \( \Drift \).
  
\item For \( j=1,\dots,3 \), call \( \rul{ComplianceCheck} \) on the \( \Info \) track, and
  write the resulting bit into the \( \fld{Compliant}_{j} \) track.
  
  Then pass through the colony pair and turn each cell in which the majority 
  of \( \fld{Compliant}_{j} \), \( j=1,\dots,3 \) is false,
  into a stem cell (thus destroying the colony-pair if the result was false everywhere).
  We will see later that this will trigger healing and eventually rebuilding.  

\item\label{i:trickle-down} If the \( \Output \) field of the simulated cell is defined, write it
  into the output field of the left end-cell of each colony.
   
\end{enumerate}

The first sweep of the transfer phase (see Section~\ref{sec:transfer})
will use the information in the \( \Hold_{j}.\Doomed \) fields,
possibly destroying the colony.

Part~\ref{i:trickle-down} achieves that when the computation finishes on some
simulated machine \( M_{k} \),
its output value in cell 0 of \( M_{k} \) will ``trickle down'' to the output field of  cell 0 of \( M_{1} \),
as needed in Theorem~\ref{thm:main}.

As seen in this construction one can find a constant \( \CRedund \) with the property
\begin{align}\label{eq:redund}
    s_{k+1}\le\Q_{k}s_{k}\le \CRedund s_{k+1}
\end{align}
(see the definition of \( s_{k} \) in~\eqref{eq:Sigma_k}),
that is a colony of \( M_{k} \) represents a cell of \( M_{k+1} \) with redundancy
factor \( \CRedund \).
The number of steps \( \U_{k} \) taken by \( M_{k} \) needed for one work period
of simulation is \( O(\Q_{k}^{2}) \).

\begin{remark}
  There is a mechanism more economical on storage, without the full-width
  \( \Work \) and \( \Hold[j] \) tracks but with some
  added complexity, see Section~\ref{sec:redundancy}.
  This allows a space redundancy factor \( 1+\delta_{k} \) with \( \prod_{k}(1+\delta_{k})<\infty \),
  yielding a constant space redundancy factor for the whole hierarchy.
\end{remark}


\subsection{Self-simulation}\label{sec:self-simulation}

Let us elaborate step~\ref{i:comp.trans} of Section~\ref{sec:simulation-phase}.

\subsubsection{New primitives}

The simulation phase makes use of the track \( \Work \) mentioned above, and the track
\begin{align*}
   \Index
 \end{align*}
that can store a certain address of a colony.

Recall from Section~\ref{sec:language} that the program
of our machine is a list of nested
``\textbf{if} \emph{condition} \textbf{then} \emph{instruction}
\textbf{else} \emph{instruction}''
statements.
As such, it can be represented as a binary string 
 \begin{align*}
   R.
 \end{align*}
If one writes out all details of the construction of the present paper, this string \( R \)
becomes explicit, an absolute constant.
But in the reasoning below, we treat it as a parameter.

Let us provide a couple of \df{extra primitives} to the rules.
First, they have access to the parameter \( k \) of machine \( M=M_{k} \), 
to define the transition function
 \begin{align*}
            \tau_{R,k}(\va).
 \end{align*}
The other, more important, new primitive is a special instruction
 \begin{align*}
   \WriteProgramBit
 \end{align*}
in the rules.
When called, this instruction makes the assignment \( \Work\gets R(\Index) \).
This is the key to self-simulation: \emph{the program has
access to its own bits}.
If \( \Index=i \) then it writes \( R(i) \) onto the current position of the \( \Work \) track.


\subsubsection{Simulating the rules}

The structure of all rules is simple enough that they can be read and
interpreted by a Turing machine in reasonable time:

\begin{theorem}
There is a Turing machine \( \Interpr \) with the property that for
all positive integers \( k \), string \( R \) that is a
sequence of rules, and a pair of bit strings \( \va=(a_{0},a_{1}) \) with \( a_{j}\in\Sigma_{k} \),
 \begin{align*}
  \Interpr(R,0^{k},\va)=\tau_{R,k}(\va).
 \end{align*}
The computation on \( \Interpr \) takes time \( O(\abs{R}\cdot \abs{\va}) \).
\end{theorem}

The proof parses and implements the rules in the string \( R \); each of these rules
checks and writes a constant number of fields.

Implementing the \( \WriteProgramBit \) instruction is straightforward:
Machine \( \Interpr \) determines the number \( i \)
represented by the simulated \( \Index \) field, 
looks up \( R(i) \) in \( R \), and writes it into the simulated \( \Work \) field.

There is no circularity in these definitions:
  \begin{itemize}
  \item 
The instruction \( \WriteProgramBit \) is written \emph{literally}
in \( R \) in the appropriate place, as ``\(\WriteProgramBit \)''.
The string \( R \) is \emph{not part} of the rules (that is of itself).  
  \item On the other hand, the computation in
\( \Interpr(R,0^{k},\va) \) 
has \emph{explicit} access to the string \( R \) as one of the inputs.
  \end{itemize}

Let us show the computation step invoking the ``self-simulation'' in detail.
In the earlier outline, step~\ref{i:comp.trans} of Section~\ref{sec:simulation-phase}
said to compute \( \tau^{*}(\tilde\va) \)
(for the present discussion, we will just consider computing 
\( \tau^{*}(\va)=\tau_{k+1}(\va) \)), where \( \tau=\tau_{k} \),
and it is assumed that \( \va \) is available on an appropriate auxiliary track.
We give more detail now of how to implement this step:

\begin{enumerate}
\item Onto the \( \Work \) track, write the string \( R \).
To do this, for \( \Index \) running from 1 to \( \abs{R} \), 
execute the instruction \( \WriteProgramBit \) and move right.
Now, on the \( \Work  \) track, add \( 0^{k+1} \) and \( \va \).
String \( 0^{k+1} \) can be written since the parameter \( k \) is available.
String \( \va \) is available on the track where it is stored.
\begin{sloppypar}
 \item Simulate the machine \( \Interpr \) on track \( \Work \), computing
   \( \tau_{R,k+1}(\va) \).  
\end{sloppypar}
\end{enumerate}

This implements the forced self-simulation.
Note what we achieved:

\begin{itemize}
  \begin{sloppypar}
\item On level 1, the transition function \( \tau_{R,1}(\va) \) is defined completely
when the rule string \( R \) is given.
It has the forced simulation property by definition, and
string \( R \) is \emph{``hard-wired''} into it in the following way.
If \( (\va',d)=\tau_{R,1}(\va) \), then
\begin{align*}
  a'_{0}.\Work\gets R(a_{0}.\Index)
\end{align*}
whenever \( a_{0}.\Index \) represents a number between 1 and \( \abs{R} \),
and the values \( a_{0}.\Sweep \), \( a_{0}.\Addr \) satisfy the conditions
under which the instruction \( \WriteProgramBit \) is 
called in the rules (written in \( R \)).
      \end{sloppypar}

      \begin{sloppypar}
\item The forced simulation property of the \emph{simulated}
transition function \( \tau_{R,k+1}(\cdot) \) is 
achieved by the above defined computation 
step---which relies on the forced simulation property of \( \tau_{R,k}(\cdot) \).
              \end{sloppypar}
\end{itemize}

\begin{remark}
  This construction resembles the proof of Kleene's fixed-point theorem, and even more
  some self-reproducing programs (like a program \( p \) in the language C causing the computer
  to write out the string \( p \)).
\end{remark}

\subsection{Transfer phase}\label{sec:transfer}

Before the transfer phase, members of the base colony-pair \( C_{0},C_{1} \) have
cells of kind \( \Member_{0} \) and \( \Member_{1} \) correspondingly,
with a possible inner bridge between them.
Now the \( \Transfer \) rule takes over: control will be transferred to the
neighbor colony-pair in the direction of the simulated head movement which we called 
the \df{drift}.
During this phase of one or two pairs of sweeps, the range of the head
includes the base colony-pair and a neighbor colony
in the direction of the drift called \df{target colony}, including possible bridges between them.
At the beginning of the phase, the current cell-pair is the first cell-pair of \( C_{0} \).
Big turns happen in the turn region as in part~\ref{i:turn-region} of the
description of the \( \Compute \) rule.

Consider \( \Drift=-1 \).

\begin{enumerate}
\item In the first sweep, the head will travel to the end of \( C_{1} \).
  
\item \label{i:doomed}
  In the second sweep, it starts traveling back.
  In any cell, if the majority of \( \Hold[j].\Doomed \), \( j=1,\dots,3 \),
is 1 then turn the scanned cell into a stem cell: 
this is the way to destroy the colony representing a cell of \( M^{*} \) that
must be killed.
Otherwise the kind of the scanned colony cells of \( C_{1} \) will turn to \( \Outer \).

\item\label{i:left-edge}
  Continuing the sweep, the head turns  the possible inner bridge between the
  base colonies into an outer bridge, and turns
  the kind of elements of \( C_{0} \) to \( \Member_{1} \).
  We don't need to worry about ``doomed'' cells as above, since
  the program (and thus also the simulated program) will never kill the member in the cell pair
  that is (like here the one represented by colony \( C_{0} \)) in the direction of the drift.

  \item If there is an \emph{adjacent} left neighbor colony \( C_{-1} \) of \( C_{0} \),
    pass to its left edge, turning its cells to kind \(\Member_{0} \).
    Otherwise start an inner bridge towards the left, killing all possible bridge or stem cells in the way.
    If the left end of the bridge reaches an outer colony \( C_{-1} \)
    before \( Q \) bridge cells are created, then pass to its left edge,
    turning its cells to kind \( \Member_{0} \).

  \item\label{i:target}
    If \( Q \) bridge cells were created, stop at the left edge of this bridge (we call it now \( C_{-1} \)).

  \item\label{i:complete}
    Make another pair of sweeps, turning, if necessary, the bridge cells in \( C_{-1} \) to
    \( \Member_{0} \).
    This pair of sweeps returns at the right end of the new \( \Member_{1} \) colony
    (the old one may not even be there anymore, if it was killed).
  
  \end{enumerate}

  If \( \Drift=1 \) then only one pair of sweeps is needed:
  the first one performs the jobs of parts~\ref{i:doomed}-\ref{i:target},
with the corresponding changes (extending into a target colony \( C_{2} \)), and the second sweep
performs the task of~\ref{i:complete}.

  The continuous fault-checking during zigging will notice
when a small burst of faults compromises this process (for example when the end of a bridge would
``bite'' into another colony), and trigger healing (see later).

\subsection{Booting}\label{sec:booting}

Ideally, the work of machine \( M \) starts from a single active cell-pair of the Booting kind,
with addresses \( \Q-1 \) and 0, the middle cell-pair of a yet to be built colony-pair.
The \( \Rider \) track of the cell-pair holds a tape segment of the simulated Turing machine \( \G \),
along with the simulated head.
Such a cell-pair will be called a \df{booting pair}.
The segment consisting of this cell-pair will be extended left-right by booting cells,
eventually creating a colony-pair, as follows.

\begin{description}
  \item[Main work]
Machine \( M \) performs \( \Q \) times the following:
\begin{itemize}
\item[] 
  Simulate \( \G \) in the active pair on the \( \Rider \) track
  for at most as many steps as the size of the represented
  tape segment of \( \G \), possibly stopping earlier if the represented head would exit that
  segment earlier.
  Move the active pair of \( M \) left or right as needed,
  adding cells to the representing segment, and giving them the appropriate addresses of their colonies.
  All new cells encountered must be stem cells (blanks), and all the ones in the segment already created
  must be of the Booting kind; otherwise call alarm.

  In all this, use zigging and feathering.
  Zigging uses small turns just as during simulation, but 
  whenever a turn of \( \G \) is simulated, \emph{make a big turn}
  (as defined in Section~\ref{sec:feathering}), with the necessary
  digressions (see Definition~\ref{def:digression}).
  When the active pair does not need moving, treat this as (say) a left turn from the
  point of view of feathering, making the necessary marking and digression.
  The simulation is continued only after the head returns to the active cell-pair from the digression.
  
\end{itemize}
\item[Output]
If the simulated computation of \( \G \) terminates (the \( \Output \)
field of simulated cell 0 of \( \G \) is written)
then write the same value to the \( \Output \) field of cell 0.
\item[Lifting]
  If the \( \Q \) steps of \( M \) are completed without the termination of the simulated
  \( \G \) computation, then
  create the colony-pair around the original pair of booting cells (and turn its cells into member cells).
 \df{Lift} (copy) the \( \Rider \) track of its cells into
  the \( \Rider \) track of the cell-pair simulated by it.
  Set the mode of the simulated cell-pair to \( \Booting \).
\end{description}

The only error-control during all this is the step counting (the cells are big enough
to carry a counter up to \( \Q \)), zigging, and the call of alarm mentioned in the main work above.
This serves to notice when booting cells were introduced by a fault
into a non-booting situation.
We introduce no mechanism to correct faults during the booting phase, since
we do not expect faults
during it---see the probability analysis in Section~\ref{sec:fault-estimation}.


\section{Annotation}    \label{sec:annotation}

The definitions introduced here, characterizing trajectories in terms of their structural integrity,
are needed both for defining the error-correcting part of the
program and for its later analysis.

\subsection{Health}            \label{sec:health}

Information on the \( \Info \) track is protected by an error-correcting code.
However, faults can ruin the simulation structure and disrupt the simulation
(and the error-correcting process) itself.
Structure is maintained with the help of a small number of fields.
The required relations among them 
allow the identification and correction of local damage.

A tape configuration with local structural integrity will be called \df{healthy}.
Health is a necessary but not sufficient condition for the proper functioning of the simulation.
It is that part of the condition that can be checked locally, and repaired locally provided it
was damaged locally.

\begin{definition}[Homogenous domain]\label{def:domains}
The tuple of fields
\begin{align*}
   \Core =(\Kind,\Drift,\Addr,\Sweep)
 \end{align*}
is called the \df{core}.   
 An interval of non-stem adjacent neighbor cells is a \df{homogenous domain} if
 its core variables with the exception of \( \Addr \) have the same value, and
 \( \Addr \) increases left to right.
The \df{left end} of a domain is the left edge of its first cell, and its \df{right end} is 
the right edge of its last cell.
\end{definition}

\begin{remark}
The track \( \ZigAddr \) will also be used in healing, but only in a limited way: when the zigging domain
is extended over stem cells, then this track directs the head back to the front.
It is not needed for the definition of homogenous domains and boundaries.  
\end{remark}

\begin{definition}\label{def:extended-colony}
  An \df{extended colony} is a colony with possibly an adjacent bridge extending it.
\end{definition}

Defining health formally can be done mechanically on the basis of the informal descriptions given here,
but the detailed description would be tedious.
Recall Definition~\ref{def:front} of the front.

\begin{itemize}
\item   A healthy configuration consists of intervals of neighbor cells.
  Health will be defined for one of these intervals in a local way.
  As a non-local condition, we also require that at exactly one of these intervals contains the front, and
  between these intervals there is a gap of size \( >\Q\B \) with nothing but stem cells and
  bridge cells in it.

\item In the interval containing the front,
  there is a base colony-pair, with possibly an inner bridge going from one member of the pair
  to the other one.
  Let \( I \) denote the interval containing this pair.
  If the sweep inside this colony-pair shows that the \( \Compute \) rule is being performed then
  the only sweep boundary allowed inside is the front.
  Sweep boundaries outside \( I \) are outside, in the turn region as in Definition~\ref{def:PadLen}.

\item Outside the base colony-pair are possibly colonies of type \( \Outer \), connected to it and
  to each other by outer bridges, and with drift directed towards the base colony-pair.
  They are called \df{left outer} colonies and their (and the bridges') cells left outer cells, or
  \df{right outer} colonies and bridges depending on whether their drift is \( +1 \) or \( -1 \).
  Stem cells are also called outer cells (both left and right).

\item  During transfer, the base colony (of the pair)
  that is in the direction of the drift is possibly extended by an inner bridge towards the target colony.
  At this point there is no other inner bridge,
  and the front may be between two bridges: the new inner bridge and an old outer bridge.
  
  In the later part of this sweep, the new inner bridge already reaches the target colony.
  If the bridge extends to a full colony then this is converted to the appropriate kinds of member
  cells in the backward sweep.
  Domains ahead and behind the front show the changes done.
  
\end{itemize}

The following lemma shows that health is a locally checkable property.
 
\begin{lemma}
  If an interval of neighbor cells in a tape configuration
  has only the kinds of boundaries allowed by the above description then it is healthy.
\end{lemma}
\begin{proof}
  In what follows we don't repeat it but each statement is clearly forced by the kind of boundaries
  allowed.
  \begin{enumerate}
  \item
Let us list the various kinds of boundary, but note that several of them may coincide:
\begin{itemize}
\item The front.
\item Between two colonies.
\item Between a colony and a bridge.
\item A turn boundary.
\end{itemize}

\item Consider an interval \( I \) of neighbor cells.
  If  \( I \) contains cells that are not outer, or it contains both left and right outer cells
  then it also contains the front.

\item Assume that \( I \) contains only left outer cells.
Then these consist of colonies connected by bridges with possibly an incomplete
bridge at the left end, with drift pointing to the right.
Interval \( I \) must end on the right side also by a colony end and possibly a bridge.

The only boundaries are the colony boundaries, the
right turn boundaries in the turn regions to the left of the colonies and of \( I \),
and on a bridge on the right end of \( I \).

The situation is similar when \( I \) consists of right outer cells.

Assume now that \( I \) contains the front;
then it also contains an interval \( K \) consisting neighbor of non-outer cells.
  
\item Let \( s \) be the maximum sweep found in \( K \).
  If \( s \) is in the computing phase then \( K \) consists
  of a base colony-pair with a possible inner bridge connecting it.
  The only boundaries inside are a front and the colony boundaries.
  Turn boundaries are outside it.

\item Suppose now that \( s \) is in the transfer phase: then the 
  the drift over \( K \) is constant.
  Suppose that, for example, \( \Drift=-1 \).
  Look at the description of the transfer phase in Section~\ref{sec:transfer}.
 In the first sweep of the transfer phase, \( K \) must still look like in the computing phase.

\item   If  \( s \) is the second (leftward) sweep of the transfer phase
  then the front divides the interval \( K \) into two parts.
  The right part shows the alterations made by the sweep
  in parts~\ref{i:doomed}-\ref{i:target} of the description of the transfer phase,
  so it already shows part of a possible left-shifted base colony-pair.
  Except that in the very last step its left end may be an internal bridge of size \( \Q \),
  to be converted into \( \Member_{0} \) cells in the following, rightward sweep.
  The left part shows the outer areas and possibly part of he original base colony-pair before its conversion.

\item In the third, rightward sweep, both halves will show part of a shifted base-colony pair,
  and the only possible change is the conversion of a bridge to members mentioned above.

  \item The case with \( \Drift=1 \) is similar.
  \end{enumerate}
\end{proof}

% \begin{premark}
% Pictures!  
% \end{premark}

\begin{lemma}\label{lem:3-boundaries}
  In a healthy configuration, over any interval of size \( <\F\B \) there are at most three boundaries between
  domains.
\end{lemma}
\begin{proof}
  The big turns are more than \( \F\B \) away from each other and from the colony ends.
  So if the interval contains a big turn then the only other boundary it can contain is the front.

  Two colony-ends can be closer than \( \F\B \) to each other in case of a big gap between
  two neighbor colonies.
  These and possibly the front can amount to three boundaries.
\end{proof}

\begin{definition}[Boundaries]%\label{def:domains}
% \begin{description}
% \item[Outer domain]
% Outer cells that could belong to the same extended colony.

% \item[Workspace domain]  
%   Part of an extended base colony, or part of a former outer
%   colony that is in the process of being transferred to.
%   The latter is called a \df{target domain}, its colony the \df{target colony}.
%   The cells of the target domain are not outer cells since the their sweep shows the start of transfer.
%   It is not extended by a bridge.
% \end{description}
A boundary of a homogenous domain is called \df{rigid} if its address is the end 
address of a colony in the same direction.
A \df{boundary pair} 
is a right boundary followed by a left boundary at distance \( <\B \).
It is a \df{hole} if the distance is positive.
It is \df{rigid} if at least one of its elements is.
\end{definition}

The health of a configuration \( \xi \) of a generalized Turing machine 
\( M \) is defined over a certain interval \( A \).
It depends on \( \xi_{\tape}(A) \), further on whether \( \xi_{\pos} \) is in \( A \) and
if it is, where.
But we will mention the interval \( A \) explicitly only where it is necessary.
In particular, some of the structures described above may fall partly or fully
outside \( A \).
A tape configuration is called \df{healthy} on an interval \( A \)
when there is a head position (possibly outside \( A \) ) that turns it into a healthy configuration.
Health only depends on the \( \Core \) track
and the lack of heal or rebuild marks on the tape.
In a healthy configuration every cell's \( \Core \) field
determines the direction in which the front is found, from the point of
view of the cell.
A violation of the health requirements can sometimes be noted immediately:

% The definition below adds more detail to the above informal description of a healthy configurations.

% \begin{definition}[Healthy configuration]\label{def:healthy}
% We require that the mode be normal, and the following conditions hold, with \( \delta=\Drift \).

% \begin{description}
% \item[Segments]
%   All non-stem cells belong to full extended colonies.
%   In more detail:
%   \begin{itemize}
%   \item An \df{extended left/right base colony} consisting of cells of kind
%     \( \Member_{0}/\Member_{1} \), possibly extended by bridges.
%     In the first transfer sweep, cells of type \( \Member_{0} \) may be in the process of turning
%     into \( \Member_{1} \) or vice versa, according to the drift.
%   \item \df{Extended outer colonies}.
%   \item Desert filling out the gaps between the above parts.
%   \end{itemize}
%   The following kinds of domain can occur.
%   \begin{itemize}    
%   \item If \( \Sweep = \TransferSw(\delta) \) then the base colony-pair is being moved left or right
%     according to \( \delta \).
%     This involves a possibly a bridge to be extended, and also the change of \( \Member_{j} \) to
%     \( \Member_{1-j} \) as the transfer requires.
%   \item If \( \Sweep = \TransferSw(\delta) +1 \) and the transfer did not take place yet,
%     then the whole bridge is being converted into a new base colony
%     as the head is traveling in direction \( -\delta \).
%   \item In the last sweep, the new member cells obtain the \( \Adj' \) values are changed as needed.
%   \end{itemize}
%   These are the only possible domains to be seen in a healthy area.
        
%     \item [The front] 
%       The farthest position \( \front(\xi) \) to which the head has 
%       advanced before starting a new zig is called the \df{front}:
%       it can be computed from the fields \( \xi.\pos \) and \( \Depth \) of the current cell, 
%       but can also be reconstructed from the tape, namely from the \( \Sweep \) track.
%       It is always inside the extended base colony or the target.

%      % \item[Workspace]

%      %   \begin{sloppypar}
%      %      The \df{workspace} is an interval of non-outer cells, such that:         
%      %    \end{sloppypar}
        
%      %    \begin{itemize}
        
%      %    \item For \( \Sweep < \TransferSw(\delta) \), it is equal to the base colony.

%      %    \item In case of \( \Sweep = \TransferSw(\delta) \), it is the smallest interval including
%      %          the base colony and the cell neighboring to \( \front(\xi) \) on the side of the base colony.

%      %    \item If \( \TransferSw(\delta) < \Sweep < \Last(\delta) \),
%      %          then it is equal to the union of the extended base colony and the target.

%      %    \item When \( \Sweep = \Last(\delta) \), it is the smallest interval including the target
%      %          (future  base) colony and \( \front(\xi) \).
%      %    \end{itemize}

%    \end{description}
 % \end{definition}

\begin{definition}[Coordination] \label{def:coordinated}
   The current cell-pair is \df{coordinated} 
   if it is possible for them to belong to a healthy configuration.
\end{definition}

% Recall that in Rule~\ref{alg:main1},
In normal mode, if lack of coordination is discovered then the healing procedure is called.
The following lemmas show 
how local consistency checking will help achieve longer-range consistency.

\begin{lemma}\label{lem:coordination1}
  In a healthy configuration, each \( \Core \) 
  value along with \( \Z \) determines uniquely the 
  \( \Core \) value of the other cell of the current cell-pair, with the following exceptions.
  \begin{itemize}
  \item During the first transferring sweep, while
    creating a bridge, the front can be a stem cell or the first cell of an outer colony.
  \item Every jump backward from a target colony can land on the last cell of a 
    bridge (whose address is not recorded in the cell from which the head is jumping)
    or the last cell of the base colony.
  \end{itemize}
  \end{lemma}
  \begin{proof}[Proof sketch]
  To compute the values in question, use \( \ZigAddr \) to find the address at the front, and
  refer to the properties listed above.
%   \begin{premark}
% Elaborate!    
%   \end{premark}
  \end{proof}

\begin{lemma}[Health extension]\label{lem:health-extension}
  Let \( \xi \) be a \emph{tape} configuration that is healthy on intervals \( A_{1}, A_{2} \) 
where \( A_{1}\cap A_{2} \) contains a whole cell body of \( \xi \).
Then \( \xi \) is also healthy on \( A_{1}\cup A_{2} \).
\end{lemma}
\begin{proof}
  The statement follows easily from the definitions.
  % \begin{premark}
  %   Elaborate!
  % \end{premark}
\end{proof}

In a healthy configuration, the possibilities of finding non-adjacent neighbor
cells are limited.

\begin{lemma}\label{lem:two-domains}
  An interval of size \( <\Q \) over which the configuration \( \xi \) is healthy
contains at most two maximal sequences of adjacent non-stem neighbor cells.
\end{lemma}
\begin{proof}
By definition a healthy configuration consists of full extended colonies, with 
possibly stem cells between them.
An interval of size \( <\Q \) contains sequences of adjacent cells 
from at most two such extended colonies.
\end{proof}

% Let us classify the boundary pairs possible in a healthy configuration.
% Rigid pairs:

% \begin{enumerate}[label=\upshape{(r\arabic*)}, ref=r\arabic*]
% \item\label{i:rigid.outer-workspace}
%   Between an outer extended colony or a desert, and a colony closer to the base.
% \item\label{i:rigid.bridge-target} Between the extended base colony 
% and the target or an outer colony.
% \end{enumerate}

% Non-rigid pairs are at the front:
% \begin{enumerate}[label=(nr\arabic*), ref=nr\arabic*]

%   \item\label{i:nr.bridge-bridge} End of a new bridge in direction \( \delta \)
% not within distance \( <\B \) of a rigid boundary of a colony of drift \( -\delta \) or its own target.
% On the other side is either desert or the end of an old bridge.

%   \item\label{i:nr.aligned} Between aligned domains:
%     \begin{enumerate}[label=(\arabic*), ref=nr\arabic{enumi}.\arabic*]
%      \item\label{i:nr.aligned.differ-1} Between sweep values differing by 1,
%      \item\label{i:nr.aligned.bridge-target} between a new bridge and the target it is being converted into,
%      \item\label{i:nr.aligned.target-member} 
% between a target in direction \( \delta \) and the remaining domain of member cells, of drift \( -\delta \),
%      \item\label{i:nr.aligned.member-target}  between a target and the member cells replacing it in the last sweep,
%      \item\label{i:nr.aligned.internal-end} 
% between an internal domain and an end domain (see Definition~\ref{def:domains}).
%     \end{enumerate}

% \end{enumerate}

% The following is worth noting.

% \begin{lemma}
% In a healthy configuration, any interval of size \( <\Q\B \) contains at most 3 boundary pairs,
% only one of which can be  a hole.
% \end{lemma}
% \begin{proof}
% Any interval of size \( <\Q\B \) contains at most one rigid left boundary and
% one rigid right boundary.
% Any nonrigid boundary coincides with the front.
% A hole can exist only at the end of a bridge.
% If there are two bridges, then the hole can only be between their ends.
% \end{proof}

\begin{lemma}\label{lem:infer-between}
In a healthy configuration, 
a cell's state shows whether it is an outer cell, and also its direction towards the front.
The \( \Core \) track of a homogenous domain can be reconstructed from any of its cells.
\end{lemma}
\begin{proof}
Whether a cell is outer is computed from some fields.
If the cell is outer then \( \Drift \) shows its direction from the front,
else the parity of \( \Sweep \) shows it.
For booting cells, the number of simulation steps performed increases towards the front.
\end{proof}

Though we did not spell out formally all details of health, let us define a strengthening of this property.

\begin{definition}[Super-health]\label{def:super-health}
  Let \( \xi  \) be a healthy configuration on an interval \( I \) that is also safe for turns
  (as in Section~\ref{sec:feathering}).
  We say that \( \xi \) is \df{super-healthy} if it ends in colonies on both sides and
  in each colony, whenever the head is not in the last sweep, the \( \Info \)
  track contains a valid codeword as defined in Section~\ref{sec:coding}.
  A tape configuration is \df{super-healthy} on \( I \)
  if it can be extended (by indicating the head position) to a super-healthy configuration on it.
\end{definition}



\subsection{Annotation, scale-up}\label{sec:annotation,scale-up}

Let us first define the notion of ``almost healthy'' (admissible) for trajectories.
Informally, an admissible configuration may differ from a healthy one in a small number
of intervals we will call ``islands''.
It may also differ from a super-healthy one in somewhat larger intervals we will call ``stains''.
Even a healthy configuration may contain stains: places in which the \( \Info \) track differs from
a codeword.
Stains pose no obstacle to the simulation, and if they are small and few then
will be eliminated by it, via the error-correcting code.

\begin{definition}[Local configuration, replacement]
\label{def:local-config}
  % A \df{local configuration on} a (finite or infinite)
  % interval \( I \) is given by values assigned to the cells
  % of \( I \), along with the following information: whether
  % the head is to the left of, to the right of or inside
  % \( I \), and if it is inside, on which cell.
  % If \( I' \) is a subinterval of \( I \), then a local configuration
  % \( \xi \) on \( I \) clearly gives rise to a local configuration
  % \( \xi(I') \) on \( I' \) as well, called its \df{subconfiguration}.
  % We can similarly talk about local tape configurations.
  Let \( \xi \) be a tape configuration and \( \zeta(I) \) a local
  tape  configuration on interval \( I \).
  Then the tape configuration \( \xi\vert\zeta(I) \) is obtained by
  replacing \( \xi \) with \( \zeta \) over the interval \( I \).
\end{definition}

\begin{definition}[Annotation]\label{def:annotation}
  Consider a region \( R \) of space-time where for each time \( t \) the set of space-time points
  belonging to \( R \) is an interval \( J(t) \).  
  For constants
\begin{align}\label{eq:stain}
   \cns{island}, \cns{stain} % = (\f+2)\beta;
\end{align}
to be specified later, % Where?
an \df{annotated trajectory} over \( R \) is a tuple
  \begin{align*}
    (\eta,\cI(\cdot), \cS(\cdot)),
  \end{align*}
  where \( \eta \) is a trajectory over \( R \), further for each time \( t \) 
  \begin{varenum}[series=annotated]{a}

  \item  \( \cI(t) \) is a set of intervals called \df{islands},
    each of size at most \( \cns{island}\beta\B \),
    where \( \eta(\cdot,t) \) can be changed into the healthy
    configuration by changing it only in the islands.
    The disorder of \( J(t) \) in \( \eta(\cdot,t) \)
    is covered by intervals of size \( \le (\beta+2)\B \) one inside each island.
    
  \item \( \cS(t) \) is a set of intervals called \df{stains}, each of size \( \le \cns{stain}\B \).
    Each island is contained in a stain, and
    \( \eta(\cdot,t) \) can be changed into a super-healthy
    configuration by changing it only in the stains.

  \item For any extended colony, at a time separated by \( 24\E\beta\Tu \) from any faults,
    the set of stains consists of some stains
    outside its interior (as in Definition~\ref{def:interior}) and
    separated from each other by at least \( \Z\B/3 \), and in addition one more stain anywhere.

    % Suppose that there is no burst in \( \eta \) in the time interval \( \rint{t-\Tu^{*}}{t} \).
    % Then \( |S(t)|\le 2 \).
    % If the head is approaching 2 stains that are at a distance \( <\Q\B \) of each other
    % then it is approaching through a colony representing a cell with \( \Turned=1 \).
  \end{varenum}
  A trajectory is \df{admissible} over \( R \) if it allows an annotation there.
  Its \df{base colony-pair} at any time \( t \) is that of a healthy configuration into which it can
  be changed by changing the islands (its position is uniquely determined).
The head is \df{free} when it is not in any island,
the observed cell-pair is in normal mode, and is coordinated.

A configuration \( \xi \) is admissible if the one-time-step trajectory
consisting of just \( \xi \) is admissible.
\end{definition}
When considering a single time \( t \)
we can refer to \( \cI(t) \) as \( \cI \) and \( \xi(t)=\eta(\cdot,t) \) as \( \xi \),
and we can talk about the annotation of \( \xi \).
A configuration may allow several possible annotations;
however, the codewords recoverable from it do not depend on the choice of the
annotation.

Formally, the proof of error-correction will happen by proving that
annotation can be extended forward in time; however, we will retain an informal language
whenever it is clear how to translate it to annotation.
Let us argue (informally, without replacing any later, formal argument), that local correction
does not have to deal with more than three islands in any area of size \( \Q\B \). 

\begin{example}[Three islands]\label{xmp:3-islands}
  Suppose that the head has arrived at some colony \( C \) from the left,
goes through a work period and then passes to the right.
In this case, if no new noise occurs then we expect that
all islands found in \( C \) will be eliminated by the healing procedure.
On the other hand, a new island \( I_{1} \) can be deposited (in the last pass).

Consider the next time (possibly much later), when the head arrives (from the right).
If it later continues to the left, then the situation is similar to the above.
Island \( I_{1} \) will be eliminated, but a new one may be deposited.
But what if the head turns back right at the end of the work period?
If \( I_{1} \) is close to the left end of \( C \), then (due to the feathering
construction) the head may never reach it to eliminate it; moreover,
it may add a new island \( I_{2} \) on the right of \( I_{1} \).
% (In the work period where the head deposits island \( I_{2} \) near the left colony
% end, it may repeatedly dip into \( I_{2} \) at the descending end of a zig at a
% right turn.
% During this dip it can expand the islands
% \( I_{1}, I_{2} \) and then emerge on the right, with the healing unfinished.)

When the head returns a third time (possibly much later), 
from the right, feathering the level of the simulated machine will
cause it to leave on the left.
Islands \( I_{1},I_{2} \) will be eliminated but a new island
\( I_{3} \) may be created by a new burst of faults before, after or during the elimination.
So the healing procedure must count with possibly three islands
possibly in close vicinity to each other..
\end{example}

Let us now define formally the codes \( \varphi_{*k},\Phi_{k}^{*} \) needed
for the simulation of history \( (\eta^{k+1},\Noise^{(k+1)}) \) by history \( (\eta^{k},\Noise^{(k)}) \).
Omitting the index \( k \) we will write \( \varphi_{*},\Phi^{*} \).
To compute the configuration encoding \( \varphi_{*} \) we proceed first as
done in Section~\ref{sec:hier-codes}, using the code \( \psi_{*} \) there,
and then initialize the kind, sweep, drift and address fields appropriately.
The value \( \Noise^{*} \) is obtained by a residue operation
as in Definition~\ref{def:sparsity}; it remains to define \( \eta^{*} \).
In the parts of the history that can be locally annotated, and which we will call \df{clean}.
if no colony has its starting point at \( x \) at time \( t \), set \( \eta^{*}(x,t)=\Vacant \).
Otherwise \( \eta^{*}(x,t) \) will be decoded from
the \( \Info \) track of this colony, at the beginning of its work period 
containing time \( t \).
More precisely:

\begin{definition}[Scale-up]\label{def:scale-up}
Let \( (\eta,\Noise) \) be a history of \( M \).
We define \( (\eta^{*},\Noise^{*}) \) \( =\Phi^{*}(\eta,\Noise) \) as follows.
Consider position \( x \) at time \( t \), and \( J=\rint{t-\Tus}{t} \).
If \( x \) is not contained in some interval \( I \) such that \( \eta \) has an annotation
over \( I\times J \) then \( \eta^{*}(x,t)=\Bad^{*} \).
Assume now that it is contained, and let
\( \chi(\cdot,u) \) be some admissible history satisfying \( \eta \) over \( I\times J \).
If \( x \) is not the start of some colony \( C \) in \( \chi \)
then let \( \eta^{*}(x,t)=\Vacant \); assume now that it is.
Then let \( t'\in J \) be the starting time in \( \chi \) of the work period of \( C \)
containing \( t \), and let \( \eta^{*}(x,t) \) be the value decoded from \( \eta(C,t') \).
In more detail, as said at the end of Section~\ref{sec:coding}, we apply the decoding
\( \psi^{*} \) to the interior of \( C \) to obtain \( \eta(x,t) \).
\end{definition}

This definition decodes admissible intervals and trajectories for \( \eta \) into 
histories of \( \eta^{*} \).
(We don't know yet whether they are trajectories of \( \eta^{*} \).)


% Recall the definition of a clean hole in Definition~\ref{def:clean-hole}.

% \begin{lemma}\label{lem:rebuild-clean}
%   Let  \( I \) be clean interval  of size \( 5\Q\B \).
%   Consider a noise-free time interval \( J \) during which the head spends time \( \ge 5\Tu \) in \( I \)
%   or passes through \( I \).
%   Then during \( J \) at some time the head will be found in a hole intersecting \( I \) that is
%   clean for \( M^{*} \).
% \end{lemma}
% \begin{proof}
% Assume, without loss of generality, that the head passes \( I \) from left to right.
% In the absence of noise, and inside a clean area,
% the healing procedure can only fail if it starts a rebuilding process.
% And a rebuilding process never fails.

% If the head passes \( I \) in normal mode, then
% the simulation makes \( I \) clean for \( M^{*} \).
% It may encounter a colony that is only healthy, not super-healthy;
% however, then the simulation will turn it into a healthy one.
% If healing is invoked but no rebuilding then we can follow the proof of Lemma~\ref{lem:combined-heals}:
% the result of each successful healing is consistent with the the continuation of a simulation.

% If a rebuilding is invoked whose whole range falls within \( I \) then it succeeds.
% Then simulation continues, with possible healings thrown in, until new rebulding starts.
% Lemma~\ref{lem:rebuild-health} shows that rebuilding will only extend the super-admissible area.  
% \end{proof}
% \Pnote{Coordinate these two lemmas, check and elaborate.}

\section{Healing and rebuilding}\label{sec:healing}

Here we define the part of the simulation program that
corrects configurations of machine \( M \) that are ``almost'' healthy.

\subsection{Stitching}\label{sec:stitching}

We will show that a configuration admissible over an interval of size \( >\Q\B/2 \)
can be locally corrected;
moreover, in case the configuration is clean, this correction
can be carried out by the machine \( M \) itself.

\begin{definition}[Substantial domains]\label{def:substantial}
Let \( \xi(A) \) be a tape configuration over an interval \( A \).
A homogenous domain of size at least \( 7\cns{island}\beta\B \)
will be called \df{substantial}.
The area between two neighboring maximal
substantial domains or between an end of \( A \) and the closest substantial domain in \( A \)
will be called \df{ambiguous}.
It is \df{terminal} if it contains an end of \( A \).
  Let
 \begin{align*}
     \Delta &= 39\cns{island}\beta, % ,\quad \Delta'=\Delta+3\cns{island}\beta. %?
\\  \E  &= 6\Delta. %?
 \end{align*}
\end{definition}

\begin{lemma}\label{lem:ambiguous}
In an admissible configuration, each half of a 
a substantial domain contains at least one cell outside the islands.
If an interval of size \( \le  \Q\B \) of a tape configuration \( \xi \) differs from a  healthy tape 
configuration \( \chi \) in at most three islands, then 
the size of each ambiguous area is at most \( \Delta\B \).
\end{lemma}
\begin{proof}
The first statement is immediate from the definition of substantial domains.
By Lemma~\ref{lem:3-boundaries},
there are at most 3 boundary pairs in \( \chi \) at a total size of \( 3\B \).
Also, there are at most 3 islands of size \( \le\cns{island}\beta\B \).
There are at most 5 non-substantial domains of sizes \( < 7\cns{island}\beta\B \)
between these: this adds up to
\begin{align*}
 < (3+3\cdot\cns{island}\beta+ 5\cdot 7\cdot\cns{island}\beta)\B<39\cdot\cns{island}\beta\B=\Delta\B.
 \end{align*}
\end{proof}

The following lemma forms the basis of the healing algorithm.

\begin{lemma}[Stitching]\label{lem:stitching}
In an admissible configuration, inside a clean interval,
let \( U,W \) be two substantial domains separated by an ambiguous area \( V \).
It is possible to change the tape on \( U,V,W \) using only information in \( U,W \) in such a 
way that the tape configuration over \( U\cup V\cup W \) becomes healthy.
Moreover, it is possible for a Turing machine to do so gradually, extending,
and changing the tape in \( U \) or \( W \) or enlarging \( U \) or \( W \)
gradually at the expense of \( V \).

The machine in question can make its turns via
``small turns'' as defined in Section~\ref{sec:sweep}.
\end{lemma}
\begin{Proof}
We distinguish several cases, based on the kind of domains involved.
At any step, if we find that \( U\cup V\cup W \) is healthy then we stop.
\begin{step+}{step:stitching.mergeable}
Assume that \( U,W \) both belong to the same extended colony: either an extended base colony, or 
an outer extended colony, or the target.
\end{step+}
\begin{prooof}
  % If both belong to the interior (as in Definition~\ref{def:interior})
  % of the area indicated, then the merging is simple.
In this case, by Lemma~\ref{lem:infer-between}, the content of the \( \Core \) track 
of \( V \) in the healthy configuration is completely determined by that of \( U,W \),
So the intervals \( U \) and \( W \), which will be recognized as the substantial ones as
opposed to \( V \), can be gradually extended towards each other in any order, 
overriding \( V \).
% Suppose that they do not belong to the interior and are, for example, towards the right from it.
% In this case, the \( \Sweep \) value may be decreasing in both \( U \) and \( W \) to the right.
% Peter: \( \Sweep \) is now constant within a domain.
% The big turns happen far enough from each other that one can require this.
% But since \( U,W \) themselves may overlap with islands, it may happen that the \( \Sweep \) value
% on the right end of \( U \) is smaller than the \( \Sweep \) value on the left end of \( W \).
% In this case, before extending \( U,W \) towards each other, the \( \Sweep \) values in both
% may need to be changed.
% By Lemma~\ref{lem:ambiguous}, the left and right halves of \( U \) contains a cell \( u_{1},u_{3} \) 
% each, not belonging to any island, and similarly with \( w_{1},w_{3} \) in \( W \).
% Let \( u_{2} \) be leftmost cell of the right half of \( U \) and \( w_{2} \) the rightmost cell of the left 
% half of \( W \).
% Then
% \begin{align*}
%  \Sweep(u_{1})\ge \Sweep(u_{2})\ge \Sweep(u_{3})\ge \Sweep(w_{1})\ge \Sweep(w_{2})\ge \Sweep(w_{3}).   
%  \end{align*}
% So the transformation will change \( \Sweep \) to \( \Sweep(u_{2}) \) everywhere in the right half of \( U \),
% and to \( \Sweep(w_{2}) \) everywhere in the left half of \( W \).
% After this, it will extend \( U \) towards \( W \), overtaking \( V \) and keeping \( \Sweep \) constant.
If \( U  \) and \( W \) belonged to different sides of the front, then the
new front will be the new boundary between \( U \) and \( W \).
\end{prooof} % step:stitching.mergeable

\begin{step+}{step:stitching.not-front}
Suppose now that \( U,W \) do not belong to the same extended colony, but they are on 
the same side of the front: without loss of generality, to the left of it.
\end{step+}
\begin{prooof}
In this case, only \( U \) can be outer or a target: suppose that it is outer.
Now \( W \) is either in a target or in the base colony.
If it is in the base colony then the base colony
cannot have an extension to the left, because given that the front
is to the right, the same sweep that created the extension would have created already a target, 
separating \( U \) (which being outer is not in the target) from \( W \) too much.
So whether \( U \) is in the base colony or not,
\( W \) is close to its colony's left end, which is in \( V \).
It must be extended to this left end.
Following this, \( U \) must be extended until its reaches \( W \).
The \( \Sweep \) values can be propagated without change, there is no need to coordinate them
between \( U \) and \( W \).

Suppose that \( U \) is in a target, then \( W \) belongs to the extended base colony.
Then \( U \) must be extended until its end-cell, and then \( W \) must be extended to meet \( U \).
Since both \( U \) and \( W \) are in the interior, the \( \Sweep \) values must be equal, so they
can be extended without change.
\end{prooof} % step:stitching.not-front

\begin{step+}{step:stitching.front}
Suppose that \( U \) and \( W \) belong to different extended colonies, with the front between them.
\end{step+}
\begin{prooof}
The \( \Sweep \) values can be extended without change, there is no need to coordinate them
between \( U \) and \( W \).

Suppose that \( U \) contains no bridge cells: in this case extend it to the right until it hits
a colony end-cell.
If you overlap \( W \), decrease \( W \) accordingly.
Due to admissibility, only bridge cells of \( W \) can be overwritten in this way
(colonies don't overlap in a healthy configuration).
Similarly, if \( W \) contains no bridge cells then extend it to the left, until it hits a colony end-cell.
Again, only bridge cells of \( U \) can be overwritten in this way.

Only one of \( U \) and \( W \) can be in an outer extended colony, suppose that \( U \) is.
If \( W \) is a target then we already extended it to its limit.
Now extend the bridge of \( U \) to meet it.
If \( W \) is in an extended base colony then extend its bridge until either meets \( U \) or
reaches full length.
In the latter case, extend a bridge of \( U \) to meet \( W \).

If \( U \) is in a target then it is already at its right end: we extend a bridge of \( W \) to meet it.
\end{prooof} % step:stitching.front
\end{Proof}


\subsection{The healing procedure}\label{sec:healing-proc}

The healing and rebuilding procedures look as if we assumed no noise or disorder.
The rules described here, however (as will be proved later), will clean an area locally under the 
appropriate conditions, and will also work under appropriately moderate noise.

Structure repair has two procedures.
The first one, called \df{healing}, performs
only local repairs of the structure: for a given (locally) admissible configuration,
it will attempt to compute a satisfying (locally) healthy configuration.
If it \df{fails}---having encountered an inadmissible configuration---then
the \df{rebuilding} procedure is called, which is designed to repair a larger interval.
To protect from noise, any one call of the healing procedure will change only 
a small part of the tape, essentially one cell: so a noise burst during healing
will have limited impact.

Recall the parameters \( \Delta,\E \) introduced in Definition~\ref{def:substantial}.
Suppose that \( \rHeal \) is called at some position \( \z \).
Then it sets
\begin{align*}
\Mode\gets\Healing,\; \Heal.\Sweep \gets 1,\; \Heal.\Addr \gets 0
\end{align*}
in the current cell-pair.
In accordance with the property of minimal change by \( \Heal \),
these fields will only be used and updated in the current cell-pair; their values are not read from
new cells onto which the head moves (in the traditional Turing machine model they would belong to the
internal state).

\begin{varenum}{h}
\item
We start by surveying an interval \( \R \) of at least \( 2\E \) cells 
around the starting point \( z \).
(It will go a little farther than \( \E \) cells in each direction,
in search of an allowed turning point, due to feathering.
If one is not found in \( 2\beta \), it restarts.)
Whenever the head steps on a stem cell or creates a new cell, 
\( \Drift \) is set to point to \( \z \) (to make sure that the head does not get
lost in a homogenous domain of stem cells).

\item\label{i:heal.germ}
During healing, we accumulate an interval of cells marked for rebuilding called a \df{germ}.
The marking is done on a track different from \( \Core \) and therefore not influencing consistency.
If the procedure finds that the germ reaches size  \( 4\beta\B > 3\beta\B \), then it calls rebuilding.
Also  if  healing already finds its center at the front of a stretch of 
at least \( 4\beta\B \) of the frontier zone of rebuilding---see later---then
it just passes control to rebuilding, which will continue its expansion.
If it finds itself between two opposing such fronts then the rightward rebuilding
gets precedence.

\item
At the end of every call of healing, as long as there are inconsistencies to correct, 
a cell is added to this interval (increasing its size even if healing
previously deleted one of the cells).
On the other hand, if no more inconsistency is found then the germ will be decreased by one cell
(erasing one of the marks at its end).
This way, if healing succeeds in fewer than  \( 4\beta \) steps then it eventually erases the germ;
otherwise it starts rebuilding.

\item 
Suppose that the survey finds a pair of substantial domains separated by an ambiguous area,
at least \( \Delta \) removed from the complement of \( R \). 
Choose the ambiguous area closest to \( z \).
Perform the first needed ``stitching'' operation (a single step) as defined in Lemma~\ref{lem:stitching}.
If the ambiguous area still remains, go to its cell closest to \( z \) and restart healing.
Starting from a clean admissible configuration, this operation creates a new admissible
configuration that is one step closer to being healthy.

If the required stitching operation is not found or fails---as
can happen for example when the ambiguous area's size decreases to \( <\B \) and turns out not to be
a legitimate boundary---then healing fails (just adding to the germ).

\item
Suppose that the whole surveyed area is found healthy.
Then if the head is found to be in the frontier zone (as defined in Section~\ref{sec:zigging}),
then return to normal mode, else move it one step towards the front and restart healing.
\end{varenum}

The healing procedure runs in \( O(\beta) \) 
steps, whereas rebuilding needs \( O(\Q^{2}) \) steps.

\begin{lemma}[Clean healing]\label{lem:combined-heals}
  Assume that the head moves in a noise-free and clean space-time rectangle
  \( \lint{a}{b}\times \lint{u}{v} \), with \( b-a>6\E\B \), \( v-u>24\E\beta\Tu \),
touching every cell of \( \lint{a}{b} \) at least once, and never in rebuilding mode.
Then before time \( v \), the area \( \lint{a+6\beta\B}{b-6\beta\B} \) becomes healthy.
\end{lemma}
\begin{proof}
  If healing was not called then after the head touched every cell of \( \lint{a}{b} \) the
  health of the area is proved.
  In healing, the head will not move away from an ambiguous area before eliminating it,
  creating a healthy interval
  \( I \) containing the two substantial intervals that originally bordered it.
  If the head leaves this interval in normal mode and the zigging does not start new healing then the
  healthy interval covered by zigging can be added to \( I \) to form an even larger healthy interval,
  and this continues until new healing starts.
  One of the substantial intervals of the new healing will be inside \( I \), and when it is completed,
  \( I \) will be extended further.
  The head may later return into the healthy interval \( I \),
  but it will then just continue the simulation without
  affecting health while staying inside.

  The margins of size \( 6\beta\B \) upper-bound the areas that healing surveyed but did not
  correct since subsequent calls moved its the center away.
  The time \( 48\E\beta\Tu \) allows \( 4\beta \) pairs of passes over an area of size \( 6\E \) cells.
\end{proof}

\subsection{Rebuilding}\label{sec:rebuilding}




This bridge extension can finish in two ways:
\begin{itemize}
\item It stops at the boundary of another colony on the left
  (which consists of cells of type \( \Member_{0} \)).
  (This can happen in the very first step, in which case no bridge is built.)
  Then this colony becomes the new left base colony.
  
\item The bridge reaches the size of \( \Q \) cells.
  In this case, the bridge will be converted into the new left base colony, the kind of its cells
  becoming \( \Member_{0} \).
\end{itemize}




Rebuilding starts by extending a germ (created by healing) to the right, in a zigging way,
expecting rebuild-marked cells on the backward zig.
This notices if rebuilding started from a burst of faults smaller than a germ
in a healthy area, and calls alarm.
Just as with healing, we will not mention disorder (since the program does not see it)---but
the analysis will take disorder into account.

The notion of front, of Definition~\ref{def:front} will be extended also to the rebuilding mode.
Here is an outline, for rebuilding that started from a cell \( z \) in the middle
of a germ.
The goal is that
\begin{varenum}{r}
\item\label{i:rebuild-size} we end up with a new decodable area
  extending at least one colony to the left and one colony to the right of \( z \).
\item\label{i:keep-healthy} the process does not destroy any healthy colony
  (possibly just created by a recent incarnation of rebuilding).
\end{varenum}

The rebuilding operation uses its own addresses, but in a special way, to avoid being
confused by the occasional deletion or insertion of cells.
It uses two address tracks: 
\( \Rebuild.\Addr_{-1} \) counts from the left end of the rebuilding area towards the head,
and \( \Rebuild.\Addr_{1} \) from the right end.

% The healing procedure can start also during rebuilding.

Here are the stages.
Recall the stitching operation from Section~\ref{sec:stitching}.
\begin{description}
 \item[Mark] Starting from the germ, extend a rebuilding area  over \( 3\Q \) cells to the left
and \( 3\Q \) cells to the right from the center \( z \).
Besides the address tracks, use also a track \( \Rebuild.\Sweep \).
Every step that changes the configuration must be accompanied by zigging, to check that
the rebuilding is indeed going on.

Rebuilding cannot assume anything about the content of the area encountered: in particular,
it may represent the traces of some other rebuilding.
Since a single burst of faults of size \( \beta \) (which cannot be completely excluded) could pass
control to this other rebuilding process, this possibility must be handled.

There is only one way that the rebuilding process can fail in a clean area: big turn-starvation,
as defined in Section~\ref{sec:feathering}.

 \item[Survey and Create]
More details of this stage will be given below.
It looks for existing colonies (possibly needing minor repair) in the rebuilding area, and 
possibly creates some.
As a result, we will have one colony called \( C_{\Left} \) 
on the left of \( z \), one called \( C_{\Right} \) on the right of \( z \),
and possibly some colonies between them.
Make all newly created colonies represent stem cells.
Direct all the other colonies with drifts and bridges towards \( C_{\Left} \).
(The creation of a bridge may result also in the creation of a new 
colony if the bridge becomes \( \Q \) cells long.)
% Survey and possibly change additional \( 0.5\Q \) cells on the left of \( C_{\Left} \) and \( 0.5\Q \) cells
% on the right of \( C_{\Right} \), making the whole interval healthy.
The interval covering \( C_{\Left} \) and \( C_{\Right} \)
will be called the \df{output interval} of rebuilding.
The pair of neighbor colonies with \( C_{\Left} \) on its left will be made the current colony-pair.

\item[Mop] Remove the rebuild marks, shrinking the rebuilding area onto the right end of \( C_{\Left} \).
\end{description}

Marked cells from some interrupted rebuilding may remain even after the mop-up operation.
These may trigger new healing-rebuilding later.

\begin{remarks}\label{rem:rebuild-precedence} 
  \begin{enumerate}
%   \item Why give precedence to the right-directed marking process?
% The issue will come up in the proof of Lemma~\ref{lem:pass-clean}.
% There, we will deal with a clean interval \( J \), initially of size, say,
% \( 6\Z\B \), and the goal is to see that
% in the absence of new noise, after a constant number of passes, \( J \) will extend to the right until it reaches
% the end of a colony pair or of a rebuilding area.
% If marking is unstoppable in both directions then this may not happen: reaching the left end of  \( J \)
% a new rebuilding process may send the head back to the right end, from which a new rebuilding process can send it back
% to the left end, and so on.
% If marking to the right has precedence then a new left-directed marking started on the right end would not stop it.

\item Giving precedence to the right rebuilding has the drawback that one can design a initial configuration
  in which even in the absence of noise, level 1 structure will never arise even locally.
  Namely, we can fill the line with short intervals \( \dots,J_{-1},J_{0},J_{1},\dots \)
  each of which is the start (say of size \( 3\Z \)) of a
  right-directed marking process.
  Then the head, after moving left on \( J_{0} \), will be captured by the process on \( J_{-1} \), then
  later captured by the similar process on \( J_{-2} \), and so on.
  But we will not need to consider such pathological configurations.
  \end{enumerate}
  
\end{remarks}

\subsubsection*{Details of the Survey and Create stage}

The complexity of this stage is due mainly to guaranteeing
property~\eqref{i:keep-healthy} above.

\begin{varenum}{s}
\item\label{i:survey.left}
Search in the marked area on left of \( z \) for a colony \( C \),
or a set of cells that looks stitchable into a colony by up to 3 healings.
The first substantial domain should be at least \( 3\beta \) cells to the left of \( z \),
to make sure that \( C \) is \df{manifestly} to the left of \( z \).
If the search and the stitching attempt are successful, mark \( C \) as \( C_{\Left} \).
Try to perform the stitchings; if one fails then restart rebuilding.

Repeat this search for a colony manifestly to the right of \( z \): if found, call it \( C_{\Right} \).

\item Suppose that only \( C_{\Left} \) is found, then create \( C_{\Right} \) on the right of \( z \).
The other case is symmetrical.

\item Suppose that neither \( C_{\Left} \) nor \( C_{\Right} \) have been found.
Then search the whole area, starting from the left, for a colony.
If no such colony is found then create a pair of adjacent
neighbor colonies \( C_{\Left},C_{\Right} \) around \( z \).

\item\label{i:survey.both} Suppose that both \( C_{\Left} \) and \( C_{\Right} \) 
  have been determined (found or created).
  Then fill in the area between them.
  For this, first search from the left, for (stitchable) colonies, one-by-one.
  Once no more are found, fill in the areas between them with stem cells.

\item Suppose that no colony has been found 
  manifestly to the left or right of \( z \), but one is found that is not such.
  Then either the left end is manifestly to the left or
the right end is manifestly to the right of \( z \).
We will still work for satisfying goal~\eqref{i:rebuild-size}.

Suppose that the left end is manifestly to the left of \( z \), then call the colony \( C_{1} \).
Create \( C_{\Left} \) on the left of  \( C_{1} \).
Now search for another colony on its right.
If not found, create \( C_{\Right} \), manifestly on the right of \( z \).
If one is found that is not manifestly on the right then
call it \( C_{2} \) and create \( C_{\Right} \) on its right.
The other case is symmetrical.

\end{varenum}

The steps above requiring the \df{creation} of colonies and bridges are destructive
(the stitching ones are not).
Therefore before a creation step, the whole survey preceding it is performed twice, 
describing the required action (erasing cells in order to build new ones in their place)
by the same appropriate symbol in all surveyed cells, on two different tracks.
These actions 
are then only performed if the two survey results are identical---otherwise
the healing procedure is restarted from its center.

\begin{lemma}\label{lem:rebuild-health}
  Suppose that a rebuilding procedure starts on one end of an admissible subinterval
  \( J\subseteq I \) of a clean interval \( I \), and runs with at most one burst of faults
  of size \( \le\beta \), to the finish (possibly restarted
  some times due to big turn starvation (see Section~\ref{sec:feathering}) without leaving \( I \).
  Denoting by \( K \) the output interval of rebuilding, then \( J\cup K \) will be admissible.
\end{lemma}
\begin{proof}
  If a burst occurs then no matter what mode it puts the machine into, 
  zigging will discover the inconsistency and call alarm.
  Then according to part~\eqref{i:heal.germ} of the healing procedure the frontier zone of
  rebuilding will be discovered, and rebuilding will be allowed to continue.

  The other event that may interrupt (restarting) rebuilding is big turn starvation.
  Assume for example that big turn starvation moves the head to the right repeatedly,
  but finally (since the head does not leave \( I \)) a big left turn is allowed to happen.
  Then over the area that has been passed during any of these rebuildings,
  there will be no big turn starvation preventing the head from turning right,
  since there the footprints of the big right turns (as define in Section~\ref{sec:feathering})
  have now been spaced far from each other.
  So eventually a big enough area will be free from closely spaced footprints and a rebuilding succeeds.
\end{proof}

% \section{Scale-up}

% This section defines the decoding map \( \Phi^{*} \) and proves that it indeed takes a trajectory
% to a trajectory.
% As indicated in Section~\ref{sec:model}, when dealing with the behavior
% of machine \( M \) over some space-time rectangle, we will assume that the noise
% over this rectangle is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse.  
% This means in simpler terms that at most one noise \df{burst} affecting an
% area of size at most \( \beta(\B\times\Tu) \) can occur
% in any \( \gamma \) neighboring work periods.
% In the present section, histories will always be assumed to have this property.
% Let us extend the notion of annotation
% in order to account for damage not only to health but also to information.

% \subsection{Annotated history}

% Let us extend annotation to histories.

% \begin{definition}[Distress and relief]\label{def:distress}
% Consider an annotated trajectory over a certain space-time region.
% If the head is free (see Definition~\ref{def:annotation}), then
% the time (and the configuration) will be called \df{distress-free}.
% A space-time point that is not distress-free and is preceded by
% a distress-free time will be called a \df{distress event}.
% This can be of two kinds: the head steps onto an island, or a burst occurs
% (creating an island and leaving the head in it).

% % The direction of the last \( \Z \) non-turning moves of the front before the distress event
% % will be called the \df{pre-distress sweeping direction}.

% Consider a space-time region \( J\times K \) starting with a distress event contained in \( J \)
% and ending with a distress-free configuration.
% Let \( J \) be here the interval of tape where the head passed during \( K \), then 
% we will call \( J\times K \) a \df{relief event}, if the following holds.
% % the free head making \( \Z \) non-turning moves.
% % Their direction is called the \df{post-distress sweeping direction}.
% % If it coincides with the pre-distress sweeping direction then we will say that
% % \df{no turn} occurred.

% \begin{alphenumIn}
% \item Any new islands occurring in \( J \times K \) are due to some new burst.
% \item The island that started the distress event disappears by the end of \( K \).
% % \item Provided no turn occurred, the only possible island intersecting \( J \) 
% % at the end of \( K \) belongs to some island caused by a burst during \( K \).
% % (In other words, the \emph{old} islands will be eliminated in \( K \).
% % This is always the case if the distress occurs in the interior of a colony.)
% \end{alphenumIn}
% \end{definition}

% \begin{premark}
%   Quantify this for a large enough \( K \).
% \end{premark}

% In what follows we will show that for any trajectory \( (\eta, \Noise) \) 
% on a space-time rectangle on which the
% noise is \( (\beta\pair{\B}{\Tu}, \gamma\pair{\B^{*}}{\Tus}) \)-sparse,
% if the starting configuration was annotated then the annotation
% can be extended to the interior of the rectangle and
% each distress event will be followed by a relief event.
% In the rest of the section we always rely on the assumption of  this 
% sparsity property of the noise.

% After a distress event (unlike in~\cite{burstyTuring13}), islands 
% may have their cell structure damaged: may contain disorder.
% However, since \( \eta \) is a trajectory, as we will see the islands will be cleaned out.
% So, relief can be said to happen in two stages: cleaning, and correcting the structure.
% However, this division is only for the purposes of proof: the machine has no
% ``disorder-detector'', and the proof just relies on the cleanness-extending properties of a
% trajectory introduced in Definition~\ref{def:traj}.

Now that our program is defined we can spell out a property announced in Section~\ref{sec:feathering}.

\begin{lemma}\label{lem:safe-for-turns}
  Suppose that a clean interval is passed over by the head without noise.
  Then it becomes safe for turns.  
\end{lemma}
\begin{proof}
  The informal argument given in Section~\ref{sec:feathering} can now be verified easily.
\end{proof}

\begin{lemma}\label{lem:k-bursts}
  Let \( P \) be a path with at most \( \Q/2\E \) bursts of faults of size \( \beta \)
  over an interval \( I = \lint{a}{b} \) that is admissible at the beginning of \( P \).
  Then by the end of \( P \), the subinterval \( \Int(I, 13\Q\B) \) will be still admissible.
  If no bursts occur on sections of the path that enter and exit \( I \) from the right then
  the subinterval \( \lint{a+13\Q\B}{b} \) will also stay admissible.
\end{lemma}
\begin{proof}
  Let us divide the bursts of faults into three groups.
  As bursts intersecting \( I \) occur, let us call each island created by a new burst
  an \df{end-island} if it is closer than \( 2\E\B \) to either one of the ends of \( I \) or to
  an earlier end-island.
  Let us call the other islands within \( 7\Q\B \) of the ends the \df{rebuilding} islands, and the rest the
  \df{interior} islands.

  By definition the end-islands are confined to within \( \Q\B \) of the ends of \( I \).
  If the head enters into any colony away from the end-islands in normal mode,
  then it will correct any two islands that are within \( 2\E\B \) from each other (an isolated
  one that was there and a new one just created).
  A burst can create an island that is not corrected within one sweep of the simulation,
  only at the end of a big turn.
  These big turns are outside the interior of any extended colony and separated from each other
  by at least \( \Z\B/3 \), therefore the islands at their ends
  will be corrected in any later pass by the head that touches them.

  Bursts can also affect rebuilding, but again the ones that do not become end-islands and
  are not corrected in one sweep are only the ones at the end of big turns.
  Even repeated rebuilding does not allow the remaining islands to be closer than \( \Z\B/3 \) to
  each other, therefore any new rebuilding process that touches one of these will correct it.
  It follows that if a rebuilding process is started at a distance of least \( 7\Q\B \) from the edges
  then, since it does not reach any end-island, it will be able to complete, not reaching the interior
  \( \Int(I,13\Q\B) \).
  Therefore the head will always reach this interior in normal mode.

  The argument clearly proves also the last statement of the lemma,
  about a path with no faults on entering from the right.
  In this case there is at most one end-island on the right end.
  Indeed, if the path returns from the right end-colony, leaving an island there,
  then due to feathering by the simulated trajectory, next time it must move right from it,
  and by passing the island, must correct it.
  \end{proof}


\section{Isolated bursts}\label{sec:1-level-noise}

Here, we will prove that the healing procedure indeed deals with isolated bursts of faults.
For the elimination of disorder created by faults
we will rely on the Escape, Spill Bound and the Attack Cleaning
properties of a trajectory in Definition~\ref{def:traj}.

Isolated bursts of faults don't create disorder larger than \( 3\beta \).
The head escapes a disorder interval \( I \) via the Escape property; while it is inside, the
spreading of this interval is limited by the Spill Bound property.
Every subsequent time when the head enters and exits \( I \) this gets decreased
via the Attack Cleaning property, so it disappears after \( O(\beta) \) such interactions---see
Lemma~\ref{lem:healing} below.

% \begin{lemma}[Local escape]
% Let \( G \) be an interval of size \( n\B \) where \( n<\Z \).
% Then in the absence of noise, the head will either escape \( G \) within time \( O(n\Tu) \),
% or it will be inside a clean hole at some point during this time, as per Definition~\ref{def:clean-hole}.
% \end{lemma}
% \begin{proof}
%   Let \( c = (\CMarg+2\CSpill) \), as used in the Dwell Cleaning property of Definition~\ref{def:traj}.
% Let us cover \( G \) by consecutive intervals of size \( c\B \) called \df{blocks}, let \( m \) be the
% number of these blocks.
% Assume that the head does not escape \( G \) within time \( m\cdot\CEsc\Tu \).
% Then there is a block \( K \) in which it spends cumulative time \( \CEsc\Tu \),
% and the Dwell Cleaning property of trajectories implies that at some point during this time, it
% will be inside a clean hole in \( K \).
% \end{proof}
 
In a clean configuration, whenever healing started with an alarm, the procedure
will be brought to its conclusion as long as no new fault occurs.
The trajectory properties, however, do not allow any conclusion about the
state of the cell to which the head emerges from disorder.
This complicates the reasoning, and may require several restarts of the healing procedure.
By design, the healing procedure can change the \( \Core \) track only in one cell.
The following lemma limits even this kind of possible damage:
it says that the head with the wrong information may increase some existing island, but will
not create any new one.

\begin{lemma}
In the absence of noise, no new island will arise.
\end{lemma}
\begin{proof}
The islands are defined only by the \( \Core \) track.
In normal mode, this track changes only at the front.
If this is not the real front, then we are already in or next to an island.

The healing procedure's change of \( \Core \) is part of a stitching operation.
Looking at the different cases of the proof of Lemma~\ref{lem:stitching}, we see that 
inside a healthy area, healing can only change the \( \Core \) track in two ways.
The first way is case~\ref{step:stitching.mergeable}, when the \( \Sweep \) values were changed
in a domain not belonging to the interior.
Such an operation can be applied to any healthy configuration without affecting health.
The second case is when the front is moved left or right.
This does not affect health either.
\end{proof}

The following lemma is central to the analysis under the condition that bursts of faults are isolated.

\begin{lemma}[Healing]\label{lem:healing}
In the absence of noise in \( M^{*} \), the history can be annotated.
Also, the decoded history \( (\eta^{*},\Noise^{*}) \) satisfies the Transition Function property 
of trajectories (Definition~\ref{def:traj}).
\end{lemma}
\begin{proof}
  In an annotated trajectory, call a space-time point a \df{distress event} if either a
  fault occurs there or the head steps onto an island.
  If after a distress event the head becomes free (according to Definition~\ref{def:annotation}),
  then we will say that \df{relief} occurred.
The extension of annotation is straightforward as long as no distress event is encountered.
The Transition Function property is observed, as no obstacle arises to the simulation.
Stains do not cause problems: the computation stage of the simulation cycle eliminates them
using the error-correcting code.

We will bound the head in a space-time rectangle of size \( O(\beta\B\times\beta^{3}\Tu) \)
before relief after any distress event.
If a burst of faults occurs of size \( \beta \)
then what we proved can be applied to the new situation.
knowing that now we will be free faults for the desired time.
This will just double the space and time bound for relief starting from the original distress.

With this bound, there are two possibilities: either the islands caused by the distress are eliminated,
or the head moves away from them in the given time.
The latter cannot happen in the middle of a sweep over some colony;
indeed, suppose that the sweep is to the right.
If the head leaves in the left direction from the islands then it will hit them immediately again as it tries
to move the front right.
If it leaves to the right then backward zigging will hit the islands immediately again.
But the head can indeed leave the islands at the point where for example
it has just turned left near the right end of some colony-pair.
This can only happen once, due to the feathering property.
Since they are
not in the interior, these islands will not affect decoding and the computation of the transition function.
At the end of the working period, the simulated head can turn left or right.
If it turns right then any islands left at the right end will be eliminated during the transfer stage.
It can turn left, but only once, due to the feathering property of the simulated machine.

Let us proceed to prove that relief happens within the given space and time bounds.
At the time of the distress event, let us draw an interval \( I \) of size \( \Q\B \) centered 
around the head.
The head will not leave it before relief, so we will consider the changes of 
the configuration inside it.
Let \( S_{1},\dots,S_{m} \) be the list of substantial domains of \( I \), and \( I' \) the subinterval
obtained by deleting \( S_{1} \) and \( S_{m} \).
and \( A_{1},\dots,A_{m-1} \) the ambiguous areas between them.
We have  \( m=O(1) \), and \( \sum_{i}|A_{i}|=O(\beta\B) \).
In what follows the boundaries of these intervals may slowly change in time, and some of the \( A_{i} \)
may get eliminated.

Every time the head is in the disorder it must leave within time \( O(\beta^{2}\Tu) \),
due to the Escape property.
It can leave disorder only \( O(\beta) \) times, since every time it leaves on the
same edge as entering, the Attack Cleaning property decreases the disorder.
When the head enters a clean area \( J \), there are several possibilities.

\begin{itemize}
\item Rebuilding mode.
  Since every progress step of rebuilding is accompanied by zigging,
  the head either leaves \( J \) within one zig or alarm will be triggered within \( O(\beta) \) steps.
\item Healing mode.
  One call of healing either finishes in \( O(\beta) \) steps or the head leaves \( J \) earlier.
  If a new call starts and finishes then \( J \) already contains \( I' \), the disorder has been
  eliminated, and we can refer to Lemma~\ref{lem:combined-heals} to finish.
\item Normal mode.
  If no alarm is triggered within \( 6\beta \) steps, then the head is in the frontier zone
  (as defined in Section~\ref{sec:zigging}), and is moving  away from \( I' \): this implies relief.
  If alarm is triggered then a new healing starts.
\end{itemize}

Since these are the only possibilities, the bound \( O(\beta\B\times\beta^{3}\Tu) \)
on the space-time rectangle until relief is proven.
% Whenever the head enters disorder, we will mark the edge where it entered as \df{attacked}.
% We introduce a few variables for the proof.
%
% \begin{itemize}
%  \item[\( R\)] is the set of those \( i \) for which \( S_{i}\cup A_{i}\cup S_{i+1} \) is clean.
%  \item[\( n_{i} \)] for \( i\in R \) is the number of 
% stitching steps by the algorithm of Lemma~\ref{lem:stitching} needed to eliminate \( A_{i} \).
% Let \( N=\sum_{i\in R} n_{_{i}} \).
% \item[\( D \)] is the total size of disorder, divided by \( \B \).
% \item[\( E \)] is the number of un-attacked edges of disorder where the head can exit without
%   entering (because it is on the side of the disorder).
%   This number never increases.
% \item[\( F \)] is the number of un-attacked edges where the head cannot exit without entering (because
%   it is away from the disorder).
% \item[\( P \)] is be the number of steps that the front moves forward (including the ones after possibly
% changing the meaning of forward at a regular turn).
% \end{itemize}
%
% The following kinds of event may occur:
% \begin{varenum}{h}
%
% \item\label{i:heal.N-decr} With a stitching step done, \( N \) decreases by 1 while possibly 
% moving the front backward.
%
% \item\label{i:heal.enter-carpet} The head enters disorder, decreasing \( F \).
%
% \item\label{i:heal.leave-carpet.E}
% The head leaves disorder on a non-attacked edge, decreasing \( E \).
%
% \item\label{i:heal.leave-carpet.F}
% The head leaves disorder on an attacked edge, decreases the disorder by at least \( \B \) and
% increases \( F \) by 1.
%
% \item\label{i:heal.clean}
% A set \( A_{i} \) becomes clean: then \( i \) gets added to \( R \); 
% this can happen only \( m \) times.
%
% \item\label{i:heal.N-incr.healing}
% \( N \) increases by \( 1 \).
% This can only happen after the head exited disorder in healing mode (with the wrong information).
% At the exit, either \( E \) or \( D \) had to decrease.
%
% \item\label{i:heal.front-move} The front moves forward in normal mode
% (possibly after making a regular turn, and thus changing the forward direction).
% This may also increase \( N \) by 1 (and is the only way to do so), 
% say by decreasing a domain \( S_{i} \).
% But it is followed by zigging.
% The zigging may hit disorder, leading to case~\eqref{i:heal.enter-carpet},
% or find something wrong---and trigger new healing, which leads to some of the other events.
% Otherwise the island around the front will be deleted, and the head becomes distress-free.
%
% \item\label{i:fail}
%   Healing fails, the head increases the germ.
%
% \item\label{i:decrease-germ}
%   The head decreases the germ in healing mode ``thinking'' that the surveyed area is healthy.
%
% \item\label{i:heal.approach-front} 
%   The head approaches the front healing mode ``thinking'' that the surveyed area is healthy.
%
% \item\label{i:rebuild}
%   Rebuilding mode starts.
%   Any change by rebuilding will be followed by zigging, which will either enter disorder or discover
%   that the germ is not big enough, and healing is restarted.
%
% \end{varenum}
%
% The above possibilities suggest a potential function
% \begin{align*}
%    N + c_{D}D + c_{E}E + c_{F}F + c_{m}m - c_{P}P
%  \end{align*}
% where \( c_{D},\dots \) are appropriate positive constants.
% Let us look at what each possibility does to the potential.
%
% Cases~(\ref{i:heal.N-decr}-\ref{i:heal.leave-carpet.E}) decrease the potential if \( c_{P}<1 \).
% Case~(\ref{i:heal.leave-carpet.F}) decreases the potential if \( c_{D}>c_{F} \).
% There are only \( O(1) \) cases of type~(\ref{i:heal.clean}), increasing \( N \) by a total of \( O(\beta) \).
%
% In case~(\ref{i:heal.N-incr.healing}), if \( c_{D} \) and \( c_{E} \) are large enough,
% the increase in \( N \) can be charged to a decrease in \( D \) or \( E \).
%
% Consider case~(\ref{i:heal.front-move}).
% If the zigging hits new disorder and \( c_{F}>1 \) then the potential decreases.
% If it triggers new healing then this will lead to one of the other cases,
% compensating for the increase of \( N \) if the constants (other than \( c_{P} \)) are large.
%
% These considerations show that relief indeed follows distress in time \( O(\beta^{2}\Tu) \).
% At that point a normal-mode step moving the front has been made, followed by zigging.
% Therefore the island causing the distress must have been eliminated, allowing the 
% simulation to continue.
% The stain caused by the island remains, but as discussed after Definition~\ref{def:annotation},
% the simulation guarantees that the
% size and number of stains remains bounded as required by that definition.
% The simulation also continues to satisfy the Transition Function property of trajectories.
\end{proof}

\section{Cleaning}\label{sec:cleaning}

This section will scale up the Spill Bound, Escape, Attack Cleaning
and Pass Cleaning properties of trajectories, proving them for the history \( (\eta^{*},\Noise^{*}) \)
decoded from a trajectory \( (\eta,\Noise) \).

\subsection{Escape}\label{sec:escape}

Here will scale up the Escape property.
Consider some fault-free path during a time interval \( J \) (later we will allow a single
burst of faults of size \( \beta \))
over some space interval \( G \) of size \( |G|=\lambda\Q\B \).
Intervals of time of length \( \Tu \)
we may consider as \df{steps}, since in a clean interval, the machine \( M \)
will perform at least one step of computation during it.

\begin{definition}\label{def:K(t)}
  For the times \( t\in J \), let \( K(t) \) denote set of all clean points in \( G \).
  Let \( K^{*}(t) \) be the union of subintervals of \( K(t) \) that are
  super-healthy (see Definition~\ref{def:super-health}) with
\begin{align*}
 k(t)=|K^{*}(t)| .
\end{align*}
The set \( K(t) \) consists of maximal disjoint intervals.
In the following lemmas, we consider one of these, \( I(t) \) during a fault-free path,
where it may slightly decrease (without disappearing), increase or merge with others.
At the beginning of the consideration, has
size \( n\B \) with \( n=\lambda\Q \),  \( \lambda\le 2\beta \).
If the head leaves \( I(t) \) within \( 4\Z n \) steps then we will call its stay \df{short},
otherwise we call the stay \df{long}.
\end{definition}

In a super-healthy clean interval in history \( \eta \) the simulation of \( \eta^{*} \) will proceed.
In what follows we will estimate the growth of \( k(t) \), due to the creation
of colonies in \( I(t) \) or adding new ones on an edge.
Recall  \( \U_{k} = \U =\Q^{3} \) in Definition~\ref{def:hier-params}.
This is an upper bound on the number of computation steps in one work period, even allowing some
calls for healing.

\begin{lemma}\label{lem:inside-hole}
  If \( n< 2\Q \) then the stay is short.
  Otherwise, every \( 2\lambda\U \) steps of its stay increase
  \( k(t) \) (the size of area of \( I(t) \) ``organized up to the next level'') by at least \( \Q\B \).
  % Therefore the stay cannot be longer than \( 2\lambda^{2}\U \) since this would increase \( I(t) \)
  % beyond the interval \( G \).
\end{lemma}
\begin{proof}
  We will see that while the head does not leave \( I(t) \),
  every certain number of steps results in some kind of progress.
  \begin{enumerate}
  \item\label{i:inside-hole.rebuild}
Suppose that we are at some time when the rebuild procedure had started,
from a base of rebuild-marked cells large enough not to result in alarm (renewed call for healing)
after the first zigging.
Then the rebuilding will be completed, increasing \( k(t) \).

\item\label{i:inside-hole.normal}
  Suppose that we are at some time when the mode is normal.
  Then within \( 4\Z \) steps of the simulation
  either alarm is called, or a step of progress will be
  made in the ordinary work of simulation.
  If the simulation performs a transfer operation and \( n<2\Q \)
  then in moving new colony-pair, it must exit \( I \).
  
\item\label{i:inside-hole.alarm}
  Suppose that at some time alarm is called.
  Then according to the proof of Lemma~\ref{lem:healing},
  we either arrive at the above case~\ref{i:inside-hole.rebuild} (failed healing)
  in \( O(\beta^{2}<\Z) \) steps or healing finishes in
  normal mode with at least one step made either to move closer to the front or to make a move
  in the ordinary work of simulation.
  
  Indeed, if healing succeeds it leaves a healthy area.
  Lemmas~\ref{lem:health-extension} and~\ref{lem:combined-heals} imply that 
  the overlapping successful healing areas can be combined, so healing will not be repeated over the same
  area, and thus can slow down progress only by a 
  factor \( O(\beta^{2}) \) (negligible compared to the \( \Z \) times slowdown by zigging), and even this
  in at most one sweep of simulation over any area.

\item\label{i:long-stay}  Suppose that \( n\ge 2\Q \) and the stay is long.
  Without rebuilding, the continued healings add some colonies to the healthy area; any such colony will be
  turned super-healthy by the first complete work-period of the simulation, taking at most \( \U \)
  steps of computation.
  If the head is already in a super-healthy area then it will perform the simulation of \( M^{*} \).
  The simulated machine \( M^{*} \) has the same program as \( M \), so it will also
  make switchbacks of at least \( \E>4\beta>2\lambda \) steps (for healing or zigging).
  Therefore every \( 2\lambda\U\ge 4\U \) computation steps
  simulating \( \ge 2\lambda \) steps of \( M^{*} \),
  will pass to the end of the super-healthy interval in which it is working, and
  whose size is at most \( \lambda\Q\B \), and extend it---unless interrupted by rebuilding.

\item  If any rebuilding starts then it will either finish and extend \( K^{*}(t) \) by a colony,
  or will be interrupted by a big turn starvation, as defined in Section~\ref{sec:feathering}.
  Suppose that big turn-starvation happens at an attempted left turn:
  then the marked rebuilding area is already of a size \( \ge 2\Q\B \).
  Therefore a next big turn starvation cannot be due to an attempted right turn on the left, since
  the marking process does not proceed so far to the left.
  Repeated big turn starvations could only happen to the right, and the head would leave on the right before the
  assumed \( \lambda\U \) computation steps.
  Since this does not happen, rebuilding succeeds.
\end{enumerate}
\end{proof}

Now consider cases where the head exits or enters \( I(t) \).

\begin{definition}\label{def:advancing-end}
  We will say that the right end of \( I(t) \) is \df{advancing} at time \( t \)
  if the rightmost colony of \( K^{*}(t)\cap I(t) \) is in the start of the process of transfer to the right.
  Advancing is defined similarly for the left end.
\end{definition}

\begin{lemma}\label{lem:long-stays}
  Consider a sequence of long stays \( J_{1},J_{2},\dots,J_{r} \)
  in \( I(t) \), possibly interrupted by any number of short ones.
  \begin{alphenum}
  \item\label{i:make-advancing}
    In a long stay, \( k(t) \) will not decrease.
    If it does not increase by at least \( \Q\B \)
    then the end on which the head leaves becomes advancing.
  \item\label{i:enter-advancing}
    If the head enters on an advancing end for a long stay
    then \( k(t) \) increases by at least \( \Q\B \) before it leaves---even
    after any number of short stays in between.
  \item\label{i:many-long}
    If the number of steps spent on long stays in \( I(t) \) is at least \( s\lambda\U \) then
    they will increase \( k(t) \) by at least \( (s/8)\Q\B \).
  \end{alphenum}
\end{lemma}
\begin{proof}
  To~\eqref{i:make-advancing}:
  At the entrance from disorder, the head may slightly damage a colony.
  But the subsequent checking would discover it and the stay is long enough for a rebuilding to succeed,
  and even to create a new colony, increasing \( k(t) \) by at least  \( \Q\B \). 
  In the absence of rebuilding, a simulation must have continued;
  but then it can exit only via a started transfer operation, creating an advancing end.

  To~\eqref{i:enter-advancing}:
  Suppose that the head leaves on a advancing right end, and then re-enters after
  possibly a number of short stays.
  Let \( C \) be the right member of the colony-pair that
  that started the right transfer operation.
  The head may destroy \( C \) when entering, but even after repeated short stays, it cannot go past it
  without either completing a rebuilding operation (and thus increasing \( k(t) \)) or
  completing the transfer operation, and entering \( C \) from the right from a new colony
  created by the transfer.
  The long stay suffices for either the transfer or a rebuilding to succeed.

  To~\eqref{i:many-long}:
  Let us pair the long stays: \( (J_{1},J_{2}) \), \( (J_{3},J_{4}) \),\dots.
  If \( r \) is odd and \( J_{r} \) has \( a\lambda\U \) steps for \( a\ge 2 \)
  then by Lemma~\ref{lem:inside-hole}
  it advances \( k(t) \) by \( \ge\flo{a/2}\Q\B\ge (a/4)\Q\B \).
  Similarly if the longer of \( J_{2i-1},J_{2 i} \) has \( a\lambda\U \) steps, then the pair has
  at most \( 2a\lambda\U \) steps, the pair advances \( k(t) \) by \( (2 a/8)\Q\B \).
  Otherwise it follows from~\eqref{i:make-advancing}-\eqref{i:enter-advancing} that the pair increases \( k(t) \) by
  \( \ge\Q\B \).  
\end{proof}

The following lemma is the scale-up of the Escape condition.

\begin{lemma}[Escape]\label{lem:escape}
  Let \( c  = 9 \), so  \( 2 c + 1 = 19 = \CEsc \) as defined in~\eqref{eq:cns.traj}.
  In the absence of \( \Noise^{*} \), the
  head will leave any interval \( G \) of size \( n\B \) with \( n=\lambda\Q \),
  \( 1\le\lambda\le 3\beta \), within \( (2c+1)\lambda^{2}\U \) steps.
\end{lemma}
\begin{Proof}
  Consider a time interval of length \( (2 c + 1)\lambda^{2}\U\Tu \).
  Suppose that a burst of faults of size \( \le\beta \) happens during it: then we will consider the
  larger part \( J \) of before or after the burst (or the whole interval if there is no fault).
  Let 
\begin{align*}
 \d=\CPass/3 ,\; \g = \cei{\CEsc(7\d)^{2}}.
\end{align*}
We will consider a sequence of times 
\begin{align*}
 t_{i}=t_{0}+i\g\Tu,
\end{align*}
where \( t_{0} \) is our starting time.
The time intervals \( \rint{t_{i}}{t_{i+1}} \), each comprising \( \g \) steps,  will be called \df{skips}.
Consider adjacent space intervals \( L_{1},L_{2},\dots,L_{7} \) of size \( \d\B \).
If the head is in \( L_{4} \) then by the Escape property of trajectories
it will escape \( L_{1}\cup\dots\cup L_{7} \) within
time \( \g\Tu \), and in the process pass either over \( L_{1}\cup L_{2}\cup L_{3} \)
or over \( L_{5}\cup L_{6}\cup L_{7} \).
In the long run, counting such passes will help applying the Pass Cleaning property.
Let us partition the interval \( G \) into subintervals \( L_{j} \) of form \( \lint{a}{b} \) of size
\( \d\B \) called \df{blocks}; the last one can be smaller.
From the above, it follows that each skip passes over at least 3 blocks
(starting before them and ending after them), either in the left or in the
right direction.

  We will show the contribution to the growth of \( |K(t)| \)
made by the various events as they occur during various kinds of skip.
Clearly it cannot grow larger than \( |G| \) while the head stays in \( G \).
Let us call a skip \df{clean} if in it the head does not touch disorder in any \( L_{j} \),
otherwise we will call it \df{disordered}.

\begin{step+}{step:escape.dirty}
  The number of disordered skips is at most
\(    18\lambda\Q(\passno/d+1) \).
\end{step+}
\begin{pproof}
    If \( L=L_{j+2} \) in a right pass
    for \( \passno \) values of \( j \) then the Pass Cleaning property implies that it becomes clean.
    Similarly for left sweep.
    So it can contain disorder only for \( 2\passno \) skips in which it is \( L_{j+2} \).

    If \( L=L_{j+1} \) for \( \passno \) values of \( j \) in left-right passes then the
    Pass Cleaning property implies that \( L_{j+2} \) becomes clean.
  After this, due to the Attack Cleaning property, if there is still disorder in \( L \) then
  each left-right pass decreases it by \( \B/2 \).
  So \( L \) can contain disorder only for at most \( 2(\passno + \d) \) skips with left-right passes,
  and the same number of skips for right-left passes, so at most \( 4(\passno+\d) \) passes altogether.

  The same reasoning works for \( L=L_{j+r} \) for any \( r\ne 2 \) when the head touches disorder in \( L \).
  So the number of skips in which \( L \) is equal to any of the \( L_{j+r} \) for \( r=1,\dots,5 \) and
  \( L \) contains disorder is at most \( 18(\passno+\d) \).
  A bound on number of skips in which the head touches disorder in any \( L_{j} \) is obtained if we multiply this
  bound by the total number \( \lambda\Q/\d \) of skips.
\end{pproof} % step:escape.dirty
\begin{step+}{step:escape.short}
  The number of clean skips happening during short stays (as in Definition~\ref{def:K(t)}) is
  at most \( 3\lambda^{2}\Z\Q^{2} \).
\end{step+}
\begin{pproof}
  If a clean skip belongs to a stay in an interval \( I(t) \) then this interval has length at least
  \( \CPass\B \).
  It follows that by the Attack Cleaning property, after exiting, the next entry either increases \( |I(t)| \) by
  \( \ge\B/2 \) or joins it to another interval in \( K(t) \) of size \( \ge\B/2 \).
  The total number of such joining events is at most \( n = \lambda\Q \).
  The number of increases by \( \B/2 \) is also at most \( 2n \), so the number of
  such short stays is at most \( 4n \).
  In each short stay the number of steps is \( \le 4\Z n \), so the number of skips is at most
  \( (4/3)\Z n \).
  Multiplying this by \( 4 n \) gives the bound \( (16/3)\Z n^{2} \le 6\lambda^{2}\Z\Q^{2} \).
\end{pproof} % step:escape.shor
\begin{step+}{step:escape.long}
The total number of steps spent in long stays is \( >(c\lambda^{2} -1)\U \).
\end{step+}
\begin{pproof}
  It follows from part~\ref {step:escape.dirty} above that
  the number of steps in  disordered skips is \(  \le  18\g\lambda\Q(\passno/d+1) \).
  The number of steps in clean skips during short stays is \( \le 3\g\lambda^{2}\Z\Q^{2} \).
  So the number of steps of both kinds is still \( <\Q^{3}=\U \).
  Subtracting this from the total number \( c\lambda^{2}\U \)  of steps gives the estimate.
\end{pproof} % step:escape.long
Lemma~\ref{lem:long-stays} implies from part~\ref{step:escape.long}
that the long stays increase \( |K^{*}(t)| \) by \( >(c\lambda/8 - 1/8\lambda)\Q\B \).
If \( c=9 \) then this becomes \( >|G|=\lambda\Q\B \).
\end{Proof}


\subsection{Weak attack cleaning}

This section will scale up the Attack Cleaning
property of trajectories (Definition~\ref{def:traj})
to machine \( M^{*} \), but first only in 
a weaker version, restricting the number of bursts of faults in the relevant interval.

\begin{definition}[Trap]\label{def:trap}
  Recall the definition of trains in Section~\ref{sec:feathering}.
In an interval \( I \), clean except possibly up to \( 3\passno \) islands of size \( \beta \),
a point is considered a \df{leftward trap} if (after changing it in the islands),
it is at a right-directed frontier zone with a train having \( \FCount_{1}\ge\F \)
(indicating an imminent left turn).
\end{definition}

The Attack Cleaning property says the following for the present case.
Let \( P \) be a path that is free of any fault of \( \eta^{*} \).
For current colony-pair \( \pair{x}{x'} \) (where \( x'< x+2\Q\B \)), suppose that the interval
\( I=\lint{x-\CMarg\Q\B}{x'+\Q\B} \) is clean for \( M^{*} \).
Suppose further that the transition function, applied to \( \eta^{*}(x,t) \), directs the head right.
Then by the time the head comes back to \( x-\CMarg\Q\B \),
the right end of the interval clean in \( M^{*} \)
containing \( x \) advances to the right by at least \( \Q\B /2\).

 \begin{lemma}[Weak attack cleaning]\label{lem:weak-attack-clean}
   In addition of the above condition of attack cleaning,
   assume that the faults of size of trajectory \( \eta \) in the interval
   \( I \) covered by at most \( \s<\Q/3\E \) bursts of size \( \beta \).
   Then the conclusion holds.
\end{lemma}
\begin{proof}
The computation phase of the simulation on the colony-pair \( \pair{x}{x'} \) is completed,
then the transfer phase begins, possibly entering the disorder to the right of \( x'+\Q\B \).
We argue that there are only two ways for the head to get back to \( x-\CMarg\Q\B \).
\begin{varenum}{at}
\item\label{i: weak-attack-clean.normal} The transfer into a new colony-pair with starting point
\( y\ge x'+\Q\B \) succeeds despite the disorder, and the clean interval extends over it, before
the head moves left to \( x-\CMarg\Q\B \) in the course of the regular simulation.
Some inconsistencies may be discovered along the way, but they are corrected by healing.

\item\label{i: weak-attack-clean.rebuild} The inconsistencies encountered along the way trigger some
rebuilding processes.
Eventually a complete, clean rebuilding area is created, the rebuilding succeeds, leaving a clean
colony also to the right of \( x'+\Q\B/2 \).
\end{varenum}

By the Spill Bound property, disorder can spill left of \( x'+\Q\B \) only by
\( \CSpill\B \) as long as the path \( P \) is fault-free.
If a burst of faults creates an island at a distance \( \ge\E\B \) from the disorder and other islands
then this island will be healed unless it is at a rightward trap.
The feathering property assures that such remaining islands are at a distance \( \ge\Z\B/3 \) from
each other, ready to be healed at any next pass.
There are at most \( \s \) bursts of size \( \le\beta \),
so the possibly not correctable islands are all
within distance \( \s\E\B \) of the disorder, to the right of
\begin{align*}
   R = x'+\Q\B -(\s\E+\CSpill)\B.
\end{align*}
Consider this as the new left end of the disorder; the bound \( \s \) on the number of bursts
implies that size of the new spill is still \( <\Q\B/3 \).

Suppose that rebuilding is not initiated (with creating a substantial germ):
then the head can move deeper left into the colony of \( x' \) only by the normal
course of simulation: the transfer stage of the simulation must be carried out, and this requires 
at least as many attacks to the right as the number of sweeps in the transfer stage.
Every attack (followed by return) extends the clean interval further,
until the whole target colony becomes clean, and the transfer completed.
This is the case~\eqref{i: weak-attack-clean.normal}.

Recall the rebuilding procedure in Section~\ref{sec:rebuilding}:
the rebuilding area extends \( 3\Q \) cells to the left and right from its initiating cell.
This may become as large as \( 6\Q\B \) to the left and right.
If initiated, its starting position \( z \) is to the right of \( R \) as defined above.
It then may extend to the left to at most \( z-6\Q\B \) (this is over-counting,
since the cells of the colony of \( x \) are all adjacent):
if the head moves to the left of this, then the rebuilding must have succeeded.
Its many sweeps will result in attacks that clean an area to the right of the starting point.
The procedure may be restarted several times, but those re-startings will also be initiated
to the right of \( z \).
The rebuilding also must find or create a colony manifestly to the right of the restarting site.
Since \( R>x'+\Q\B/2 \) this will move the boundary of the area clean in \( M^{*} \) 
by at least \( \Q\B/2 \): this is the case of~\eqref{i: weak-attack-clean.rebuild}.

In the process described above,
it is possible that the rebuilding finds a competing colony \( C \)
starting at some \( y \in x'+\Q\B + \lint{-\beta\B}{0}\) which
slightly (by the size of an island) overlaps from the right with the colony of \( x \).
The rebuilding may decide to keep \( C \) and to overwrite the rest of the colony of \( x' \) as a bridge
(or even target).
This does not affect the result.
\end{proof}

The following lemma draws the consequences of several applications of weak attack cleaning.

\begin{lemma}\label{lem:weak-repeated-attack}
  Let \( I_{0} \) be an interval of size \( > 28\Q\B \), and \( J \) an adjacent
  interval of size \( k\Q\B \) on its right.
  Assume that \( I_{0} \) is super-healthy at the beginning of a path \( P \) whose
  faults are coverable by fewer
  than \( \Q/2\E \) bursts over the interval  \( I_{0}\cup J \), that passes \( I_{0} \)
  at least \( 2^{k+14} \) times from left to right.
  Assume also that no faults occur on segments of the path that enter and exit \( I_{0} \) from the right.
  Then at some time during the path, the interval \( J \) becomes admissible.
  The same statement holds if we switch left and right.
\end{lemma}
\begin{proof}
  We will apply Lemma~\ref{lem:k-bursts} to intervals into which the \( I_{0} \) and the
  right extensions of its admissible area.
  It is easy to see that \( \Int(I,13\Q\B) \) will always stay admissible; at any time \( t \)
  let \( I(t) \) be the largest admissible interval containing this.
  At the first pass, the rightmost colony of \( I(t) \) will be at a distance \( \le 13\Q\B \) from the right end.
  The lemma implies that the right end of \( I(t) \) will not move left.
  Every time when \( P \) passes to the right of \( I(t) \), there will be an attack, extending \( I(t) \) by \( \Q\B \).
  However, not every time that path \( P \) passes \( I_{0} \) to the right, will it necessarily pass
  also \( I(t) \); it may turn back before.
  Feathering makes sure, however, that in \( 2^{i} \) passes, it moves right at least \( i \) times,
  so the \( 2^{k+14} \) passes are sufficient to include extend \( I(t) \) over the whole interval \( J \).
  \end{proof}


\subsection{Pass cleaning}\label{sec:pass-cleaning}

\begin{sloppypar}
  We will assume that \( \passno \) is an even number.
The scaled-up version of the Pass Cleaning property
considers a path \( P \) with no faults of \( \eta^{*} \), as it makes
\begin{align}\label{eq:passno}
 \passno^{*}=\passno + 8
 \end{align}
 passes over an the interval \( I \) of size \( \CPass\Q\B \),
 and claims that they make \( \Int(I,\CMarg\Q\B) \) clean for \( \eta^{*} \).
 The weak version of this property uses the additional assumption that the faults of \( P \)
 are coverable by at most \( 3\passno \) bursts of size \( \beta \).
 This is what we will prove first, so let us use this assumption.
 Since there are few bursts, there will be long intervals that are fault-free.
 Specifically, \( I \) is made up of clean subintervals of size \( \ge 4\Z\B \) that we will call \df{basic holes}
 separated from each other and the ends by distances \( \le 12\passno\Z\B \).
 \end{sloppypar}
 
The pass cleaning property of \( \eta \) cleans the basic holes (except for margins of size \( \le\CMarg\B \)).
By the Spill Bound property, they may erode on the edges by a further amount \( \CSpill\B \).
We will call these somewhat smaller intervals still basic holes.
One more pass will make the basic holes, according to Lemma~\ref{lem:safe-for-turns}, safe for turns.
The following two lemmas will show how some order will be established on them in two more passes.
Recall that the maximum number of cells in a healing area, \( \E  = O(\beta) \)
from Definition~\ref{def:substantial}, is a constant, much smaller than the zigging distance (in cell widths)
defined in~\eqref{eq:FDef} as \( \Z=\passno^{1+\rho} \).

\begin{definition}\label{def:directed}
A clean interval \( J \) of size \( >3\Z\B \) not containing the head is called \df{right-directed}
if it is safe for turns, the head is to the right of \( J \),
and its cells, maybe with the exception areas of size \( 3\passno\E\B \) on the ends,
point towards a front at the right end of \( J \) (in normal operation or rebuilding),
with the corresponding frontier zone inside \( J \).
\end{definition}

\begin{lemma}\label{lem:make-directed}
  Consider an interval \( \lint{a}{b} \) of size \( \ge 4\Z\B \) that is safe for turns.
  If a path passes it noiselessly
  from left to right then it will leave a clean right-directed interval \( \lint{a}{b'} \) with 
  \( b'\ge b-\CSpill\B \).
  The same is true when interchanging left and right.
\end{lemma}
\begin{proof}
  Assume that the starting mode is normal.
  If it remains normal then \( J \) naturally becomes directed by the time the head exits.
  Since it exits at the front (more precisely at the right of the frontier zone),
  the frontier zone stays behind.
  Let us see that also in all other cases when the head exits it leaves behind the
  right frontier zone.
  
  If healing gets triggered then as long as healing succeeds it extends a healthy area.
  It moves towards right only if the front is on the right.
  If it does not succeed then rebuilding gets triggered,
  and since it does not touch the left end anymore, it either succeeds or exits on the right
  while trying to extend towards the right.
 The same considerations work if the starting mode is healing, except possibly on
 the part of size \( \le\E\B \) of the left end where the first healing took place, whose purview
 was not completely contained in \( J \).
\end{proof}

Recall the definition of the feathering parameter \( \F \) in~\eqref{eq:FDef}.

\begin{lemma}\label{lem:keep-directed}
  Consider a fault-free path.
  \begin{alphenum}
  \item\label{i:stay-directed}
    Suppose that an interval \( J \) is right-directed, and the head enters it from the right.
    If the head leaves on the right then \( J \) will stay right-directed. 
  \item\label{i:other-end}  If the head leaves on the left then within \( 2\Z \) cells of the right end
    of \( J \) there will be a footprint of a big left turn in \( J \) (as defined in Section~\ref{sec:feathering}).
% \item\label{i:no-inside-trap}
%   If the path passes a hole \( \lint{a}{b} \) from right to left 
%   then any rightward trap it leaves there must be within \( 2\E\B \) of the left end.
  \end{alphenum}
  The same conclusions hold if we switch left to right.
\end{lemma}
\begin{proof}
  To~\eqref{i:stay-directed}: Both in normal operation and rebuilding,
  the head moves the front with itself and returns to it from every zig, except when it is
  captured at an end of \( J \) (by faults or disorder).
  If an island is encountered or faults occur then this will be healed, except when the healing interval
  (whose maximum size is \( \E\B \)) intersects
  the boundary of \( J \) (where the disorder may capture the head).

  To~\eqref{i:other-end}: the front can turn back only at leftward traps;
  otherwise the simulation or rebuilding will move it forward, and the forward
  zigging prevents faults or disorder from turning it back prematurely.
  On turning back, it will leave behind a footprint of a big left turn,
  though the at most \( 3\passno \) bursts of size \( \le\beta \) covering the faults 
  happening in between can shorten this train (of size \( \Z/2 \))
  by \( 3\passno\ll\Z \) cells.

 %  To~\eqref{i:no-inside-trap}:
 %  Let us assume that the trap is in \( \lint{a+2\E\B}{b} \), and arrive at contradiction.
 %  Now, it does not turn back the head only if it triggers an alarm.
 %  Given the distance from \( a \) the healing concludes.
 % If it succeeds then again, the head turns the front back and erases the trap.
 % Otherwise it starts a rebuilding; but the latter starts to the right (and is not interrupted due to the
 % precedence of the right direction in rebuilding), erasing the trap.
\end{proof}

\begin{lemma}[Weak pass cleaning]\label{lem:weak-pass-clean}
  Suppose that a path \( P \) has no faults of \( \eta^{*} \),
  it makes \( \passno^{*} \) passes over the interval \( I \) starting from the left,
  with its faults covered by at most 
\begin{align*}
 \s=\passno^{*}(\passno^{*}+2^{\CPass+\CMarg + 14}).
\end{align*}
bursts of size \( \beta \) in \( I \).
Then by end of the \( \passno ^{*} \)th pass the interior
\( \Int(I, \CMarg\Q\B) \) becomes clean for \( \eta^{*} \).
\end{lemma}

\begin{Proof}
  Let us number the passes after the first \( \passno \) of them like pass 1,2,3,\dots.
  \begin{step+}{step:weak-pass-clean.pass-clean}
    The first \( \passno+1 \) passes make the basic holes clean and safe for turns, except
    for margins of size \( \le(\CMarg+\CSpill)\B \).
  \end{step+}
  \begin{pproof}
 As shown above, the basic holes, of size \( \ge 4\Z\B \) and
 separated from each other and the ends by distances \( \le 4\s\Z\B \),
 become and stay clean for \( \eta \) in the first \( \passno \) passes, except for margins
 of size \( \le(\CMarg+\CSpill)\B \).
 Now the next pass, called pass 1, will make the basic holes safe for turns.
\end{pproof} % step:weak-pass-clean.pass-clean

Assume that pass 1 was from right to left; otherwise, we start one pass later.
Since we will finish in 7 passes we will still finish by \( \passno^{*}=\passno+8 \).
Let us call at any time a subinterval of \( I \) a \df{hole} if it is clean and safe for turns
with the possible exception of  \( \s \) islands, and is maximal with this property.
Holes will grow from basic holes.

  \begin{step+}{step:weak-pass-clean.traps}
    Passes 2 and 3 will leave each hole left-directed, with the footprint of a left turn
    on the left end of each but possibly the last one.
\end{step+}
\begin{pproof}
  \begin{itemize}
  \item Lemma~\ref{lem:make-directed} shows that the
next pass turns each basic hole into a right-directed hole, with possibly a single island
caused by faults.
 \item Lemma~\ref{lem:keep-directed}\eqref{i:stay-directed} shows that
   the limited number of bursts of size \( \beta \) between this and the next pass
   that cover the faults, keeps the holes right-directed (just possibly adding some islands).
\item
  Lemma~\ref{lem:make-directed} shows again that the following leftward pass turns
  each hole into a left-directed one.
  Lemma~\ref{lem:keep-directed}\eqref{i:other-end} shows that this same leftward pass
  leaves a footprint of a left turn within \( \E\B \) of the right end of each hole.
  \end{itemize}
\end{pproof} % step:weak-pass-clean.traps

\begin{step+}{step:weak-pass-cleaning.opposing}
  After pass 4 (which is from the left),
  each interval between holes has a footprint of a big left turn on its left and one
  of a big right turn on its right.
\end{step+}
\begin{pproof}
  Feathering allows the footprint of a big left turn
  on the right end of a hole \( J \) to be erased only if it is
  moved to  a distance at least \( \ge\F\B \) to he right.
  As \( \F \gg \) the upper bound \( \s\Z \) on the separation between the holes,
  the process erases the disorder between \( J \) and the next one, \( J' \) unless \( J' \) contains
  a rightward trap at its end.
  In the latter case, a footprint of the big left turn at the right end of \( J \) remains.
  Lemma~\ref{lem:keep-directed}\eqref{i:other-end} shows that a footprint of a 
  big right turn is created on the left end of every remaining hole.
\end{pproof} % step:weak-pass-cleaning.opposing

\begin{step+}{step:weak-pass-clean.make-clean}
  Passes 5 and 6 make the interval clean and safe for turns, except possibly one island of size \( \beta \)
  caused by faults during pass 6.
\end{step+}
\begin{pproof}
  Having each boundary between holes surrounded by the footprints of big left and right turns,
  the next pass (which is from the right) will erase all these boundaries.
  So it makes the whole interval (other than the edges) clean.
  The following pass will make it safe for turns.
\end{pproof} % step:weak-pass-clean.make-clean
  
\begin{step+}{step:weak-pass-clean.finish}
  Pass 7 will clean \( \Int(I, 7\Q\B) \) for \( \eta^{*} \).
\end{step+}
\begin{pproof}
  The intrusions from right to left into \( I \) before the next pass
  may create up to \( \s \) islands of disorder, of size \( \beta \),
  but these islands are placed at least \( \Z\B/3 \) apart.
  Indeed, in order for an island not to be cleaned, it must be at the right end of a big left turn,
  and so will leave a left turning footprint.
  So a later intrusion can only leave an island if it does not see \( 2\E \) cells of this
  footprint and is therefore at a distance \( >\Z\B \) to its left, or passes it by a distance \( \F\B \).
  These intrusions create no rightward trap.

  When the full rightward pass comes, the head 
  cannot pass without either already having an area already clean for \( \eta^{*} \) or
  cleaning, healing and rebuilding.
  No rebuilding started farther than \( 6\B\Q \) from the boundary will exit \( I \): it may only result in
  alarm if it encounters disorder or the single burst of faults of size \( \le\beta \) occurs.
  As seen above, any disorder encountered at a time will be of size \( \le\beta\B \), at a distance
  at least \( \Z\B/3 \) from the next one.
  Hence once the healing restarts after this disorder is cleaned,
  by part~\eqref{i:heal.germ}, it will discover the rebuilding front, allowing it to continue.
  So eventually the rebuilding succeeds.

  This process may be repeated as the head moves right, and indeed the head cannot pass through \( I \)
  without continuing this process, making it super-healthy.
  The only missed areas are those that may contain a rebuilding interval
  reaching the edge of \( I \).
\end{pproof} % step:weak-pass-clean.finish
\end{Proof}


% \begin{remark}
% Left and right are not symmetric in the above proof.
% Consider a right-directed basic hole with a rebuilding area being extended on the right.
% The head may be captured and returned, extending some other rebuilding area towards the left.
% Without giving preference to the right direction, this switching between left and right-directed rebuilding
% could happen an unbounded number of times.
% Using the asymmetry of the definition of rebuilding in Section~\ref{sec:rebuilding},
% the right-directed extension of the basic hole will not be overridden by a left-directed rebuilding.  
% \end{remark}

\begin{lemma}\label{lem:repeated-attack}
  Consider the statement of Lemma~\ref{lem:weak-repeated-attack} with the interval
  \( I_{0} \) having the same length \( k\Q\B \) with \( k=\CPass+\CMarg \) as the interval \( J \).
  The conclusion holds also
  without the bound on the number of bursts covering the faults over the interval  \( I_{0}\cup J \).
\end{lemma}
\begin{proof}
  Let \( J_{1}= I_{0} \), \( J_{0}=J \).
    We will show that if the conclusion does not hold then the paths passes over all the
  infinite sequence of consecutive adjacent intervals  \( J_{2},J_{3},\dots \) 
  on the left of \( J_{1} \), of size \( |J_{1}| \).
  Since the path is finite, this leads to a contradiction.
  Let \( i=1 \).
 \begin{enumerate}
 \item\label{i:repeated-attack.2}
   Suppose the conclusion of  Lemma~\ref{lem:weak-repeated-attack} does not hold.
   Then the faults needed at least \( \Q/2\E  \) bursts of size \( \beta \) to cover them
   over \( J_{i+1}\cup J_{i} \) during this time,
   consequently at least \( \passno^{*}+2^{k+14} \) bursts happened during
   some two consecutive pair of the  \( 2^{k+14} \) rightward passes over \( J_{i+1} \).
  % We will show that there is a rightward pass over \( J_{i+1} \) in \( K_{i} \), which leads to contradiction
  % with the choice of \( K_{i} \).

\item\label{i:repeated-attack.choice}
  By the Escape property, each fault is covered in a burst of size \( \beta \)
  contained in a segment covering an interval \( >3\CPass\Q\B \)
  with no more faults outside the burst.
  Since these segments don't pass over \( J_{i+1} \),  each contains a fault-free pass over \( J_{i+2} \).
  
  Suppose that \( \Int(J_{i+2},\CMarg\Q\B) \) becomes clean
  at some time during the first \( \passno^{*} \) of these passes.
  There are still \( 2^{k+14} \) fault-free passes over \( J_{i+2} \), so we are
  back at the situation of part~\ref{i:repeated-attack.2} with \( i\gets i+1 \).


    \item\label{i:repeated-attack.1}
      Suppose that \( J_{i+2} \) does not become clean during the first \( \passno^{*} \) passes.
      Then by Lemma~\ref{lem:weak-pass-clean}, since \( k=\CPass+\CMarg \), 
      the number set of faults need a number bursts of size \( \beta \) covering
      them in \( J_{i+2} \) exceeding  \( \s = \passno^{*}(\passno^{*}+2^{k+14}) \).
      Then at least \( \passno^{*}+2^{k+14} \) bursts happen over \( J_{i+2} \)
      between some consecutive pair of the left-right passes over \( J_{i+2} \).
  By the Escape property, each burst is contained in a segment
  covering an interval \( >3\CPass\Q\B \) with no more fault outside the bursts.
  Since these segments don't pass over \( J_{i+2} \),
  each contains a fault-free pass over \( J_{i+3} \).
  This brings us back to the situation of part~\ref{i:repeated-attack.choice} with \( i\gets i+1 \).
\end{enumerate}
\end{proof}


Let us remove the bound on the number of bursts
in the Pass Cleaning property of \( \eta^{*} \)

\begin{lemma}[Pass cleaning]\label{lem:pass-clean}
  Let \( P \) be a space-time path without faults of \( \eta^{*} \) that makes
  at least \( \passno^{*} \) passes over an interval \( I \) of size \( \CPass\Q\B \).
  Then there is a time during \( P \) when \( \Int(I,\CMarg\Q\B) \) becomes clean.
\end{lemma}
\begin{proof}
  Let \( J_{1}= I \).
  We will show that if the conclusion does not hold then the paths passes over all the
  infinite sequence of consecutive adjacent intervals  \( J_{2},J_{3},\dots \) 
  on the left of \( J_{1} \), of size \( |J_{1}| \).
  Since the path is finite, this leads to a contradiction.
Let \( i=1 \), \( k=\CPass+\CMarg \).

   \begin{enumerate}
  \item\label{i:pass-clean.1}
  By weak pass cleaning (Lemma~\ref{lem:weak-pass-clean}),
  if \( \Int(J_{i},\CMarg\Q\B) \) did not become clean for \( \eta^{*} \),
  the number of bursts of size \( \beta \)
  in \( J_{i} \) needed to cover the faults is more than \( \s = \passno^{*}(\passno^{*}+2^{k+14}) \).
  Therefore there is a time interval between two consecutive left-right passes over \( J_{i} \)
  with at least \( 2(\passno^{*}+2^{k+14}) \) bursts over \( J_{i} \).
  Due to the Escape property, each part of \( P \)
  containing one of these bursts has a noise-free segment
  of size \( >2\CPass\Q\B \) either on the left or on the right of \( J_{i} \).
  Without loss of generality we can assume that at least half of them are on the left, giving
   \( \passno^{*}+2^{k+14} \)  noise-free passes over \( J_{i+1} \) during this time.

  \item\label{i:first-choice}
  If  \( J_{i+1} \) does not become clean during the first \( \passno^{*} \) of these passes
  then restart the reasoning, going back to part~\ref{i:pass-clean.1}, setting \( i\gets i+1 \).
  Otherwise
by Lemma~\ref{lem:repeated-attack}, interval \( J_{i} \) becomes clean during the next \( 2^{k+14} \)
      noise-free passes over \( J_{i+1} \), contrary to the assumption.
\end{enumerate}
\end{proof}

\subsection{Attack cleaning and spill bound}

Let us remove the bound on the number of bursts in the scale-up of the Attack Cleaning property.

\begin{lemma}[Attack cleaning]\label{lem:attack-clean}
  Consider the situation of Lemma~\ref{lem:weak-attack-clean}.
  The conclusion holds also if the number of bursts of size \( \le\beta \) needed to cover the
  faults of \( \eta \) in
  \( I=\lint{x-\CMarg\Q\B}{x'+\Q\B} \) is not bounded by \( \Q/3\E \).
\end{lemma}
\begin{proof}
  By Lemma~\ref{lem:weak-attack-clean}, we need now only to consider the case
  when there are at least \( \Q/3\E \) bursts.
  Consider the path \( P'\subseteq P \) containing the first \( \passno^{*}+2^{\CMarg+16} \) of these.
  The Escape property implies that \( P' \) passes the
  interval \( J \) of length \( \CPass\Q\B \) on the right of \( I \) this many times.
  The Pass Cleaning property then implies that \( \Int(J,\CMarg\Q\B) \) becomes
  clean for \( \eta^{*} \) during \( P' \).
  Then Lemma~\ref{lem:repeated-attack} (applied in the left direction)
  implies that within the next \( 2^{\CMarg+16} \)
  passes, the disorder of \( \eta^{*} \) of length \( \le (1+\CMarg)\Q\B \) between the old clean interval
  ending at \( x'+\Q\B \) and the new one beginning at \( x'+(\CMarg+1)\Q\B \) will be erased.
    \end{proof}

Let us prove the scaled-up version of the spill bound property.

\begin{lemma}[Spill bound]\label{lem:spill-bound}
Suppose that an interval \( I \) of size \( > 2\CSpill\Q\B \) is clean for \( \eta^{*} \). and
let \( P \) be a path that has no faults of \( \eta^{*} \).
Then \( \Int(I,\CSpill\Q\B) \) stays clean for \( \eta^{*} \).
\end{lemma}
\begin{proof}
  Without loss of generality, consider exits and entries of the path on the left of \( I \).
  Let \( C_{0},C_{1}\) be the two leftmost colonies in \( I \), where
  by definition \( C_{0} \) is at the very end of \( I \).
  We can assume that \( C_{0} \) 
  is damaged since the Spill Bound property of \( \eta \) allows a spill of size \( \CSpill\B \) into it.
  Consider a part of the path entering \( I \) and then leaving again on the left.
  As long as disorder is at least at a distance \( \E\B \) away from \( C_{1} \),
  when the path enters these colonies it just continues the simulation.
  Any burst of faults will be corrected or leave an island subject to the limitations of health.

  The islands created by faults in \( C_{0} \) can be divided into two groups: one is a group 
  starting from the left end in such a way that the distance between consecutive elements
  is at most \( \E\B \).
  
  The other ones are at a distance \( \ge\Z\B/3 \) from each other, at left turning footprints.
  It follows that if the faults can be covered by at most \( \Q/3\E \) bursts of size \( \beta \)
  in \( P \) over \( C_{0} \) then no colony on the right of \( C_{0} \)
  will be damaged, since the first group never passes beyond \( C_{0} \).
  On the other hand, if  than \( \Q/3\E \) bursts are needed in \( C_{0} \)
  then we can finish just is in the proof of Lemma~\ref{lem:attack-clean}.
\end{proof}

% \section{After a large burst}

% Our goal is to show that the simulation \( M\to M^{*} \)
% of Definition~\ref{def:scale-up} is indeed a simulation.
% Section~\ref{sec:1-level-noise} shows this as long as the head operates in an
% area that is clean for the simulated machine\( M^{*} \) (can be annotated), 
% and has no noise for machine \( M^{*} \) (that is its bursts on the level of machine \( M \)
% are isolated).
% In other words, essentially the Transition Function property of Definition~\ref{def:traj} of
% trajectories for the simulated machine \( M^{*} \) has been taken care of.

% \begin{sloppypar}
% The new element is the possibility of large areas that cannot be annotated: they
% may not even be clean, even on the level of machine \( M \).
% \end{sloppypar}

\section{Proof of the theorem}\label{sec:computation}

Above, we constructed a sequence of generalized Turing machines \( M_{1},M_{2}\dots \)
with cell sizes \( \B_{1},\B_{2},\dots \) where \( M_{k} \) simulates \( M_{k+1} \).
The sequences and dwell periods were also specified in Definition~\ref{def:hier-params}.
Here, we will use this construction to prove Theorem~\ref{thm:main}.


\subsection{Fault estimation}\label{sec:fault-estimation}

The theorem says that there is a Turing machine \( M_{1} \) that can reliably (in the defined sense)
simulate any other Turing machine \( \G \).
Before the simulation starts, the input \( x \) of \( \G \) must be encoded by a code depending on
its length \( |x| \).
We will choose a code that represents the input \( x \) as the information content of a
pair of cells of \( M_{k_{0}} \) for an appropriate \( k_{0}=k_{0}(x) \), and set their kind to \( \Booting \).
Note that the code does not depend on the length of the computation to be performed.
At any stage of the computation there will be a highest level \( K \) such that a generalized Turing
machine \( M_{K} \) will be simulated, with its cells of the Booting kind.

As the computation continues and the probability of some fault occurring increases, the
encoding level will be raised again and again, by lifting mechanism of Section~\ref{sec:booting}.
Let \(  \cE_{k}  \) be the event that no burst of level \( k \) occurs in the space-time region
\( \lint{-2\B_{k}}{2\B_{k}}\times\lint{0}{\Tu_{k+1}} \).
This implies that the history on level \( k \) can be annotated in this region.
Let
\begin{align*}
   \cD_{k} = \bigcap_{i<k}\cE_{i}\cap\neg \cE_{k+1},
\end{align*}
then \( \Prob(\bigcap_{k}\cE_{k})\ge 1 -\sum_{k}\Prob(\cD_{k}) \).
Let us show
\begin{align}\label{eq:fault-estim-0}
 \sum_{k}\Prob(\cD_{k})  = O(\eps).
\end{align}
The number of top level steps on level \( k \) is at most \( 3\Q_{k}\F_{k} \),
where \( \F_{k} \) is the feathering digression size defined in~\eqref{eq:FDef}.
Indeed, in Section~\ref{sec:booting} machine \( M_{k} \) performs at most \( \Q_{k} \) simulation
steps, but the feathering with big turns may introduce digression steps of up to \( 2\F_{k} \) per turn.

From Definition~\ref{def:hier-params} feathering constant \( \F_{k} \) of~\eqref{eq:FDef}
is obtained as
 \begin{align*}
 \F_{k}=(8 k + c_{2})^{3+2\rho}<k^{4}
 \end{align*}
 if  \( \rho \) is a small enough constant and \( k \) is large.
Hence
\begin{align*}
  3 \F_{k}\Q_{k} \le 3 c_{1}^{2}k^{4}\cdot 2^{1.2^{k}}.
\end{align*}
Lemma~\ref{lem:sparsity} bounds the probability of burst of level \( k \) by
\( \eps \cdot 2^{-1.5^{k-1}} \), giving
\begin{align*}
   \Prob(\cD_{k}) = O(\eps k^{4} 2^{1.2^{k}-1.5^{k-1}}),
\end{align*}
which shows~\eqref{eq:fault-estim-0}.

Suppose that machine \( \G \) produces output at its step \( t \) (recall that there is no halting,
but the output in cell 0 will not change further).
Let \( t' \) be any time after the point in the  work of machine \( M_{1} \)
at which the simulation of \( \G \) reaches this stage.
For each \( k \) let \( \tau_{k} \) be the (random) last time before \( t' \)
when the head of the simulated machine \( M_{k} \) reaches position 0.
Let \( \cE'_{k} \) be the event that no burst of level \( k \)
occurs in the space-time region \( \lint{-\B_{k}}{\B_{k}}\times\rint{\tau_{k}}{\tau_{k}+\Tu_{k}} \).

Then \( \bigcap_{k}\cE_{k}\cap\bigcap_{k}\cE'_{k} \) implies that at time \( t' \) the \( \Output \) field
of cell 0 has the output of machine \( G \).
By an argument equivalent to the one given above, the probability of this event is \( 1-O(\eps) \).

Just as above, it is easy to see \( \Prob(\cap_{k}\cE'_{k})\ge 1-O(\eps) \).
(Even though the rectangles defining the events \( \cE'_{k} \) are random,
a probability estimate can be obtained for \( \neg\cE'_{k} \) similarly to \( \neg\cE_{k} \) above:
just average over all possible values of \( \tau_{k} \).)

 \subsection{Space- and time-redundancy}\label{sec:redundancy}

 Even with the simple tripling error-correcting code, there is a constant \( c>1 \) such that
 a colony of level \( k \) uses at most
 \( c \) times more space than the amount of information contained in the
 cell of level \( k+1 \) that it simulates.
 Therefore if \( k \) is the level that needs to be simulated before an output of \( G \) can be reached
 then the space used at that time is at most \( c^{k} \) times the space needed to just store the information.
 If \( G \) produces output at time \( t \) then this is about \( c^{k}t \).
 The size of cells of level \( k \) is of the order of \( 2^{1.2^{k}} \), and this is also the
 the order of the number \( t \) of steps of \( G \) that can be simulated.
 So \( k \) is about \( d\log\log t \) with \( d\approx 1/\log 1.2 \).
 This gives a space redundancy factor
 \begin{align*}
 c^{k}\approx c^{d\log\log t}=(\log t)^{\alpha}  
\end{align*}
 for some \( \alpha>0 \).
 The estimate for time redundancy is similar.

 \section{Discussion}

\paragraph{A weaker but much simpler solution}
If our Turing machine could just simulate a 1-dimensional
fault-tolerant cellular automaton, it would become
fault-tolerant, though compared to a fault-free Turing machine computation of length \( t \),
the slow-down could be quadratic.
(Such a solution would be only \emph{relatively} simpler, being a reduction to a complex, existing one.)
We did not find an easy reduction by just having the simulating
Turing machine sweep larger and larger
areas of the tape, due to the possibility of the head being trapped too long in some large disorder created by
the group of faults.
Trapping can be avoided, however, 
\emph{provided that the length \( t \) of the computation is known in advance}.
The cellular automaton \( C \) can have length \( t \) , and we could define
a ``kind of'' Turing machine \( T \) with a \emph{circular tape} of size \( t \) simulating \( C \).
The transition function of \( T \) would move the head to the right in every step
(with any backward movement just due to faults).

 \paragraph{Decreasing the space redundancy}
 We don't know how to reduce the time redundancy significantly, but
 the space redundancy can be apparently reduced to a multiplicative constant.
 Following Example~\ref{xmp:Reed-Solomon}, it is possible to
 choose an error-correcting code with redundancy that is only a factor \( \delta_{k} \)
 with \( 
   \prod_{k=1}^{\infty}(1-\delta_{k})>1/2
 \).
 This also requires a more elaborate organization of the computation phase described in
 Section~\ref{sec:simulation-phase} since
 the total width of all other tracks must be only some \( \delta_{k} \) times the width
 of the \( \Info \) track.
 For cellular automata, such a mechanism was described in~\cite{GacsSorg01}.

 \paragraph{Other models}
 There is probably a number of models worth exploring with more parallelism than Turing machines, but less
 than cellular automata: for example having some kind of restriction on the number of active units.
 On the other hand, a one-tape Turing machine seems to be the simplest computation model for which a reasonable
 reliability question can be posed, in the framework of transient, non-conspiring faults of constant-bounded
 probability.

 A simpler, universal computation model is the so-called \df{counter machine}.
 This has some constant number of nonnegative integer counters (at least two for universality), and an internal state.
 Each transition can change each counter by \( \pm 1 \), depends on both the internal state
 and on the set of those counters with zero value.
 A fault can change the state and can change the value of any counter by \( \pm 1 \).
 It does not seem possible to perform reliable computation on such a machine in any reasonable sense.
 The statement of such a result
 cannot be too simple-minded, since there is \emph{some} nontrivial task that such a machine can
 do: with \( 2n \) counters, it can remember almost \( 2n \) bits of information with large probability forever
 Indeed, let us start the machine with \( n \) counters having the value 0, and the other \( n \) having some
 large value (depending on the fault probability \( \eps \)).
 The machine will remember forever (with large probability) which set of counters was 0.
 It works as follows (in the absence of a fault):
 at any one time, if exactly \( n \) values have value 0, then increase each nonzero counter by 1.
 Otherwise decrease each nonzero counter by 1.
 
 This sort of computation seems close to the limit of what counter machines can do reliably, but
 how to express and prove this?
 
\bibliographystyle{plain}
\bibliography{reli,gacs-publ}

\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t%%% End: 
